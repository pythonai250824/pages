
# Random forest 

### ‚ùå Why Random Forest Skips Pruning

Random Forest uses a **different strategy** to avoid overfitting:

- Each tree is trained on a **random subset** of the data (bootstrapping)
- At each split, only a **random subset of features** is considered
- Each tree is grown **fully** (no pruning)

Instead of simplifying individual trees, Random Forest relies on the **diversity of the forest** ‚Äî the idea that many different trees making mistakes in different ways will average out to a good prediction.

Why is This Effective?

- A single tree might overfit ‚Äî but averaging many trees leads to stable predictions
- Fully grown trees can capture patterns better; randomness reduces the risk of memorizing noise
- No need to manually tune tree size ‚Äî the ensemble handles it through averaging

### üîç Does Random Forest Use Gini Impurity?

Yes ‚Äî by default, **Random Forest uses Gini Impurity** to decide how to split nodes in each decision tree it builds.

What is Gini Impurity?

Gini Impurity is a measure of how "pure" a node is. In classification:
- A node is **pure** if all examples belong to the same class.
- A node is **impure** if there's a mix of classes.


The goal is to **choose the split that minimizes impurity**

**Why is Gini used in Random Forest?**

Because each tree in the forest is a regular decision tree (CART), and CART uses Gini by default.
It is:
- Fast to compute
- Works well for classification
- Often performs similarly to entropy (information gain), but with less computation

### üå≤ Does Random Forest Use CART Trees?

Yes ‚Äî **Random Forest uses CART (Classification and Regression Trees)** as the base tree model for both classification and regression tasks.

**What is CART?**

**CART** stands for **Classification and Regression Trees**, introduced by Breiman et al. in 1986. It's a type of decision tree algorithm with the following characteristics:

‚úÖ Key Features of CART:
- **Binary Splits Only**: Every internal node splits into exactly two branches (left/right)
- **Supports Two Tasks**:
  - **Classification** ‚Üí typically uses **Gini Impurity** or **Entropy**
  - **Regression** ‚Üí uses **Mean Squared Error (MSE)**
- **No Multi-way Splits**: Unlike algorithms like ID3 or C4.5, CART only splits one feature at a time into two groups
- **Can Grow Fully**: Unless explicitly pruned or limited, CART trees grow until all leaves are pure or stopping conditions are met

**üå≥ How CART Fits into Random Forest**

Random Forest is essentially a collection (an ensemble) of CART trees:
- Each tree is built using the CART algorithm
- Random Forest adds randomness by:
  - Bootstrapping the data (random samples with replacement)
  - Randomly selecting features for each split (`max_features`)

This combination allows the forest to reduce overfitting while keeping the power of CART.

**Summary**

| Feature | CART | Random Forest |
|--------|------|----------------|
| Tree Type | Binary-only CART | Ensemble of CART trees|
| Supports Classification? | ‚úÖ | ‚úÖ |
| Supports Regression? | ‚úÖ | ‚úÖ |
| Uses pruning? | Optional | ‚ùå Not used by default |
| Split criterion | Gini, Entropy, MSE | Gini/Entropy per tree |

Ensemble = a method that combines multiple models (="weak learners") to produce a stronger, more accurate model

Random Forest builds **many CART trees** with added randomness ‚Äî and combines their predictions to form a powerful, robust model

### ‚öñÔ∏è Does Random Forest Require Feature Scaling?

No ‚Äî **Random Forest does not require feature scaling** such as standardization or normalization.

**Why Not?**

Random Forest is based on **decision trees**, which split data using thresholds like:
```text
Is feature X <= some value?
```

This splitting behavior is **not affected** by the magnitude or distribution of the feature values.

- Whether a feature ranges from 0‚Äì1 or 0‚Äì1000, the tree only cares about where to split the data to separate classes.
- Unlike models like Logistic Regression or KNN, **distance and scale do not impact the tree‚Äôs logic.**

**When Is Scaling Needed?**

Feature scaling is needed when the algorithm relies on **distance, gradient optimization, or numerical stability**

Examples:
- ‚úÖ **Needed**:
  - Logistic Regression
  - Linear Regression (if using gradient descent)
  - Polynomial Regression
  - K-Nearest Neighbors (KNN)
  - Support Vector Machines (SVM)
- ‚ùå **Not Needed**:
  - Decision Trees
  - Random Forest
  - XGBoost
  - LightGBM

**Summary**

| Model Type | Needs Scaling? |
|------------|----------------|
| Random Forest | ‚ùå No |
| Decision Tree | ‚ùå No |
| Logistic Regression | ‚úÖ Yes |
| Linear Regression (with GD) | ‚úÖ Yes |
| Polynomial Regression | ‚úÖ Yes |
| KNN | ‚úÖ Yes |

So with Random Forest, **you can skip feature scaling** ‚Äî and it‚Äôll still perform just fine