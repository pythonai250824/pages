# מה זה TensorFlow

<img src="deepex2.jpg" style="width: 15%" align="right" />

טנסר-פלואו היא ספריית קוד פתוח מבית Google  
היא נועדה לעזור לנו לבנות מודלים של למידת מכונה ובעיקר רשתות נוירונים  
הספרייה מאוד חזקה וגמישה ומשמשת הרבה מאוד בתחומים כמו ראיית מחשב, עיבוד שפה, חיזוי ועוד

---

## למה להשתמש ב־TensorFlow

- מאפשרת לבנות רשתות נוירונים בצורה קלה או מתקדמת  
- תומכת גם ב־CPU וגם ב־GPU כך שאפשר לעבוד מהר יותר  
- מתחברת בצורה נוחה עם ספריות אחרות כמו NumPy  
- יש לה קהילה מאוד גדולה והמון מדריכים ודוגמאות באינטרנט  
- אפשר להריץ את המודלים גם על מובייל וגם על שרתים

## איך מתקינים את TensorFlow

בשורת הפקודה או ב־Jupyter מריצים

```
pip install tensorflow
```

לאחר ההתקנה אפשר לבדוק שהספרייה קיימת בעזרת

```python
import tensorflow as tf
print(tf.__version__)
```

## איך מתחילים לעבוד עם TensorFlow

לרוב אנחנו נשתמש בממשק שנקרא `Keras` שנמצא בתוך TensorFlow  
הממשק הזה מאפשר לנו לבנות מודלים בצורה פשוטה ונוחה גם למתחילים

אנחנו יכולים ליצור רשת נוירונים על ידי הגדרה של שכבות  
למשל רשת פשוטה עם שכבת קלט אחת שכבת חבויה אחת ושכבת פלט אחת תיראה כך

**מה זה Keras**

ממשק עבודה גבוה לבניית רשתות נוירונים
הוא חלק מתוך TensorFlow ומאפשר ליצור מודלים בצורה פשוטה ומובנת  
הממשק מתאים במיוחד למתחילים וגם למפתחים מתקדמים שרוצים לעבוד מהר

המודל נותן לנו את היכולת לבנות מודלים בצורה של בלוקים  
כל שכבה שאנחנו מוסיפים היא פשוט שורה אחת של קוד  
לא צריך לכתוב נוסחאות מתמטיות ולא צריך לנהל את המודל בצורה ידנית

**יתרונות של Keras**

- ממשק נוח מאוד לקריאה ולהבנה גם בלי רקע מתמטי עמוק  
- אפשר לבנות רשתות פשוטות או מורכבות תוך שליטה על כל שלב  
- קיים אינטגרציה מושלמת עם TensorFlow  
- קהילה רחבה והמון דוגמאות מוכנות באינטרנט  
- תומך בכל סוגי הבעיות כמו סיווג רגרסיה ניתוח תמונה או טקסט

### Keras Models

יש שני סוגים עיקריים של בניית מודלים ב־Keras

- **Sequential API**

הדרך הפשוטה ביותר  
בונים את המודל כשכבה אחרי שכבה בסדר קווי

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()  # Create an empty sequential model
model.add(Dense(10, activation='relu'))  # Hidden layer with 10 neurons using ReLU activation function
model.add(Dense(1, activation='sigmoid'))  # Output layer with 1 neuron using sigmoid activation function
```

- **Functional API**

מאפשר חיבור יותר גמיש בין שכבות
מתאים לרשתות מורכבות עם הסתעפויות או כניסות מרובות

**מה הכוונה לרשתות מורכבות עם הסתעפויות או כניסות מרובות**

כאשר אנחנו בונים מודל פשוט בעזרת Sequential  
כל שכבה באה אחת אחרי השנייה בצורה ישרה וקווית אבל לפעמים אנחנו רוצים לבנות רשת מתקדמת יותר עם מבנה גמיש יותר  
בדיוק בשביל זה נשתמש בממשק שנקרא Functional API

**הסתעפויות בתוך הרשת (Branches)**

לפעמים אנחנו רוצים שהרשת תתפצל למספר מסלולים למשל שהפלט של שכבה אחת יועבר לשתי שכבות שונות  
כל שכבה תעבד את המידע בצורה שונה ואז נחבר ביניהן שוב בהמשך

אפשר לחשוב על זה כמו כביש שמתפצל לשניים וכל מסלול לומד משהו אחר אחר כך מחברים את כל הידע ביחד לשכבה אחת משותפת

**כניסות מרובות (Multiple Inputs)**

לפעמים יש לנו יותר מסוג קלט אחד למשל ייתכן שיש לנו גם מידע טקסטואלי וגם מידע מספרי  
אנחנו רוצים לעבד כל סוג של מידע בצורה שונה לכן כל קלט ייכנס לרשת אחרת, עם שכבות שמתאימות לו  
ובהמשך נחבר את שתי הרשתות לתוצאה אחת משותפת

**דוגמה מעשית**

מערכת שמנסה לחזות התנהגות של לקוחות בבנק  
- קלט ראשון הוא פרטים דמוגרפיים כמו גיל מגדר והכנסה  
- קלט שני הוא היסטוריה של עסקאות שביצע הלקוח  
כל אחד עובר דרך שכבות נפרדות ואז מתאחדים לרשת אחת כך כל חלק מהרשת מתמקצע בסוג נתונים אחר

**מתי משתמשים בזה**

- כשיש נתונים מטקסט ותמונות ביחד  
- כשיש צורך במודל שמבצע מספר משימות במקביל  
- כשבונים רשתות מתקדמות כמו ResNet או Inception  
- כשיש מבנה נתונים מורכב שלא מתאים למודל פשוט קווי

**נספח - קוד לדוגמא**

```python
from tensorflow.keras.layers import Input, Dense, Concatenate
from tensorflow.keras.models import Model

# כניסה ראשונה – פרטי לקוח (4 מאפיינים)
input_a = Input(shape=(4,), name='customer_info')
x = Dense(8, activation='relu')(input_a)

# כניסה שנייה – היסטוריית רכישות (10 ערכים)
input_b = Input(shape=(10,), name='purchase_history')
y = Dense(8, activation='relu')(input_b)

# שילוב של שני הפלטים
combined = Concatenate()([x, y])
z = Dense(4, activation='relu')(combined)
output = Dense(1, activation='sigmoid')(z)

# בניית המודל
model = Model(inputs=[input_a, input_b], outputs=output)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
```

## תהליך עבודה כללי עם TensorFlow

### שלב 1 – טעינת הנתונים והכנה

אנחנו מתחילים בטעינה של קובץ הנתונים אל תוך pandas  
לאחר מכן מבצעים פעולות ניקוי כמו המרת עמודות טקסט לערכים מספריים  
בנוסף מבצעים סטנדרטיזציה כדי שכל הערכים יהיו בטווח אחיד

### שלב 2 – בניית המודל

אנחנו יוצרים את המודל באמצעות מחלקת `Sequential`  
ומוסיפים לו שכבות `Dense` עם מספר נוירונים ופונקציית הפעלה לכל שכבה

### שלב 3 – קומפילציה של המודל

כאן אנחנו קובעים איך המודל ילמד  
אנחנו בוחרים optimizer כמו `sgd` או `adam`  
וגם פונקציית הפסד כמו `binary_crossentropy` או `mse` לפי סוג הבעיה

ראה הסבר בהמשך ל `binary_crossentropy` 

**מה זה Optimizer ברשת נוירונים**

אופטימייזר הוא החלק במודל שאחראי **לשפר את המשקלים** של הנוירונים במהלך האימון  
המטרה שלו היא לגרום למודל לטעות פחות בכל איטרציה  
הוא עושה את זה בעזרת חישוב של נגזרות (שיפועים) והתאמות קטנות לערכים של המשקלים

**מה זה SGD**

 קיצור של Stochastic Gradient Descent  
זוהי הגרסה הבסיסית ביותר של אופטימייזר

- בכל איטרציה הוא מחשב את השגיאה של המודל  
- מחשב את השיפוע (gradient) של פונקציית ההפסד  
- מעדכן את המשקלים של הנוירונים כדי להקטין את ההפסד

הוא פשוט מהיר וקל להבנה אבל לפעמים הוא קופץ יותר מדי או נתקע במקומות לא טובים

**מה זה Adam**

 קיצור של Adaptive Moment Estimation  
הוא נחשב לאחד האופטימייזרים הכי טובים ומתקדמים לשימוש כללי

- הוא שומר על ממוצע של השיפועים הקודמים  
- וגם על "זיכרון" של כמה השיפועים היו חזקים בעבר  
- הוא מתאים אוטומטית את גודל הצעד (learning rate) בכל כיוון

התוצאה היא אופטימיזציה הרבה יותר **יציבה מהירה וחכמה** לרוב כשלא יודעים במה לבחור – פשוט מתחילים עם Adam

**מתי לבחור מה**

- אם רוצים להבין לעומק איך אופטימיזציה עובדת – תתחילו עם SGD  
- אם רוצים תוצאות טובות מהר – תשתמשו ב־Adam  
- אם המודל לא לומד טוב תנסו להחליף בין האופטימייזרים ולראות הבדל


### שלב 4 – אימון המודל

אנחנו מאמנים את המודל באמצעות הפונקציה `fit`  
קובעים כמה שורות יהיו בכל חבילה באמצעות `batch_size`  
וגם כמה פעמים המודל יעבור על כל הנתונים באמצעות `epochs`

**מה זה batch_size**

כשאנחנו מאמנים רשת נוירונים אנחנו לא נותנים לה את כל הנתונים בבת אחת  
במקום זה אנחנו מחלקים את הנתונים **לחבילות קטנות יותר** שנקראות Batches  
כל חבילה כזאת מכילה מספר מסוים של שורות מהנתונים

המספר הזה נקרא `batch_size`

**דוגמה**

אם יש לנו 10,000 שורות נתונים  
ו־batch_size שלנו הוא 32  
אז בכל פעם המודל יראה רק 32 שורות  
יחשב את השגיאה רק עליהן  
ויעשה עדכון קטן למשקלים  
ורק אז יעבור ל־32 הבאות

הוא יחזור על זה שוב ושוב עד שעבר על כל ה־10,000 שורות  
וזה נקרא **Epoch** אחד

**למה לא נותנים את כל הנתונים ביחד**

- כדי לחסוך בזיכרון  
- כדי לאפשר למודל ללמוד בהדרגה  
- כדי להכניס אקראיות לתהליך הלמידה ולשפר תוצאות

**איך בוחרים batch_size**

- ערכים נפוצים הם 32, 64 או 128  
- ערכים קטנים (למשל 16) יכולים לשפר דיוק אבל להאט את הלמידה  
- ערכים גדולים (למשל 256) יגרמו ללמידה מהירה יותר אבל פחות מדויקת לפעמים

**כמה epochs צריך?**

התשובה היא – זה תלוי

אם נעצור מוקדם מדי המודל לא ילמד מספיק (נקרא underfitting)

אם נמשיך יותר מדי המודל ילמד "יותר מדי טוב" ויתחיל לזכור את הדוגמאות (נקרא overfitting)

**אז איך בוחרים מספר נכון**

- מתחילים מערך בסיסי כמו 50 או 100  
- עוקבים אחרי הביצועים של המודל על קבוצת הטסט  
- אם הדיוק על הטסט מתחיל לרדת – עוצרים את האימון  
- אפשר להשתמש ב־EarlyStopping שעוצר את האימון אוטומטית כשהמודל מפסיק להשתפר

**כלל אצבע**

- מודלים פשוטים עובדים טוב עם 10–100 epochs  
- מודלים עמוקים או דאטה גדול יכולים להזדקק ל־200 ואפילו יותר  
- תמיד עדיף להתחיל עם מספר בינוני (למשל 100) ולעקוב אחרי הגרפים


### שלב 5 – בדיקה וחיזוי

לאחר סיום האימון נבחן את הביצועים של המודל על קבוצת הטסט  
נשתמש במדדים כמו `accuracy` או `confusion matrix`  
נוכל גם להזין למודל לקוחות חדשים ולראות את התחזית שהוא מחזיר
