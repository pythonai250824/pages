# NLP SPAM DETECTION

## דוגמא בפייתון לגילוי ספאם

בדוגמה הזו נשתמש בקובץ `smsspamcollection.tsv`

הקובץ מכיל נתונים על הודעות אימייל, כולל תוכן ההודעה, האורך שלה וסימני הפיסוק שבה

בנוסף, הקובץ כולל תווית עבור כל הודעה – `ham` (אימייל לגיטימי) או `spam` (אימייל זבל)

המטרה שלנו היא להשתמש במודל למידה מונחית כדי לחזות האם אימיילים עתידיים יסווגו כ־`spam` או לא

<img src="images/nlpe1.jpg" style="width: 60%" />

```python
      df.count() = 5,572 rows
```

<img src="images/nlpe2.jpg" style="width: 60%" />

<img src="images/nlpe3.jpg" style="width: 70%" />

בשלב זה אנו מפרידים את הנתונים לשני חלקים:
- קבוצת **אימון** – משמשת לאימון המודל (במקרה הזה 70% מהנתונים)
- קבוצת **בדיקה** – משמשת לבדוק את ביצועי המודל על טקסטים שהוא לא ראה (30%)

ההפרדה הזו חשובה כדי לוודא שהמודל לא רק "שינן" את הנתונים, אלא באמת למד לזהות דפוסים כלליים

<img src="images/nlpe4.jpg" style="width: 60%" />

### 🧮 מטריצת ספירה (Count Vector Matrix)

בשלב זה אנו ממירים את ההודעות ממילים לטבלה של מספרים, באמצעות **שיטת Bag of Words**  
כל שורה מייצגת הודעה  
כל עמודה מייצגת מילה מתוך כלל אוצר המילים  
והערך בכל תא מציין כמה פעמים המילה הופיעה באותה הודעה

לדוגמה: אם המילה "free" מופיעה 3 פעמים בהודעה מסוימת – זה יוצג כ־3 בטבלה

**📏 מה זה `(3900, 7245)`?**

זהו גודל המטריצה שנוצרה לאחר הספירה:

- `3900` = מספר ההודעות בקבוצת האימון (train set)  
- `7245` = מספר המילים הייחודיות בכל הודעות האימון (גודל אוצר המילים)

כלומר: יצרנו מטריצה בגודל **3900 שורות × 7245 עמודות** שבה כל תא הוא מספר ההופעות של מילה בהודעה

📌 זהו השלב שבו הטקסט הפך לנתונים מספריים – מוכנים ללמידת מכונה

כאשר אנו מבצעים המרה של טקסט למספרים באמצעות **CountVectorizer**, נוצרת **מטריצה** – כלומר טבלה של מספרים

- כל **שורה** מייצגת הודעה אחת (מסמך טקסט יחיד)
- כל **עמודה** מייצגת מילה ייחודית מאוצר המילים (כל המילים שהופיעו בכל ההודעות)
- כל **תא** בטבלה מכיל את מספר הפעמים שהמילה מופיעה באותה הודעה

לדוגמא

אם יש לנו 2 הודעות:

1. `"I love NLP"`  
2. `"NLP is fun"`

**אוצר המילים (Vocabulary):**
["I", "love", "NLP", "is", "fun"]

**מטריצת הספירה:**

| הודעה \ מילה | I | love | NLP | is | fun |
|--------------|---|------|-----|----|-----|
| הודעה 1      | 1 | 1    | 1   | 0  | 0   |
| הודעה 2      | 0 | 0    | 1   | 1  | 1   |

- `3900` שורות = 3900 הודעות טקסט (train set)
- `7245` עמודות = 7245 מילים שונות שהופיעו בכלל ההודעות

כל שורה = מסמך אחד  
כל עמודה = מילה ייחודית  
כל תא = כמה פעמים המילה מופיעה במסמך

**❓ למה יש לנו 5572 שורות אבל רק 3900 במספר המסמכים?**

**✅ 1. סך כל השורות בטבלה**
- הנתונים בקובץ `smsspamcollection.tsv` מכילים **5572 הודעות טקסט**
- כלומר: 5572 שורות בטבלה = 5572 מסמכים פוטנציאליים

**✂️ 2. חלוקה ל־Train ו־Test**

בשלב ההכנה לקראת המודל, מבצעים חלוקה של הנתונים:

- **70%** → לאימון (Train set) → בערך **3900 שורות**
- **30%** → לבדיקה (Test set) → בערך **1672 שורות**

ה־`CountVectorizer` מאמן את עצמו **רק על קבוצת האימון (3900 הודעות)**  
ולכן מטריצת הספירה שהוא מחזיר היא בגודל **3900 × N מילים**


<img src="images/nlpe5.jpg" style="width: 60%" />

Print sample row:
```
{'are': 0.2524422607133928, 'going': 0.3230441330419753, 'make': 0.36308344981075574, 'me': 0.22163671451015382, 
 'moan': 0.5328892367086496, 'princess': 0.44029157228894206, 'to': 0.15576398008518208, 'yes': 0.35100930940159797, 
 'you': 0.1602042707313863}
```

### 🧮 חישוב ה- TF-IDF

בשלב זה אנו מעדכנים את ערכי הספירה לפי חשיבותן היחסית של המילים בכל המסמכים  
המטרה היא לתת משקל גבוה למילים ייחודיות ומידעיות, ולהפחית משקל ממילים שכיחות מדי כמו "the" או "is"

כך נוצר ייצוג מדויק יותר של הטקסט עבור המודל

**🧠 מה זה TfidfTransformer ומה הוא עושה?**

**📌 המטרה:**
להמיר את מטריצת הספירה (שנוצרה ע"י CountVectorizer) למטריצה חכמה יותר  
כזו שלא רק סופרת מילים – אלא יודעת להעריך **כמה מילה באמת חשובה** במסמך

**למה CountVectorizer לא מספיק?**

ה- CountVectorizer פשוט סופר כמה פעמים כל מילה מופיעה  
אבל:
- מילים נפוצות כמו `"the"`, `"is"` או `"and"` יופיעו המון – וזה **מטעה**
- הן לא באמת מייחדות מסמך אחד מאחר
- הן מקבלות משקל גבוה בטעות

**🧠 מה כן עושה TF-IDF?**

ה- TF-IDF = Term Frequency – Inverse Document Frequency  
הוא מחשב ציון משוקלל לכל מילה לפי:
- **TF** – כמה פעמים המילה מופיעה במסמך (כמו בספירה)
- **IDF** – כמה היא נדירה בכל שאר המסמכים

$$
\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \log \left( \frac{N}{df(t)} \right)
$$

**🧪 מה עושה `TfidfTransformer`?**

המחלקה `TfidfTransformer` מקבלת את מטריצת הספירה (3900 × 7245)  
ומחשבת **ציון TF-IDF לכל מילה במסמך**

- אם מילה נדירה (למשל: `"unsubscribe"`) → תקבל **משקל גבוה**
- אם מילה מאוד נפוצה (למשל: `"the"`) → תקבל **משקל נמוך**

**🧠 למה זה טוב?**

ה- TF-IDF נותן ייצוג מדויק יותר לטקסט  
כך המודל יוכל להבין:
- אילו מילים באמת משמעותיות
- ואילו רק "רעש רקע"

וזה עוזר מאוד לשפר את הביצועים של המודל

**דוגמא:**

נניח שיש לנו שני מסמכים (הודעות):

- Document 1: `"I love NLP"`
- Document 2: `"NLP is fun"`

מתוך זה נבנה אוצר מילים עם 5 מילים ייחודיות:

- Vocabulary = ["I", "love", "NLP", "is", "fun"]

**🎯 מטריצת TF-IDF**

| מילה / הודעה | I      | love   | NLP    | is     | fun    |
|--------------|--------|--------|--------|--------|--------|
| Document 1   | 0.1003 | 0.1003 | 0      | 0      | 0      |
| Document 2   | 0      | 0      | 0      | 0.1003 | 0.1003 |

**הסבר המספרים**

- המילה **"NLP"** מופיעה בשני המסמכים → IDF = log(2/2) = 0 → משקל = 0
- המילים **"I"**, **"love"**, **"is"**, **"fun"** מופיעות רק במסמך אחד → IDF = log(2/1) = 0.301  
- TF עבור מילים שמופיעות במסמך = 1/3 (כי בכל מסמך 3 מילים)  
- TF-IDF = 1/3 × 0.301 ≈ **0.1003**

📌 כך אפשר לראות איך מילה נדירה מקבלת **ציון גבוה יותר**, בעוד מילה שכיחה מדי (כמו "NLP" פה) מקבלת **ציון 0**

<img src="images/nlpe6.jpg" style="width: 60%" />

### מהו TfidfVectorizer?

ה- TfidfVectorizer מקצר את הפעולה הקודמת

זו רק דוגמא פה כיצד להשתמש בו

זה פשוט שילוב של 2 השלבים הקודמים לתוך שלב מאוחד

**קודם כל נבין מה קרה בשני השלבים:**

1. **CountVectorizer**  
   סופר כמה פעמים כל מילה מופיעה בטקסט  
   יוצר מטריצה של: כמה פעמים מילה א מופיעה במסמך ב

2. **TfidfTransformer**  
   לוקח את מטריצת הספירה ומחשב TF-IDF  
   כלומר – במקום רק לספור, הוא שוקל את החשיבות היחסית של כל מילה

**אז למה בכלל יש `TfidfVectorizer`?**

כי הוא משלב **שני שלבים באחד**:

- גם בונה את אוצר המילים (כמו CountVectorizer)
- וגם מחשב TF-IDF (כמו TfidfTransformer)

כלומר:
במקום לכתוב:

```python
X_counts = CountVectorizer().fit_transform(X_train)
X_tfidf = TfidfTransformer().fit_transform(X_counts)
```

אפשר פשוט לכתוב:

```python
X_tfidf = TfidfVectorizer().fit_transform(X_train)
```

**מתי נשתמש בזה?**

- אם אתה **רוצה שליטה מלאה** – תשתמש ב־`CountVectorizer` + `TfidfTransformer`
- אם אתה **רוצה קיצור דרך** ונוחות – פשוט תשתמש ב־`TfidfVectorizer`

**📌 סיכום קצר:**

| שיטה | שלבים | יתרון |
|------|--------|--------|
| CountVectorizer + TfidfTransformer | 2 שלבים | שליטה ידנית |
| TfidfVectorizer | שלב אחד | פשוט ויעיל |



<img src="images/nlpe7.jpg" style="width: 30%" />

### 🧠 אימון מודל סיווג עם LinearSVC

**🧠 מה זה LinearSVC ואיך הוא שונה ממודלים אחרים?**

**📌 רקע קצר:**

- **LinearRegression** – מודל לחיזוי ערכים מספריים (רגרסיה)
- **SVM (Support Vector Machine)** – שיטה כללית לסיווג ולרגרסיה, שפועלת על ידי חיפוש גבול שמפריד בין מחלקות
- **SVC (Support Vector Classifier)** – מימוש של SVM לסיווג (classification) כללי, שיכול להיות גם לא־ליניארי

**✅ אז מה זה LinearSVC?**

ה- `LinearSVC` הוא גרסה מהירה ופשוטה של SVM שמתמקדת רק במקרים שבהם **יש הפרדה ליניארית בין המחלקות**  
כלומר – אם אפשר להפריד בין המחלקות בקו ישר (במימד גבוה – מישור או היפר-מישור)

**⚙️ תכונות עיקריות של LinearSVC:**

- מתאים מאוד ל־**datasets עם הרבה דוגמאות ומאפיינים** (כמו טקסט!)
- משתמש ב־**L2 regularization** כברירת מחדל
- מאפשר להגדיר פרמטר בשם `C` לקביעת רמת העונש על שגיאות
- הרבה יותר **מהיר** מ־`SVC` כשיש הרבה תכונות (features), כמו TF-IDF

**💬 למה משתמשים בו ב־NLP?**

בטקסט יש **אלפי מאפיינים (מילים)** לכל מסמך  
לכן מודל כמו `LinearSVC` שמתמודד טוב עם **מימד גבוה** ועובד מהר – הוא בחירה מעולה

**📌 סיכום:**

| מודל             | שימוש                | קו הפרדה     | מתאים ל־           |
|------------------|----------------------|--------------|--------------------|
| LinearRegression | חיזוי מספרים         | רציף         | רגרסיה             |
| SVC              | סיווג כללי           | גם לא-ליניארי | דאטה מורכב         |
| LinearSVC        | סיווג ליניארי מהיר  | קו ישר        | טקסטים ומימד גבוה  |


בשלב הזה אנו משתמשים באלגוריתם למידת מכונה מסוג **Linear Support Vector Classifier (LinearSVC)**  
זהו מודל נפוץ לביצוע **סיווג טקסט**, במיוחד כאשר מדובר בנתונים לאחר עיבוד TF-IDF

**⚙️ מה כל שורה עושה?**

`from sklearn.svm import LinearSVC`  

מייבא את מחלקת `LinearSVC` מתוך מודול הסיווג של Scikit-Learn

`clf = LinearSVC()`  

יוצרת מופע חדש של המודל – כלומר מכונה מוכנה ללמידה

`clf.fit(X_train_tfidf, y_train)`  

מאמנים את המודל על הנתונים:
- `X_train_tfidf` = המאפיינים המספריים של הודעות האימון (לאחר TF-IDF)
- `y_train` = התוויות (spam / ham)

**📌 תוצאה**

בסיום השלב הזה, המודל `clf` יודע:
- לזהות דפוסים בטקסטים
- ולסווג הודעות חדשות כ־`spam` או `ham`

זהו שלב האימון בפועל של המודל


<img src="images/nlpe8.jpg" style="width: 60%" />

<img src="images/nlpe9.jpg" style="width: 55%" />

<img src="images/nlpe10.jpg" style="width: 65%" />

<img src="images/nlpe11.jpg" style="width: 60%" />




