## ğŸ¶ğŸ± ××™××•×Ÿ ×¨×©×ª CNN ×œ×¡×™×•×•×’ ×›×œ×‘ ××• ×—×ª×•×œ (Colab)

## ğŸš€ Getting Started in Google Colab

To run this notebook in the cloud, open:
[https://colab.research.google.com/](https://colab.research.google.com/)

Create a new notebook by clicking **File â†’ New notebook** and copy the code blocks below step-by-step

## CNN_dog_cat_dataset.zip

ğŸ—‚ï¸ Before running the training code, **upload the file** `CNN_dog_cat_dataset.zip` to your **Google Drive** inside a folder named `content`  
(Or modify the code to reflect a different folder structure if needed)

This dataset is required for training the CNN to classify dog and cat images

## CNN code

### ğŸ”§ Importing Required Libraries and Configuring GPU (if available)

```python
# Importing standard libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf

# Optional: prevent TensorFlow from using GPU (for debugging or testing)
tf.config.set_visible_devices([], 'GPU')

# Check how many GPUs are available
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
```
Output:
```python
Num GPUs Available:  1
```

Explanation:

* `numpy`, `pandas` are used for data handling
* `matplotlib`, `seaborn` are used for plotting and visualization
* `tensorflow` is the deep learning framework used to build and train the CNN
* `tf.config.set_visible_devices([], 'GPU')` disables GPU usage intentionally (can be removed to use GPU)
* `list_physical_devices('GPU')` checks how many GPUs are detected by TensorFlow\` â€“ ××¦×™×’ ×›××” ×›×¨×˜×™×¡×™ GPU ×–××™× ×™×

### ğŸ“ ×”×ª×—×‘×¨×•×ª ×œÖ¾Google Drive

×›×“×™ ×œ×”×©×ª××© ×‘×§×‘×¦×™× ××”×“×¨×™×™×‘, ××—×‘×¨×™× ××ª Colab ××œ ×—×©×‘×•×Ÿ Google Drive ×©×œ×š

```python
import pandas as pd
import zipfile
from google.colab import drive

drive.mount('/content/drive')  # ××—×‘×¨ ××ª ×”×“×¨×™×™×‘, ×™×•×•×¦×¨ ×§×™×©×•×¨ ×œ× ×ª×™×‘ /content/drive
```

### ğŸ“Œ ×‘×“×™×§×ª ××™×§×•× × ×•×›×—×™

```bash
pwd
```

×¤×§×•×“×” ×©××¦×™×’×” ××ª ×”× ×ª×™×‘ ×”× ×•×›×—×™ ×©×œ ×¡×‘×™×‘×ª ×”×¢×‘×•×“×”
×œ×¨×•×‘ ×”×ª×•×¦××” ×ª×”×™×”:

```
/content
```

### ğŸ“¦ ×—×™×œ×•×¥ ×§×•×‘×¥ ZIP ×¢× ×”×ª××•× ×•×ª

```python
import zipfile

zip_file = '/content/drive/MyDrive/content/CNN_dog_cat_dataset.zip'  # ×”× ×ª×™×‘ ×œ×§×•×‘×¥ ×”×“×—×•×¡ ×‘×“×¨×™×™×‘

with zipfile.ZipFile(zip_file, 'r') as zip_ref:
  zip_ref.extractall('/content/dataset')  # ×—×™×œ×•×¥ ×”×§×‘×¦×™× ×œ×ª×™×§×™×™×” ×—×“×©×”
```

### ğŸ“‚ ××¢×‘×¨ ×œ×ª×™×§×™×™×” ×¢× ×”×“××˜×”

```bash
cd dataset/
```

```bash
cd CNN_dog_cat_dataset/
```

×‘×ª×•×š ×”×ª×™×§×™×™×” ×”×–×• ×™×•×¤×™×¢×• ×›×›×œ ×”× ×¨××” ×ª×™×§×™×•×ª ×‘×©× `train` ×•Ö¾`test` ×”××›×™×œ×•×ª ××ª ×”×ª××•× ×•×ª ×©×œ ×›×œ×‘×™× ×•×—×ª×•×œ×™×

### ğŸ§ª ×‘×“×™×§×ª ×”×ª×™×§×™×™×” ×œ××—×¨ ×—×™×œ×•×¥

```bash
ls
```

Output:
```
single_prediction/  test_set/  training_set/
```

××¦×™×’ ××ª ×ª×•×›×Ÿ ×”×ª×™×§×™×™×” ×”× ×•×›×—×™×ª ×•××•×•×“× ×©×”×§×‘×¦×™× ×•×”××‘× ×” ×ª×§×™× ×™×

### ğŸ–¼ï¸ ×”×’×“×¨×ª ×§×•× ×¤×™×’×•×¨×¦×™×” ×œ×˜×¢×™× ×ª ×ª××•× ×•×ª

```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Training data generator with augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,         # Normalize pixel values to [0,1] â€” helps speed up training and stabilize gradients
    shear_range=0.2,        # Apply a slight diagonal transformation (shear) â€” simulates natural changes in camera angle
    zoom_range=0.2,         # Apply random zoom-in effect â€” helps the model recognize objects at different scales
    horizontal_flip=True    # Flip images horizontally â€” helps the model handle symmetry (e.g., cat facing left or right)
)

# Testing data generator â€” only normalization, no augmentation
test_datagen = ImageDataGenerator(rescale=1./255)
```

### ğŸ“¥ ×˜×¢×™× ×ª ×”×ª××•× ×•×ª ××”×ª×™×§×™×•×ª

```python
training_set = train_datagen.flow_from_directory(
    '/content/dataset/CNN_dog_cat_dataset/dataset/training_set',
    target_size=(64, 64),
    batch_size=32,
    class_mode='binary'
)

test_set = test_datagen.flow_from_directory(
    '/content/dataset/CNN_dog_cat_dataset/dataset/test_set',
    target_size=(64, 64),
    batch_size=32,
    class_mode='binary'
)
```

* `target_size=(64, 64)` â†’ ×©×™× ×•×™ ×’×•×“×œ ×”×ª××•× ×•×ª ×œ×’×•×“×œ ××—×™×“
* `batch_size=32` â†’ ×›××” ×ª××•× ×•×ª × ×˜×¢× ×•×ª ×‘×›×œ ×¤×¢×
* `class_mode='binary'` â†’ ×¡×™×•×•×’ ×‘×™× ××¨×™ (×—×ª×•×œ ××• ×›×œ×‘)

### ğŸ§  ×‘× ×™×™×ª ×¨×©×ª CNN

```python
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

cnn = Sequential()

# ×©×›×‘×ª ×§×•× ×‘×•×œ×•×¦×™×” ×¨××©×•× ×”
cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))
cnn.add(MaxPooling2D(pool_size=2, strides=2))

# ×©×›×‘×ª ×§×•× ×‘×•×œ×•×¦×™×” ×©× ×™×™×”
cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))
cnn.add(MaxPooling2D(pool_size=2, strides=2))

# Flatten
cnn.add(Flatten())

# Fully Connected
cnn.add(Dense(units=128, activation='relu'))
cnn.add(Dense(units=1, activation='sigmoid'))  # ×‘×’×œ×œ ×©×–×” ×¡×™×•×•×’ ×‘×™× ××¨×™
```

### ×”×¡×‘×¨ ×œ×§×•×“

**ğŸ§  ××” ×–×” Conv2D ×•Ö¾MaxPooling2D ×•×œ××” ××©×ª××©×™× ×‘×”×?**

×‘×‘× ×™×™×ª ×¨×©×ª ×¢×¦×‘×™×ª ×§×•× ×‘×•×œ×•×¦×™×•× ×™×ª (CNN), ×™×© ×©×ª×™ ×©×›×‘×•×ª ×—×©×•×‘×•×ª ×‘××™×•×—×“:

#### ğŸ”· Conv2D â€“ ×©×›×‘×ª ×§×•× ×‘×•×œ×•×¦×™×”

×§×•× ×‘×•×œ×•×¦×™×”2×“ ×”×™× ×©×›×‘×” ×©××—×¤×©×ª **×××¤×™×™× ×™× ×—×©×•×‘×™×** ×‘×ª××•× ×” ×›××• ×§×•×•×™×, ×¦×‘×¢×™×, ×’×‘×•×œ×•×ª, ×ª×‘× ×™×•×ª, ××¨×§× ×•×¢×•×“  
×”×™× ×¢×•×©×” ××ª ×–×” ×‘×¢×–×¨×ª **×¤×™×œ×˜×¨×™× ×§×˜× ×™×** (×œ××©×œ ×‘×’×•×“×œ 3x3) ×©×–×–×™× ×¢×œ ×¤× ×™ ×”×ª××•× ×” ×•×‘×•×“×§×™× ×›×œ ××–×•×¨ ×‘× ×¤×¨×“

×‘×›×œ ×¤×¢× ×©×”×¤×™×œ×˜×¨ ×¢×•×‘×¨ ×¢×œ ××–×•×¨ ×‘×ª××•× ×”, ×”×•× ××—×©×‘ **×¢×“ ×›××” ×”×××¤×™×™×Ÿ ×”×–×” ×§×™×™× ×©×**  
×”×ª×•×¦××” ×”×™× **××¤×ª ×××¤×™×™× ×™×** ×©××“×’×™×©×” ××™×¤×” × ××¦××™× ×”×“×‘×¨×™× ×”×—×©×•×‘×™×

#### ğŸ”· MaxPooling2D â€“ ×©×›×‘×ª ××™×¦×•×¢

 ××§×¡×¤×•×œ2×“ ×©×›×‘×” ×©××—×¨×™ ×”×§×•× ×‘×•×œ×•×¦×™×”, ×•××˜×¨×ª×” **×œ×”×§×˜×™×Ÿ ××ª ×’×•×“×œ ×”× ×ª×•× ×™×**  
×‘××§×•× ×œ×©××•×¨ ××ª ×›×œ ×”×¢×¨×›×™× ×××¤×ª ×”×××¤×™×™× ×™×, ×”×™× ×œ×•×§×—×ª **××ª ×”×¢×¨×š ×”×›×™ ×’×‘×•×” ××›×œ ××–×•×¨ ×§×˜×Ÿ** (×œ××©×œ ×—×œ×•×Ÿ 2 ×¢×œ 2)

×œ××” ×–×” ×˜×•×‘?
- ×—×™×¡×›×•×Ÿ ×‘×—×™×©×•×‘×™×
- ×”×¤×—×ª×ª ×¡×™×›×•×Ÿ ×œ××•×‘×¨Ö¾×¤×™×˜×™× ×’
- ×©××™×¨×” ×¨×§ ×¢×œ ×”×××¤×™×™× ×™× ×”×—×–×§×™× ×‘×™×•×ª×¨

#### ğŸ¤” ××– ×œ××” ××©×ª××©×™× ×‘×–×” ×›××Ÿ?

×”××˜×¨×” ×©×œ× ×• ×”×™× ×œ×–×”×•×ª ×”×× ×ª××•× ×” ×”×™× ×©×œ **×›×œ×‘ ××• ×—×ª×•×œ**  
- Conv2D ×¢×•×–×¨×ª ×œ×–×”×•×ª ×ª×‘× ×™×•×ª ×—×©×•×‘×•×ª ×›××• ×¢×™× ×™×™×, ×¤×¨×•×•×”, ××•×–× ×™×™×...
- MaxPooling2D ×©×•××¨×ª ×¨×§ ××ª ××” ×©×—×©×•×‘ ×•××¤×©×˜×ª ××ª ×”×ª××•× ×”

×‘×™×—×“, ×”×Ÿ ×¢×•×–×¨×•×ª ×œ××•×“×œ ×œ×œ××•×“ **×™×™×¦×•×’ ×—×›× ×©×œ ×”×ª××•× ×”**, ×œ×¤× ×™ ×©×”×•× ××§×‘×œ ×”×—×œ×˜×” ×¡×•×¤×™×ª

#### ğŸ” Why `Sequential`?

`Sequential` is the simplest way to build a model in Keras  
It means the model is built **layer by layer in order**, where each layer has one input and one output  
Perfect for models with a straight flow like this CNN (no branching or merging layers)

#### ğŸ§  Why `filters=32` in `Conv2D`?

- `filters=32` means the layer will learn **32 different patterns (features)** from the image
- These filters might learn to detect:
  - edges
  - textures
  - curves
  - small details in the image
- More filters = more ability to extract different features
- 32 is a common default starting point â€” later layers could use more

These filters are **not predefined** â€” the model learns them from the data  
They are essentially **small weight matrices** (e.g. 3x3) that slide over the image and detect patterns

**ğŸ” Are They Different Types of Filters?**

Yes â€” each filter becomes specialized to detect different visual patterns, such as:

- **Edges** (horizontal, vertical, diagonal)
- **Textures**
- **Curved lines**
- **Color transitions**
- **Animal features** (fur, eyes, ears)
- **Background structures**

These filters start with **random weights**, but during training, backpropagation updates them so they become useful for the task (e.g. classifying cats vs dogs)

**ğŸ§  Why 32 Filters?**

Using 32 different filters gives the network **a broad set of feature detectors**  
- Each one captures a different aspect of the image  
- More filters = more expressive power (but also more computation)

#### ğŸ”² Why `kernel_size=3`?

- The kernel (filter) is the **window size** the layer uses to scan the image
- A size of `3x3` means each filter looks at a **3Ã—3 patch** of the image at a time
- Itâ€™s a standard choice: small enough to be efficient, large enough to capture patterns

#### âš¡ Why `activation='relu'`?

- `ReLU` stands for Rectified Linear Unit
- It replaces negative values with 0 and keeps positive ones
- This adds **non-linearity**, allowing the model to learn more complex patterns
- Itâ€™s fast and works very well in most CNNs

#### ğŸ“ Why `units=128` in `Dense`?

- After flattening, the dense (fully connected) layer learns **combinations of features**
- `128` is a typical choice â€” enough neurons to learn patterns, but not too large to overfit
- You can adjust this number for model performance and training speed

#### ğŸ¯ Why `units=1` with `activation='sigmoid'`?

- We're doing **binary classification**: cat (0) or dog (1)
- `units=1` means the output is a **single number between 0 and 1**
- `sigmoid` squashes the output into a **probability**
  - Closer to 1 â†’ more likely a dog
  - Closer to 0 â†’ more likely a cat

### âš™ï¸ ×§×•××¤×™×œ×¦×™×” ×•××™××•×Ÿ ×”××•×“×œ

```python
cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

cnn.fit(x=training_set, validation_data=test_set, epochs=25)
```

* `adam` â†’ ××œ×’×•×¨×™×ª× ××™××•×Ÿ ××ª×§×“×
* `binary_crossentropy` â†’ ×¤×•× ×§×¦×™×™×ª ×¢×œ×•×ª ×¢×‘×•×¨ ×¡×™×•×•×’ ×‘×™× ××¨×™
* `epochs=25` â†’ ×”×¨×©×ª ×ª×œ××“ ×‘××©×š 25 ××—×–×•×¨×™× ×¢×œ ×›×œ ×”×“××˜×”

×”×ª×”×œ×™×š ×¢×©×•×™ ×œ×§×—×ª ×›××” ×–××Ÿ ...

<img src="images/cnn15.jpg" style="width:100%"/>

### ğŸ¾ ×—×™×–×•×™ ×¢×œ ×ª××•× ×” ×—×“×©×”

```python
import numpy as np
from keras.preprocessing import image

# ×˜×•×¢× ×™× ××ª ×”×ª××•× ×”
img_path = '/content/dataset/CNN_dog_cat_dataset/dataset/single_prediction/cat_or_dog_1.jpg'
test_image = image.load_img(img_path, target_size=(64, 64))

img = image.load_img(img_path)
plt.imshow(img)
plt.axis('off')
plt.show()

# Convert image to array
test_image = image.img_to_array(test_image)

# Normalize pixel values (rescale to [0,1])
test_image = test_image / 255.0

# Expand dimensions to match the CNN model input shape
test_image = np.expand_dims(test_image, axis=0)

# Predict using the trained CNN model
result = cnn.predict(test_image)
print(result)

# Interpret the result
if result[0][0] > 0.5:
    prediction = 'dog'
else:
    prediction = 'cat'

print(prediction)
```

<img src="images/cnn17.png" style="width:40%"/>

<img src="images/cnn18.jpg" style="width:40%"/>

```python
import numpy as np
from keras.preprocessing import image

# ×˜×•×¢× ×™× ××ª ×”×ª××•× ×”
img_path = '/content/dataset/CNN_dog_cat_dataset/dataset/single_prediction/cat_or_dog_2.jpg'
test_image = image.load_img(img_path, target_size=(64, 64))

...

# Interpret the result
if result[0][0] > 0.5:
    prediction = 'dog'
else:
    prediction = 'cat'

print(prediction)
```

<img src="images/cat.jpg" style="width:40%"/>

×”×¡×‘×¨:

* `image.load_img` ×˜×•×¢×Ÿ ××ª ×”×ª××•× ×” ×•××›×•×•×¥ ××•×ª×” ×œ×’×•×“×œ ×”××ª××™×
* `img_to_array` ×××™×¨ ××ª ×”×ª××•× ×” ×œ××¢×¨×š ××¡×¤×¨×™
* ×—×™×œ×•×§ ×‘Ö¾255 ×× ×¨××œ ××ª ×¢×¨×›×™ ×”×¤×™×§×¡×œ×™× ×œ×˜×•×•×— \[0,1]
* `expand_dims` ××•×¡×™×£ ××™××“ ×›×“×™ ×œ×”×ª××™× ××ª ×”×¦×•×¨×” ×©××¦×¤×” ×œ×” ×”××•×“×œ (batch size)
* `cnn.predict` ××—×–×™×¨ ×”×¡×ª×‘×¨×•×ª ×©×”××•×‘×™×™×§×˜ ×”×•× ×›×œ×‘
* ×”×”×©×•×•××” ××•×œ 0.5 ×§×•×‘×¢×ª ××ª ×”×§×˜×’×•×¨×™×” ×”×¡×•×¤×™×ª

**ğŸ§© Why Do We Use `np.expand_dims` Before Prediction?**

CNN models expect all inputs to be in 4D: `(batch_size, height, width, channels)`  
If you forget to expand the dimensions, youâ€™ll get an error like:

> ValueError: Input 0 is incompatible with layer... expected ndim=4, found ndim=3

So `expand_dims` is essential for reshaping the image correctly before passing it to `model.predict()`

Before predicting, we typically load and preprocess a single image â€” this image will usually have a shape like:

(1, 64, 64, 3)

The extra dimension at the beginning (the `1`) represents the **batch size**

**Channels** ×”× ×©×›×‘×•×ª ×”×¦×‘×¢ ×©××¨×›×™×‘×•×ª ×›×œ ×¤×™×§×¡×œ ×‘×ª××•× ×”
×›×œ ×¤×™×§×¡×œ ×‘×¢×¦× ××—×–×™×§ ×›××” ×¢×¨×›×™× â€“ ××—×“ ×œ×›×œ ×¢×¨×•×¥ ×¦×‘×¢

[255, 120, 0]  â†’ Orange Pixel

**âœ… What does `np.expand_dims(test_image, axis=0)` do?**

It adds a new dimension **at position 0**, turning the shape from:

(64, 64, 3)

into:

(1, 64, 64, 3)

This allows the model to process the image as a batch of size 1


If `result[0][0] > 0.5` â†’ the model predicts **dog**, otherwise it predicts **cat**

This is because the output layer uses a **sigmoid activation function**, which returns a probability between 0 and 1:
- Values **closer to 1** indicate a higher confidence that the image is a dog
- Values **closer to 0** indicate a higher confidence that the image is a cat

<img src="images/cnn16.png" style="width:80%"/>