# ğŸ¤– ××›×•× ×ª ×•×§×˜×•×¨×™× ×ª×•××›×™× (SVM â€“ Support Vector Machine)

## ğŸ§  ××” ×–×” SVM?

SVM ×”×™× ×©×™×˜×ª **×œ××™×“×ª ××›×•× ×” ××•× ×—×™×ª (Supervised Learning)**, ×©× ×•×¢×“×”:
- **×œ×¡×•×•×’** ×“×•×’×××•×ª (Classification)
- ××• ×œ×‘×¦×¢ **×¨×’×¨×¡×™×”** (Regression â€“ ×—×™×–×•×™ ×¢×¨×›×™× ×¨×¦×™×¤×™×)

×”××˜×¨×” ×”×¢×™×§×¨×™×ª:  
**×œ××¦×•× ××ª ×”×’×‘×•×œ ×©××¤×¨×™×“ ×‘×¦×•×¨×” ×”×›×™ ×˜×•×‘×” ×‘×™×Ÿ ×§×‘×•×¦×•×ª ×©×•× ×•×ª ×©×œ ×“×•×’×××•×ª**

<img src="svm1.png" />

## ğŸ“ˆ ×©×™××•×©×™× × ×¤×•×¦×™× ×©×œ SVM

- **×–×™×”×•×™ ×˜×§×¡×˜ ×•×›×ª×‘ ×™×“**
- **×¡×™×•×•×’ ×ª××•× ×•×ª**
- **×–×™×”×•×™ ×¤× ×™×**
- **×—×™×–×•×™ ×‘××“×¢×™ ×”×¨×¤×•××”**
- **× ×™×ª×•×— ×¨×’×©×•×ª ×‘×˜×§×¡×˜**

 ×–×•×”×™ ×©×™×˜×” ×¢×•×¦××ª×™×ª ×œ×œ××™×“×ª ××›×•× ×”, ×•×”×™× ××©××©×ª ×‘××’×•×•×Ÿ ×¨×—×‘ ×©×œ ×ª×—×•××™×:

- âœï¸ **×–×™×”×•×™ ×˜×§×¡×˜ ×•×›×ª×‘ ×™×“**  
  ×¡×™×•×•×’ ×©×œ ××•×ª×™×•×ª ××• ×¡×¤×¨×•×ª, ×œ×“×•×’××” OCR (×–×™×”×•×™ ×ª×•×•×™× ××•×¤×˜×™)

- ğŸ–¼ï¸ **×¡×™×•×•×’ ×ª××•× ×•×ª**  
  ×œ×“×•×’××”: ×–×™×”×•×™ ×× ×ª××•× ×” ×”×™× ×©×œ ×—×ª×•×œ ××• ×›×œ×‘, ××• ×¦×™×œ×•× ×¨×¤×•××™ ×ª×§×™×Ÿ/×—×¨×™×’

- ğŸ™‚ **×–×™×”×•×™ ×¤× ×™×**  
  ×”×©×•×•××ª ×¤× ×™× ×œ×¦×•×¨×š ××™××•×ª ×–×”×•×ª

- ğŸ§¬ **×—×™×–×•×™ ×‘××“×¢×™ ×”×¨×¤×•××”**  
  ×œ×“×•×’××”: ×—×™×–×•×™ ×ª×•×¦××•×ª ×©×œ ×˜×™×¤×•×œ×™× ×¨×¤×•××™×™× ××• ××—×œ×•×ª

- ğŸ’¬ **× ×™×ª×•×— ×¨×’×©×•×ª ×‘×˜×§×¡×˜**  
  ×–×™×”×•×™ ×× ×”×•×“×¢×” ×”×™× ×—×™×•×‘×™×ª, ×©×œ×™×œ×™×ª ××• × ×™×™×˜×¨×œ×™×ª (Sentiment Analysis)

ğŸ’¡ *×œ××¨×•×ª ×”×”×ª×¤×ª×—×•×ª ×©×œ ×¨×©×ª×•×ª × ×•×™×¨×•× ×™×, SVM × ×©××¨ ×¨×œ×•×•× ×˜×™ ×‘××™×•×—×“ ×›×©×™×© ××¢×˜ ×“×•×’×××•×ª ×•×“××˜×” ××™×›×•×ª×™*

---

## ğŸ¯ ××” ×”××˜×¨×” ×©×œ SVM?

×œ××¦×•× ××ª **×”×§×•/××™×©×•×¨ (Hyperplane)** ×©××¤×¨×™×“ ×‘×¦×•×¨×” ××§×¡×™××œ×™×ª ×‘×™×Ÿ ×§×‘×•×¦×•×ª 
×”××˜×¨×” ×”×™× ×œ×”×’×“×™×œ ××ª ×”××¨×—×§ ××”×§×• ××œ ×”× ×§×•×“×•×ª ×”×§×¨×•×‘×•×ª ×‘×™×•×ª×¨ â€“ ×”× ×§×¨××•×ª **×•×§×˜×•×¨×™× ×ª×•××›×™×**

**×•×§×˜×•×¨×™× ×ª×•××›×™× â€“ Support Vectors**

×”× ×§×•×“×•×ª ×”×§×¨×•×‘×•×ª ×‘×™×•×ª×¨ ×œ×§×• ×”×”×¤×¨×“×”

- ×”×Ÿ ××œ×• ×©×§×•×‘×¢×•×ª ××ª ××™×§×•× ×”×§×•
- ×× ×ª×–×™×– × ×§×•×“×” ××—×¨×ª â€“ ×”×§×• ×œ× ×™×–×•×–
- ×× ×ª×–×™×– ×•×§×˜×•×¨ ×ª×•××š â€“ ×”×§×• ×™×©×ª× ×”

ğŸ“Œ ××œ×” "×”× ×§×•×“×•×ª ×”×—×©×•×‘×•×ª ×‘×™×•×ª×¨" ×‘××™××•×Ÿ ×©×œ SVM

<img src="svm2.png" />

**×œ××” "×•×§×˜×•×¨ ×ª×•××š" ×•×œ× "× ×§×•×“×” ×ª×•××›×ª"?**

×”×”×¡×‘×¨ ×”××ª××˜×™:

×‘××ª××˜×™×§×”, ×‘×¤×¨×˜ ×‘×’×™××•××˜×¨×™×” ×•×œ××™×“×ª ××›×•× ×”:

×›×œ × ×§×•×“×” ×‘××¨×—×‘ ××™×•×¦×’×ª ×›Ö¾×•×§×˜×•×¨

×œ×“×•×’××”: [3, 4] ×–×• × ×§×•×“×”, ××‘×œ ×’× ×•×§×˜×•×¨ ××”××•×¦× (0,0) ××œ [3, 4]

×›×œ×•××¨: × ×§×•×“×” = ×•×§×˜×•×¨ ×©××¨××” ×œ××Ÿ "×œ×”×’×™×¢" ××”×¨××©×™×ª

××– ×‘××•× ×—×™× ×©×œ ×œ××™×“×ª ××›×•× ×”:

×”× ×ª×•× ×™× ×©×œ× ×• ×”× ×•×§×˜×•×¨×™× ×‘××¨×—×‘

×•×œ×›×Ÿ ×’× ×”Ö¾Support Vectors ×”× ×•×§×˜×•×¨×™× ×©× ××¦××™× ×”×›×™ ×§×¨×•×‘ ×œ××™×©×•×¨ ×”×”×¤×¨×“×”

×•×œ××” "×ª×•××š"?
×›×™ ×”× ××œ×• ×©:

×ª×•××›×™× ×‘××™×§×•× ×©×œ ××™×©×•×¨ ×”×”×¤×¨×“×”

×›×œ×•××¨: ×”× ××œ×• ×©×§×•×‘×¢×™× ××•×ª×•

×× ×ª×–×™×– ××—×“ ××”× â€” ×”××™×©×•×¨ ×™×–×•×–!

---

## â“ ×œ××” × ×§×•×“×•×ª ×”×ª××™×›×” (Support Vectors) ×›×œ ×›×š ×—×©×•×‘×•×ª ×‘Ö¾SVM?
**×•×”×× ×”×§×• **×—×™×™×‘** ×œ×¢×‘×•×¨ ×“×¨×›×Ÿ?**


### âœï¸ ××”×™ ××©×•×•××ª ××™×©×•×¨?

××©×•×•××ª ××™×©×•×¨

```
wáµ€x + b = 0
```

- `w` ×”×•× **×•×§×˜×•×¨ ×”××©×§×œ×™×** (×”×›×™×•×•×Ÿ ×©×œ ×”××™×©×•×¨)
- `x` ×”×•× **×•×§×˜×•×¨ ×”× ×§×•×“×”** (×›×œ × ×§×•×“×” ×‘××¨×—×‘)
- `wáµ€x` ×–×” ×›××• ××›×¤×œ×” ×¡×§×œ×¨×™×ª: `wâ‚xâ‚ + wâ‚‚xâ‚‚ + ...`
- `b` ×”×•× **×”×™×¡×˜ (bias)** â€“ ×§×•×‘×¢ ××™×¤×” ×”××™×©×•×¨ × ×—×ª×š ×¢× ×”×¦×™×¨

ğŸ” ×“×•×’××” ×‘Ö¾2D:

× × ×™×—:

```
w = [2, -3]
b = 6
```

××– ××©×•×•××ª ×”××™×©×•×¨ ×”×™×:

```
2xâ‚ - 3xâ‚‚ + 6 = 0
```

×–×• ××©×•×•××ª ×§×• ×™×©×¨ ×‘Ö¾2D.

- ×× `wáµ€x + b > 0` â†’ ×”× ×§×•×“×” ×‘×¦×“ ××—×“ ×©×œ ×”××™×©×•×¨
- ×× `wáµ€x + b < 0` â†’ ×‘×¦×“ ×”×©× ×™
- ×× `wáµ€x + b = 0` â†’ ×”× ×§×•×“×” × ××¦××ª **×‘×“×™×•×§ ×¢×œ ×”××™×©×•×¨**

### ğŸ›¤ï¸ ××” ×–×” Hyperplane?

- ×‘Ö¾2D: ×§×• ×™×©×¨
- ×‘Ö¾3D: ××©×˜×—
- ×‘Ö¾4D ×•××¢×œ×”: ×¤×©×•×˜ × ×§×¨× "Hyperplane"

<img src="svm3.png" />

## âœ… ××” ×”×Ÿ × ×§×•×“×•×ª ×”×ª××™×›×”?

- × ×§×•×“×•×ª ×”×ª××™×›×” ×”×Ÿ **×”× ×§×•×“×•×ª ×”×§×¨×•×‘×•×ª ×‘×™×•×ª×¨ ×œ××™×©×•×¨ ×”××¤×¨×™×“**
- ×”×Ÿ ×™×•×©×‘×•×ª ×‘×“×™×•×§ **×¢×œ ×’×‘×•×œ×•×ª ×”×¨×•×•×—** (margin boundaries)
- ×”×Ÿ ××§×™×™××•×ª ××ª ×”×ª× ××™:

$$
y_i(w^T x_i + b) = 1 \quad \text{××•} \quad -1
$$

- `xáµ¢` â€” ×•×§×˜×•×¨ ×©×œ ×ª×›×•× ×•×ª (×¤×™×¦'×¨×™×) ×©×œ ×”× ×§×•×“×”
- `yáµ¢` â€” **×”×ª×•×•×™×ª ×”×××™×ª×™×ª** (×œ×™×™×‘×œ) ×©×œ ×”× ×§×•×“×”:
  - `+1` ×× ×”×™× ×©×™×™×›×ª ×œ××—×œ×§×” ××—×ª
  - `-1` ×× ×”×™× ×©×™×™×›×ª ×œ××—×œ×§×” ×”×©× ×™×™×”

### ğŸ“ ×”×× ×”×§×• ×—×™×™×‘ ×œ×¢×‘×•×¨ ×“×¨×›×Ÿ?

### ğŸ”¹ ×”××™×©×•×¨ ×”××¨×›×–×™ (×”×”×™×¤×¨Ö¾×¤×œ×™×™×Ÿ):

$$
w^T x + b = 0
$$

- **×œ× ×¢×•×‘×¨ ×“×¨×š × ×§×•×“×•×ª ×”×ª××™×›×”**
- ×”×•× ×¢×•×‘×¨ **×‘×××¦×¢** ×‘×™×Ÿ ×©×ª×™ ×”×§×‘×•×¦×•×ª


### ğŸ”¹ ×’×‘×•×œ×•×ª ×”×¨×•×•×— (margin boundaries):

- ×’×‘×•×œ ×¢×œ×™×•×Ÿ:

$$
w^T x + b = +1
$$

- ×’×‘×•×œ ×ª×—×ª×•×Ÿ:

$$
w^T x + b = -1
$$

âœ… **×›×Ÿ! ×’×‘×•×œ×•×ª ×”×¨×•×•×— ×—×™×™×‘×™× ×œ×¢×‘×•×¨ ×“×¨×š × ×§×•×“×•×ª ×”×ª××™×›×”.**

## ğŸ’¡ ×œ××” ×”×Ÿ ×”×›×™ ×—×©×•×‘×•×ª?

- ×›×™ ×¨×§ × ×§×•×“×•×ª ×”×ª××™×›×” ××©×¤×™×¢×•×ª ×¢×œ ×”×¤×ª×¨×•×Ÿ
- ×›×œ ×©××¨ ×”× ×§×•×“×•×ª **×œ× ××©× ×•×ª ××ª ×”××™×§×•× ×©×œ ×”×”×™×¤×¨Ö¾×¤×œ×™×™×Ÿ**
- ×”×¤×ª×¨×•×Ÿ ×©×œ \( w \) ××‘×•×¡×¡ ××š ×•×¨×§ ×¢×œ×™×”×Ÿ

$$
w = \sum_i \alpha_i y_i x_i
$$

×™×© ×•×§×˜×•×¨ ×ª×•××š ×¨×§ ×›××©×¨ Î±áµ¢ â‰  0

| ×¢×¨×š ×©×œ Î±áµ¢ | ××” ×–×” ××•××¨?                         |
|----------------------|--------------------------------------|
| Î±áµ¢ = 0     | ×”×“×•×’××” ×œ× ××©×¤×™×¢×” (×œ× ×•×§×˜×•×¨ ×ª×•××š)     |
| Î±áµ¢ > 0     | ×”×“×•×’××” ×›×Ÿ ××©×¤×™×¢×” (×•×§×˜×•×¨ ×ª×•××š)        |
| Î±áµ¢ = C     | ×”×“×•×’××” ×™×•×©×‘×ª ×¢×œ ×”×’×‘×•×œ ×”××•×ª×¨ ×‘Ö¾ Soft Margin |

## ğŸ§  ×¡×™×›×•× :

| ××¨×›×™×‘ | ×¢×•×‘×¨ ×“×¨×š × ×§×•×“×•×ª ×”×ª××™×›×”? |
|--------|---------------------------|
| ×”××™×©×•×¨ ×”××¨×›×–×™  wáµ€x + b = 0 | âŒ ×œ× ×—×™×™×‘ |
| ×’×‘×•×œ×•×ª ×”×¨×•×•×— wáµ€x + b = Â±1 | âœ… ×›×Ÿ, ×—×™×™×‘ |

×•×œ×›×Ÿ ×”×Ÿ × ×§×¨××•×ª **×•×§×˜×•×¨×™ ×ª××™×›×”** â€“ ×”×Ÿ ×××© **×ª×•××›×•×ª** ×‘××™×§×•× ×©×œ ×”×§×•! ğŸ’™


---

## ğŸ”“ ××¨×•×•×— ×¨×š â€“ Soft Margin ×•××¨×•×•×— ×§×©×” - Hard Margin

×‘×¢×•×œ× ×”×××™×ª×™ ×”× ×ª×•× ×™× ×œ× ×ª××™×“ ××•×¤×¨×“×™× ×‘×¦×•×¨×” ××•×©×œ××ª.

**Soft Margin:**
- ×××¤×©×¨ ×›××” ×˜×¢×•×™×•×ª ×§×˜× ×•×ª
- × ×•×ª×Ÿ ×œ××•×“×œ ×œ×”×™×•×ª **×’××™×© ×™×•×ª×¨**
- ×¢×•×–×¨ ×œ×× ×•×¢ **Overfitting**

**Hard Margin:**

×× ×™×—×™× ×©×”× ×ª×•× ×™× × ×™×ª× ×™× ×œ×”×¤×¨×“×” ×‘×¦×•×¨×” ××•×©×œ××ª â€“ ××™×Ÿ ×©×’×™××•×ª!

×›×œ×•××¨: ××¤×©×¨ ×œ××¦×•× ×§×• ×©××¤×¨×™×“ 100% × ×›×•×Ÿ ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª.

×××¤×™×™× ×™×:

×œ× ×××¤×©×¨ ××£ ×˜×¢×•×ª (××™×Ÿ × ×§×•×“×•×ª ×‘×¦×“ ×”×œ× × ×›×•×Ÿ)

×“×•×¨×© ×©×”× ×ª×•× ×™× ×™×”×™×• ×œ×™× ×™××¨×™×ª × ×¤×¨×“×™× (linearly separable)

×××•×“ ×¨×’×™×© ×œ×¨×¢×© â€” × ×§×•×“×” ××—×ª ×©×’×•×™×” ×™×›×•×œ×” ×œ×”×¨×•×¡ ×”×›×•×œ

ğŸ“‰ ××ª×™ ×œ×”×©×ª××©:

×¨×§ ×›×©××ª×” ×‘×˜×•×— ×©××™×Ÿ ×—×¤×™×¤×” ×‘×™×Ÿ ×”××—×œ×§×•×ª

×œ× ××ª××™× ×œ×¨×•×‘ ×”××§×¨×™× ×”×××™×ª×™×™×

<img src="svm4.png" style="width: 60%"/>

---

## âš™ï¸ ×¤×¨××˜×¨ C

×¤×¨××˜×¨ ×—×©×•×‘ ×××•×“ ×‘Ö¾SVM ×©××—×œ×™×˜ **×›××” × ××¤×©×¨ ×˜×¢×•×™×•×ª**:

| ×¢×¨×š C | ××” ×–×” ×¢×•×©×”? |
|-------|---------------|
| ×’×‘×•×”  | ×¤×—×•×ª ×¡×œ×—× ×™ ×œ×˜×¢×•×™×•×ª (××•×“×œ ×§×©×™×—, ×¤×—×•×ª ×’××™×©) |
| × ××•×š  | ×¡×œ×—× ×™ ×™×•×ª×¨ â€“ ×××¤×©×¨ ×©×’×™××•×ª ×§×˜× ×•×ª (××•×“×œ ×›×œ×œ×™ ×™×•×ª×¨) |

<img src="svm5.png" style="width: 80%"/>

---

## ×“×•×’×× ×‘×¤×™×™×˜×•×Ÿ

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm

# Create a simple dataset for apples and bananas
# Using two features: sweetness (x-axis) and weight (y-axis)
apples = np.array([[3, 150], [4, 130], [2, 160], [3, 140], [3.5, 145]])
bananas = np.array([[7, 120], [6, 110], [8, 115], [7.5, 125], [6.5, 118]])

# Combine features and create labels (-1 for apples, 1 for bananas)
X = np.vstack([apples, bananas])
y = np.array([-1, -1, -1, -1, -1, 1, 1, 1, 1, 1])

# Create and train the SVM model
# Using a linear kernel for simplicity
clf = svm.SVC(kernel='linear', C=1000)  # clf=classifier
clf.fit(X, y)

# Extract the model parameters
w = clf.coef_[0]  # The weights (normal vector to the hyperplane)
b = clf.intercept_[0]  # The bias (b in wÂ·x + b = 0)
print(f"Model weights (w): {w}")
print(f"Model bias (b): {b}")

# Create a new test point
test_point = np.array([[5, 135]])  # A point with sweetness=5, weight=135
prediction = clf.predict(test_point)[0]
class_name = "Banana" if prediction == 1 else "Apple"
print(class_name)
```

<img src="svm7.png" style="width: 80%" />

Output:
```
Model weights (w): [ 0.18791946 -0.26845638]
Model bias (b): 33.14765100671113
Apple

Decision boundary equation: 0.19*x1 + -0.27*x2 + 33.15 = 0
```

## ğŸŒŒ ×’×¨×¢×™×Ÿ â€“ Kernel

×›××©×¨ ×”× ×ª×•× ×™× **×œ× × ×™×ª× ×™× ×œ×”×¤×¨×“×” ×‘×§×• ×™×©×¨**, × ×©×ª××© ×‘×§×¨× ×œ×™× ×›×“×™ ×œ×”×¤×•×š ××ª ×”××¨×—×‘:

- × ×‘×¦×¢ **××™×¤×•×™ ×œÖ¾××¨×—×‘ ×—×“×©** (×œ×¨×•×‘ ×’×‘×•×” ×™×•×ª×¨)
- ×‘××¨×—×‘ ×”×—×“×© â€“ ×›×Ÿ × ×™×ª×Ÿ ×œ×”×¤×¨×™×“ ×‘×™× ×™×”× ×‘×§×• ×™×©×¨!

<img src="svm6.png" style="width: 80%"/>



## ğŸ© Kernel Trick

"×˜×¨×™×§ ××ª××˜×™" ×©×××¤×©×¨:
- ×œ×—×©×‘ ××ª **×”××›×¤×œ×” ×”×¤× ×™××™×ª ×‘××¨×—×‘ ×”×—×“×©**
- ×‘×œ×™ ×‘×××ª ×œ×—×©×‘ ××ª ×”××™×§×•× ×”×—×“×© ×©×œ ×›×œ × ×§×•×“×”!

×–×” ×—×•×¡×š **×”×¨×‘×” ×××•×“ ×–××Ÿ ×•×–×™×›×¨×•×Ÿ**.


## ğŸ“Œ ×¡×•×’×™ Kernels × ×¤×•×¦×™×:

- **Linear** â€“ ××ª××™× ×›×©××¤×©×¨ ×œ×”×¤×¨×™×“ ×‘×§×• ×™×©×¨
- **Polynomial** â€“ ××ª××™× ×›×©×™×© ×§×©×¨×™× ××•×¨×›×‘×™×
- **RBF (Gaussian)** â€“ ×‘×¨×™×¨×ª ××—×“×œ, ××ª××™× ×œ×”×¨×‘×” ×‘×¢×™×•×ª
- **Sigmoid** â€“ ×›××• × ×•×™×¨×•× ×™× ×‘×¨×©×ª ×¢×¦×‘×™×ª


×“×•×’×× ×œ×”×¤×¨×“×” ×œ×¡×•×’×™ ×¤×¨×—×™× ×©×•× ×™× ×¢×œ ×¤×™ ××•×¨×š ×¢×œ×™ ×”×›×•×ª×¨×ª SEPAL
<img src="svm9.png" style="width: 80%" />

---

## ğŸ” ×“×•×’×× - ×‘×¢×™×™×ª XOR

×‘×¢×™×™×ª XOR ×”×™× ×“×•×’×× ×§×œ××¡×™×ª ×œ× ×ª×•× ×™× ×©×œ× × ×™×ª× ×™× ×œ×”×¤×¨×“×” ×‘×§×• ×™×©×¨:

<img src="svm17.png" style="width: 70%" />

### ×¢× ×§×¨× ×œ RBF × ×™×ª×Ÿ ×œ×¤×ª×•×¨ ××ª ×‘×¢×™×™×ª XOR:

```python
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import numpy as np

# ×¡×˜ × ×ª×•× ×™× ×©×œ XOR
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 1, 1, 0])

# ×™×¦×™×¨×ª ××•×“×œ ×¢× ×§×¨× ×œ RBF
model = SVC(kernel='rbf')
model.fit(X, y)

# ×”×“××™×”
h = 0.01
x_min, x_max = -0.5, 1.5
y_min, y_max = -0.5, 1.5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
plt.title('Solving XOR using RBF')
plt.show()
```



××‘× ×™× ×©×œ× × ×™×ª× ×™× ×œ×—×œ×•×§×” ×¢×œ ×™×“×™ ×§×• ×™×©×¨:

![XOR Problem](https://miro.medium.com/max/1400/1*_7OPgojau8hkiPUiHoGK_w.png)

---

## ×—×œ×•×§×” ×œ train-test ×•×‘×“×™×§×ª ×“×™×•×§

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
import seaborn as sns

# Create a dataset with 20 points manually
# Class 0: Points forming a cluster on the left
# Class 1: Points forming a cluster on the right
X = np.array([
    # 10 points for Class 0
    [2, 3], [1, 2], [2, 2], [1, 3], [3, 2],
    [2, 1], [1, 1], [3, 3], [2.5, 2], [1.5, 2.5],
    
    # 10 points for Class 1
    [6, 5], [5, 6], [7, 6], [6, 7], [5, 5],
    [7, 5], [6, 6], [5, 7], [7, 7], [6, 4]
])

# Create labels (0 for first 10 points, 1 for last 10 points)
y = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])

# Split the data into training and testing sets (70% train, 30% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Scale the features for better SVM performance
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create and train the SVM model
clf = svm.SVC(kernel='linear', C=1.0)
clf.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Print performance metrics
print(f"Model Accuracy: {accuracy:.2f}")
print("\nConfusion Matrix:")
print(conf_matrix)
print("\nClassification Report:")
print(class_report)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Class 0', 'Class 1'],
            yticklabels=['Class 0', 'Class 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.tight_layout()
plt.show()
```

<img src="svm12.png" style="width: 70%" />

<img src="svm13.png" style="width: 70%" />

Output:
```
Model Accuracy: 1.00

Confusion Matrix:
[[3 0]
 [0 3]]

Classification Report:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00         3
           1       1.00      1.00      1.00         3

    accuracy                           1.00         6
   macro avg       1.00      1.00      1.00         6
weighted avg       1.00      1.00      1.00         6
```

## ×©×™××•×© ×‘- GridSearchCV ×œ××¦×™××ª ×”×¤×¨××˜×¨×™× ×”××™×“×™××œ×™×™×

The **gamma parameter** is a crucial hyperparameter in SVM that determines the influence of individual training examples. It's primarily used in non-linear kernels like 

RBF (Radial Basis Function), polynomial, and sigmoid

High gamma values (e.g., 10, 100):

Creates a decision boundary with "tight" curves around individual data points

Low gamma values (e.g., 0.001, 0.01):

Creates a decision boundary with smoother, wider curves

<img src="svm15.png" style="width: 70%" />

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
import seaborn as sns
from time import time

# Load the digits dataset (larger dataset with 1797 samples, 64 features, 10 classes)
digits = datasets.load_digits()
X = digits.data
y = digits.target

print(f"Dataset shape: {X.shape} - {X.shape[0]} samples, {X.shape[1]} features")
print(f"Number of classes: {len(np.unique(y))}")

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define a smaller parameter grid to make computation more manageable
# but still diverse enough to show differences between kernels
param_grid = {
    'C': [0.1, 1, 10],
    'gamma': ['scale', 0.01, 0.1],  # scale gamma:  1 / (n_features * [variance of X.var])
    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],
    'degree': [2, 3]  # Only relevant for poly kernel
}

# Create an SVM classifier
svm_clf = svm.SVC(probability=True, random_state=42)

# Set up GridSearchCV
print("Starting grid search. This might take a few minutes with this larger dataset...")
start_time = time()
grid_search = GridSearchCV(
    estimator=svm_clf,
    param_grid=param_grid,
    cv=3,  # Reduce to 3-fold cross-validation for speed
    n_jobs=-1,  # Use all available cores
    verbose=1,
    scoring='accuracy',
    return_train_score=True
)

# Perform the grid search
grid_search.fit(X_train_scaled, y_train)
search_time = time() - start_time

# Print the best parameters
print(f"\nGrid search completed in {search_time:.2f} seconds")
print(f"Best parameters: {grid_search.best_params_}")

```

<img src="svm14.png" style="width: 70%" />

Output:
```
Dataset shape: (1797, 64) - 1797 samples, 64 features
Number of classes: 10
Starting grid search. This might take a few minutes with this larger dataset...
Fitting 3 folds for each of 72 candidates, totalling 216 fits

Grid search completed in 5.38 seconds
Best parameters: {'C': 10, 'degree': 2, 'gamma': 0.01, 'kernel': 'rbf'}
```


