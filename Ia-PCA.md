
## PCA - Principal Component Analysis

שיטת **PCA** (ניתוח רכיבים עיקריים) היא שיטה נפוצה בלמידת מכונה וסטטיסטיקה שנועדה לבצע **צמצום ממדים** – כלומר, להפוך דאטה עם הרבה תכונות (Features) לדאטה עם פחות תכונות, מבלי לאבד יותר מדי מידע חשוב.

פי.סי.איי יוצרת **ציר חדש** שמורכב משילוב של התכונות המקוריות – הציר הזה נקרא **רכיב עיקרי** (Principal Component) – והוא משמר את מירב השונות בדאטה.

### למה להשתמש ב־PCA?

- דאטה עם הרבה תכונות גורם לבעיות: **עומס חישובי**, **קושי בויזואליזציה**, ו־**הסתברות לאוברפיטינג**
- PCA מצמצם את מספר התכונות, אבל שומר על כמה שיותר **שונות** מהדאטה המקורי

שונות (Variance) היא מדד שמראה **עד כמה ערכים מתרחקים מהממוצע שלהם**
כלומר – כמה הנתונים "מתפזרים" סביב הממוצע

## PCA and Unsupervised Learning

ב־Unsupervised Learning אין לנו תוויות (labels), ולכן אין דרך לדעת איזו תכונה הכי חשובה
PCA מאפשרת לצמצם תכונות **באופן חכם ולא מפוקח** – בלי לדעת מה חשוב למה

המטרה היא להקרין את הדאטה למרחב חדש עם פחות ממדים, תוך כדי שמירה על כמה שיותר מהמידע (variance)

### שים לב:
פי.סי.איי לא בוחר תכונות קיימות – הוא מייצר **חדשות** שהן שילובים ליניאריים של הישנות

## Principal Components

הרכיבים העיקריים הם צירים חדשים שנוצרים משילוב של התכונות המקוריות
כל רכיב הוא **ציר אורתוגונלי** לרכיבים האחרים, ומכיל כמה שיותר שונות (variance)

<img src="images/pca1.png" style="width: 90%" />

## האם PC1 ו־PC2 הם וקטורים?

כן  
כל רכיב עיקרי (PC) הוא וקטור  

### תכונות של הרכיבים

- PC1 is a vector pointing in the direction of the greatest variance in the data  
- PC2 is a vector orthogonal to PC1 that explains the remaining variance  
- All principal components are orthogonal to each other  
- All components are normalized – meaning their length is 1

### מה קורה כש־PC1 הוא הווקטור [0.7, 0.7]

אם הווקטור של PC1 הוא [0.7, 0.7] זה אומר שהכיוון החדש עובר בזווית של 45 מעלות  
זה הכיוון שבו הנתונים מתפזרים הכי הרבה  

### הקרנת נקודה על הכיוון הזה

נניח שהנקודה היא [90, 70]  
נחשב את ההקרנה שלה על הציר החדש:

$$
projection = [90, 70] \cdot [0.7, 0.7] = 90×0.7 + 70×0.7 = 63 + 49 = 112
$$

הנקודה נשמרת כ־112 על הציר החדש PC1  

### הקרנת כל הדאטה

- מקרינים את כל הנקודות על הווקטור [0.7, 0.7]  
- כל נקודה נופלת על ערך אחד בציר החדש  
- מתקבלת רשימה של ערכים כמו: 112, 120, 95, 134, 108 ...

### חישוב שונות

עכשיו מחשבים את השונות של הערכים האלה  
אם הכיוון שנבחר הוא באמת PC1  
נקבל את השונות הכי גבוהה שאפשר לקבל מכל כיוון אחר

### מסקנה

פי.סי.איי בוחר את הכיוון שבו ההקרנות של כל הנקודות הכי שונות אחת מהשנייה  
כלומר יש שם הכי הרבה מידע

## Explained Variance

כשאנחנו עושים PCA, אנחנו רוצים לדעת כמה מידע (שונות) נשאר אחרי שהפכנו את הדאטה למשהו פשוט יותר

שונות (Variance) היא מדד שמראה **עד כמה ערכים מתרחקים מהממוצע שלהם**
כלומר – כמה הנתונים "מתפזרים" סביב הממוצע

### הנוסחא של שונות:

$$
\text{Variance} = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2
$$

זו בעצם **הממוצע של ריבועי המרחקים מהממוצע**

אקספיריאנס ואריאנס זה כמה מתוך כל השונות בדאטה נשמרה בכל רכיב חדש שיצרנו
אם רכיב עיקרי מסביר הרבה שונות – זה אומר שהוא שומר הרבה מידע מהדאטה המקורי

### דוגמה:
נניח שיש לנו ציונים של 100 תלמידים במתמטיקה, אנגלית ומדעים  
PCA יוצר רכיבים חדשים במקום שלושת המקצועות – כדי לייצג את התלמידים עם פחות מידע

### האם יוצרים רכיב אחד במקום שלושה ציונים?

כן! PCA יכול לקחת שלושה ציונים (למשל: מתמטיקה, אנגלית, מדעים) ולהמיר אותם לרכיב אחד חדש בלבד  
המטרה היא לייצג את ההבדלים בין התלמידים בצורה פשוטה, בלי לאבד הרבה מידע

### איך זה עובד?

במקום לרשום לכל תלמיד את שלושת הציונים שלו:
- מתמטיקה: 85  
- אנגלית: 90  
- מדעים: 80  

פי.סי.איי יוצר רכיב חדש שהוא שילוב מתמטי של שלושת הציונים האלו, למשל:  
**0.5 × מתמטיקה + 0.3 × אנגלית + 0.2 × מדעים = 85.2**

### למה זה טוב?

- כי הרכיב החדש שומר את עיקר ההבדלים בין התלמידים  
- זה עוזר לצמצם ממדים ולעבוד עם פחות נתונים  
- קל יותר לנתח ולהציג גרפים  
- מאפשר קלאסטרינג או חיזוי מדויק יותר

### מציאת היחסים

איך PCA יודע לבחור את המשקלים (למשל 0.5, 0.3, 0.2) בכל רכיב? התשובה: הוא לא מנחש – הוא מחשב אותם מתמטית לפי הכיוון שבו השונות של הדאטה הכי גדולה

💡 הרעיון המרכזי
PCA מחפש כיוונים חדשים (וקטורים) שבהם הנתונים מתפזרים הכי הרבה
הכיוון שבו הנתונים משתנים הכי הרבה – הוא הרכיב הראשון (PC1)
והכיוון הבא אחריו, שהוא מאונך לו, הוא PC2 וכן הלאה

### האם כל הרכיבים מאונכים זה לזה? 

כן  
ב־PCA כל רכיב חדש שנוצר הוא תמיד מאונך לרכיבים הקודמים  

### מה זה אומר בפועל

אם יש שלושה משתנים כמו מתמטיקה אנגלית ומדעים  
לצורך הדוגמא נאמר שהחלטנו להשתמש בשלושה רכיבים

- PC1 is chosen as the direction with the **maximum variance** in the data  
- PC2 is chosen to be **orthogonal to PC1** and explains as much of the **remaining variance** as possible  
- PC3 is chosen to be **orthogonal to both PC1 and PC2** and explains the **variance that still remains**

כל ציר חדש בזווית של 90 מעלות לצירים הקודמים  
זה אומר שהוא לא חופף להם בשום צורה

### חשוב לדעת:

- לפעמים מספיק לשמור **רכיב אחד** שמכיל למשל 90% מהשונות  
- לפעמים שומרים 2 או 3 רכיבים – תלוי כמה מידע רוצים לשמר

### 🎓 דוגמה מספרית לשונות מוסברת (Explained Variance) ב־PCA

נניח שיש לך דאטה עם שני משתנים:
- גובה (בס"מ)
- משקל (בק"ג)

מדדת 10 אנשים וקיבלת שונות (Variance) מסוימת בכל אחד מהמשתנים בנפרד:

- שונות של **גובה**: 100 (למשל, סטיית תקן בערך 10 ס"מ)
- שונות של **משקל**: 25 (למשל, סטיית תקן בערך 5 ק"ג)

אם נחבר את השונות של שני המשתנים ביחד, נקבל את **השונות הכוללת**:

$$
100 + 25 = 125
$$


כעת, ביצעת PCA ויצרת שני רכיבים חדשים (**Principal Components**):

- הרכיב הראשון (PC1) הוא ציר חדש בדאטה, שמנסה "לתפוס" כמה שיותר מתוך השונות הכוללת
- הרכיב השני (PC2) הוא ציר חדש שמנסה לתפוס את השונות שנשארה אחרי שהראשון תפס

### 📈 שונות מוסברת לפי רכיב:

| רכיב | שונות מוסברת |
|------|---------------|
| PC1  | 100           |
| PC2  | 25            |

### חישוב:

- הרכיב הראשון (PC1) הצליח להסביר:

$$
\frac{100}{125} = 0.8 = 80\%
$$

כלומר, הרכיב הראשון שומר **80% מהמידע המקורי**, כי הוא "הצליח" לתפוס את רוב השונות שהייתה בדאטה.

- הרכיב השני (PC2) הסביר את השונות שנותרה:

$$
\frac{25}{125} = 0.2 = 20\%
$$

### ❗ למה זה חשוב?

אם נבחר רק את PC1, נפסיד רק **20% מהמידע**, ונוכל לעבוד עם משתנה יחיד במקום שני משתנים

לעומת זאת, אם PC1 היה תופס רק **40%** מהשונות, היינו מאבדים **60%** מהמידע אם היינו משתמשים רק ברכיב אחד


### האם שני הרכיבים ביחד מסבירים את כל השונות

כאשר יש שני משתנים בדאטה אפשר ליצור לכל היותר שני רכיבים עיקריים ב־PCA  
כל השונות בדאטה יכולה להתחלק רק בין שני הרכיבים האלה  

לכן:

$$
\text{Explained Variance of PC1} + \text{Explained Variance of PC2} = 100\%
$$

### בדוגמה שלנו

- PC1 מסביר 80%  
- PC2 מסביר 20%  

יחד:

\[
80\% + 20\% = 100\%
\]

אם משתמשים בשני הרכיבים יחד לא מאבדים שום מידע  
אם משתמשים רק ברכיב אחד שומרים רק את האחוז מהשונות שהוא מסביר

### האם באמת חסכנו משהו כשיצרנו 2 רכיבים מ־2 משתנים

אם הדאטה התחיל עם שני משתנים וביצענו PCA ויצרנו שני רכיבים  
אז לא באמת צמצמנו את המימדיות  
רק סובבנו את הצירים – לא דחסנו את הדאטה  

### אז למה בכל זאת עושים את זה

- מאפשר להבין טוב יותר את המבנה של הדאטה  
  לדוגמה PC1 מראה את הכיוון שבו הנתונים מתפזרים הכי הרבה  
  זה יכול ללמד משהו על הקשרים בין המשתנים  

- מאפשר להסתפק רק ברכיב אחד  
  אם PC1 מסביר נגיד 95% מהשונות אפשר להתעלם מ־PC2  
  וזה כן חיסכון אמיתי – הופכים את הדאטה לחד־ממדי  

- שימוש בהדמיה  
  אם מתחילים עם הרבה משתנים (למשל 10)  
  אפשר לצמצם ל־2 רכיבים ולהציג את הדאטה על גרף דו־ממדי מובן

