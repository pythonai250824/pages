# KNN - ×ª×¨×—×™×©×™ ×ª×™×§×• ×•×©×™×˜×•×ª ××ª×§×“××•×ª

## ×˜×™×¤×•×œ ×‘××§×¨×™ ×ª×™×§×•

××—×ª ×”×‘×¢×™×•×ª ×©×¢×œ×•×œ×•×ª ×œ×”×ª×¢×•×¨×¨ ×‘××œ×’×•×¨×™×ª× KNN ×”×™× ×›××©×¨ ×§×™×™× ×ª×™×§×• (tie) ×‘×™×Ÿ ××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª. ××¦×‘ ×–×” ××ª×¨×—×© ×›××©×¨ K ×©×›× ×™× ××ª×—×œ×§×™× ×‘××•×¤×Ÿ ×©×•×•×” ×‘×™×Ÿ ×©×ª×™ ×§×˜×’×•×¨×™×•×ª ××• ×™×•×ª×¨

**×“×•×’××”**: × × ×™×— ×©×‘×—×¨× ×• K=4 ×•××¦×× ×• ×©××ª×•×š ××¨×‘×¢×ª ×”×©×›× ×™× ×”×§×¨×•×‘×™× ×‘×™×•×ª×¨, 2 ×”× ×ª×¤×•×—×™× ×•-2 ×”× ×ª×¤×•×–×™×. ××™×š × ×—×œ×™×˜ ×œ××™×–×• ×§×˜×’×•×¨×™×” ×œ×¡×•×•×’ ××ª ×”×¤×¨×™ ×”×—×“×©?

### ××¡×˜×¨×˜×’×™×•×ª ×œ×˜×™×¤×•×œ ×‘×ª×™×§×•:

1. **×‘×—×™×¨×ª K ××™-×–×•×’×™**: ×”×©×™×˜×” ×”×¤×©×•×˜×” ×‘×™×•×ª×¨ ×”×™× ×œ×”×©×ª××© ×‘-K ××™-×–×•×’×™ (3, 5, 7, ×•×›×•') ×›×“×™ ×œ×× ×•×¢ ×ª×™×§×• ××œ×›×ª×—×™×œ×”. ×–×” ×¢×•×‘×“ ×”×™×˜×‘ ×¢×‘×•×¨ ×‘×¢×™×•×ª ×¡×™×•×•×’ ×‘×™× ××¨×™ (×©×ª×™ ×§×˜×’×•×¨×™×•×ª)

2. **×©×§×œ×•×œ ×œ×¤×™ ××¨×—×§**: ×‘××§×•× ×œ×ª×ª ×œ×›×œ ×©×›×Ÿ ××©×§×œ ×–×”×”, × ×©×§×œ×œ ××ª ×”×§×‘×•×¦×” ×©×œ ×”× ×§×•×“×” ×œ×¤×™ 1 ×—×œ×§×™ d, ×›××©×¨ d ×”×•× ×”××¨×—×§ ××”× ×§×•×“×” ×”×—×“×©×”. ×›×š, ×©×›× ×™× ×§×¨×•×‘×™× ×™×•×ª×¨ ××§×‘×œ×™× ×”×©×¤×¢×” ×’×“×•×œ×” ×™×•×ª×¨:

$$\text{××©×§×œ ×”×©×›×Ÿ} = \frac{1}{d(x, x_i)}$$

×“×•×’××” ××¡×¤×¨×™×ª â€“×œ×¤×™ 

$$
\frac{1}{d}
$$

× × ×™×— ×©×”×©×ª××©× ×• ×‘-K=4 ×•×”×©×›× ×™× ×”×§×¨×•×‘×™× ×‘×™×•×ª×¨ ×œ×¤×¨×™ ×—×“×© ×”×:

| Neighbor | Class     | Distance from new fruit \( d \) | Weight $\frac{1}{d}$ |
|----------|-----------|-------------------------------|--------------------------|
| 1        | Apple ğŸ   | 1.0                           | 1.00                     |
| 2        | Apple ğŸ   | 2.0                           | 0.50                     |
| 3        | Orange ğŸŠ  | 3.0                           | 0.33                     |
| 4        | Orange ğŸŠ  | 4.0                           | 0.25                     |

- **×ª×¤×•×— ğŸ**:  
  \( 1.00 + 0.50 = 1.50 \)

- **×ª×¤×•×– ğŸŠ**:  
  \( 0.33 + 0.25 = 0.58 \)

### ×œ××” ×“×•×•×§× 1 ×—×œ×§×™ d

×›×™ ×‘×’×™×©×” ×©×œ ×¡×›×•× ××¨×—×§×™× ×™×™×ª×›×Ÿ ××¦×‘ ×©×”×©×›× ×™× ×©×œ ××—×œ×§×” ××¡×•×™××ª ×¨×—×•×§×™× ×™×•×ª×¨, ××‘×œ ×‘×’×œ×œ ×©×”××¨×—×§×™× ×©×œ ×”×§×‘×•×¦×” ×”×©× ×™×™×” ×™×•×ª×¨ ××¤×•×–×¨×™×, ×”×™× ×ª× ×¦×— ×‘×˜×¢×•×ª
××‘×œ ×¢× ××©×§×œ×™× ×©×œ 1 ×—×œ×§×™ d, ×”×©×›×Ÿ ×”×›×™ ×§×¨×•×‘ ××©×¤×™×¢ ×—×–×§ ×××•×“ â€” ×›××• ×©×¦×¨×™×š

**××¡×§× ×”:**

×œ××¨×•×ª ×©×™×© ×ª×™×§×• ×‘××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª (2 ×ª×¤×•×—×™×, 2 ×ª×¤×•×–×™×),  
×›××©×¨ ××‘×¦×¢×™× ×©×§×œ×•×œ ×œ×¤×™ ×”××¨×—×§ â€“ ×”×©×›× ×™× ×”×§×¨×•×‘×™× ××©×¤×™×¢×™× ×™×•×ª×¨,  
×•×œ×›×Ÿ ×”×§×˜×’×•×¨×™×” **×ª×¤×•×—** ×× ×¦×—×ª

3. **×”×•×¨×“×ª K ×‘-1**: ×× ××ª×¨×—×© ×ª×™×§×•, × ×™×ª×Ÿ ×œ×”×§×˜×™×Ÿ ××ª K ×‘××•×¤×Ÿ ×–×× ×™ ×‘-1 ×•×œ×‘×“×•×§ ×× ×”×ª×™×§×• × ×¤×ª×¨

4. **×”×¢×“×¤×ª ×§×˜×’×•×¨×™×”**: ×× ×™×© ×¡×™×‘×” ×œ×•×’×™×ª ×œ×”×¢×“×™×£ ×§×˜×’×•×¨×™×” ××¡×•×™××ª (×œ××©×œ ×§×˜×’×•×¨×™×” ×©×›×™×—×” ×™×•×ª×¨), × ×™×ª×Ÿ ×œ×”×©×ª××© ×‘×” ×›××›×¨×™×¢×” ×‘××§×¨×™ ×ª×™×§×•

5. **×‘×—×™×¨×” ××§×¨××™×ª**: ×‘×—×™×¨×” ××§×¨××™×ª ×©×œ ××—×ª ××”×§×˜×’×•×¨×™×•×ª ×”×©×•×•×ª

×“×•×’××ª ×§×•×“ ×œ×˜×™×¤×•×œ ×‘×ª×™×§×• ×‘×××¦×¢×•×ª ×©×§×œ×•×œ ××¨×—×§×™×:

```python
def weighted_knn_predict(X_train, y_train, x_new, k):
    # ×—×™×©×•×‘ ××¨×—×§×™× ×‘×™×Ÿ ×”× ×§×•×“×” ×”×—×“×©×” ×œ×›×œ × ×§×•×“×•×ª ×”××™××•×Ÿ
    distances = []
    for i, x_train in enumerate(X_train):
        dist = np.sqrt(np.sum((x_train - x_new) ** 2))
        distances.append((dist, i))
    
    # ××™×•×Ÿ ×”××¨×—×§×™× ×•×‘×—×™×¨×ª K ×”×§×¨×•×‘×™× ×‘×™×•×ª×¨
    distances.sort()
    k_nearest = distances[:k]
    
    # ×©×§×œ×•×œ ×”×¦×‘×¢×•×ª ×œ×¤×™ ×”××¨×—×§
    class_votes = {}
    for dist, idx in k_nearest:
        weight = 1.0 / max(dist, 0.000001)  # ×× ×™×¢×ª ×—×œ×•×§×” ×‘××¤×¡
        vote = y_train[idx]
        
        if vote in class_votes:
            class_votes[vote] += weight
        else:
            class_votes[vote] = weight
    
    # ×‘×—×™×¨×ª ×”×§×˜×’×•×¨×™×” ×¢× ×”×¦×™×•×Ÿ ×”××©×•×§×œ×œ ×”×’×‘×•×” ×‘×™×•×ª×¨
    return max(class_votes.items(), key=lambda x: x[1])[0]
```

## ××“×“×™ ×”×¢×¨×›×”: Precision, Recall, F1, Support

×œ×”×¢×¨×›×ª ×‘×™×¦×•×¢×™ ××•×“×œ KNN, ××©×ª××©×™× ×‘×›××” ××“×“×™× ×¡×˜× ×“×¨×˜×™×™×:

### 1. Precision (×“×™×•×§)
××•×“×“ ××ª ××—×•×– ×”× ×™×‘×•×™×™× ×”×—×™×•×‘×™×™× ×©×”×™×• × ×›×•× ×™× ×‘×××ª:

$$\text{Precision} = \frac{TP}{TP + FP}$$

×›××©×¨:
- TP (True Positive): ×—×™×–×•×™ ×—×™×•×‘×™ × ×›×•×Ÿ
- FP (False Positive): ×—×™×–×•×™ ×—×™×•×‘×™ ×©×’×•×™

×“×™×•×§ ×’×‘×•×” ××©××¢×•×ª×• ×©×›××©×¨ ×”××•×“×œ ×× ×‘× '×›×Ÿ', ×”×•× ×‘×“×¨×š ×›×œ×œ ×¦×•×“×§

### 2. Recall (×›×™×¡×•×™ ××• ×¨×’×™×©×•×ª)
××•×“×“ ××™×–×” ××—×•×– ××”××§×¨×™× ×”×—×™×•×‘×™×™× ×‘×××ª ×”××•×“×œ ×”×¦×œ×™×— ×œ×–×”×•×ª:

$$\text{Recall} = \frac{TP}{TP + FN}$$

×›××©×¨:
- FN (False Negative): ×—×™×–×•×™ ×©×œ×™×œ×™ ×©×’×•×™

×›×™×¡×•×™ ×’×‘×•×” ××©××¢×•×ª×• ×©×”××•×“×œ ××–×”×” ××ª ×¨×•×‘ ×”××§×¨×™× ×”×—×™×•×‘×™×™× ×”×××™×ª×™×™×

### 3. F1 Score
×××•×¦×¢ ×”×¨××•× ×™ ×©×œ Precision ×•-Recall, ×××–×Ÿ ×‘×™×Ÿ ×©× ×™ ×”××“×“×™×:

$$\text{F1} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$$

×’×‘×•×” ××©×§×£ ××™×–×•×Ÿ ×˜×•×‘ ×‘×™×Ÿ ×“×™×•×§ ×œ×›×™×¡×•×™

### 4. Support
××¡×¤×¨ ×”××•×¤×¢×™× ×‘×¤×•×¢×œ ×©×œ ×›×œ ×§×˜×’×•×¨×™×” ×‘×§×‘×•×¦×ª ×”×‘×“×™×§×”

### ×“×•×’××” ××˜×¨×™×¦×ª ×‘×œ×‘×•×œ (Confusion Matrix):

```               
                  | Predicted: Apple | Predicted: Orange | Predicted: Banana
------------------|------------------|-------------------|-------------------
Actual: Apple     |        14        |         2         |         1
Actual: Orange    |         3        |        16         |         0
Actual: Banana    |         0        |         1         |        13
```

×•×—×™×©×•×‘ ×”××“×“×™×:

```
              precision    recall  f1-score   support
       APPLE      0.82      0.82      0.82        17
       ORANGE     0.84      0.84      0.84        19
       BANANA     0.93      0.93      0.93        14

    accuracy                          0.86        50
   macro avg      0.86      0.86      0.86        50
weighted avg      0.86      0.86      0.86        50
```

×”×¡×‘×¨ ×¢×œ `macro avg` ×•Ö¾`weighted avg` ×‘×“×•"×— ×¡×™×•×•×’ (Classification Report)

×›××©×¨ ×× ×• ××©×ª××©×™× ×‘×¤×•× ×§×¦×™×” `classification_report` ×Ö¾Scikit-learn, ×× ×• ××§×‘×œ×™× ×˜×‘×œ×” ×¢× ××“×“×™× ×œ×›×œ ×§×˜×’×•×¨×™×”, ×›××•:
- **precision** â€“ ×“×™×•×§ ×”×ª×—×–×™×•×ª
- **recall** â€“ ×™×›×•×œ×ª ×œ×–×”×•×ª ××ª ×”××§×¨×™× ×”× ×›×•× ×™×
- **f1-score** â€“ the harmonic mean between precision and recall
- **support** â€“ ××¡×¤×¨ ×”×“×•×’×××•×ª ×©×œ ×›×œ ×§×˜×’×•×¨×™×”

××” ×–×” `macro avg`?

- **×××•×¦×¢ ×¤×©×•×˜** ×©×œ ×”××“×“×™× ×¢×‘×•×¨ ×›×œ ×”×§×˜×’×•×¨×™×•×ª.
- ×›×œ ×§×˜×’×•×¨×™×” ××§×‘×œ×ª **×—×©×™×‘×•×ª ×©×•×•×”**, ×‘×œ×™ ×§×©×¨ ×œ×’×•×“×œ ×©×œ×”.

ğŸ“Œ ×“×•×’××”:
×× × ×—×©×‘ ××ª `macro avg` ×œÖ¾precision:

$$
\text{macro avg precision} = \frac{0.82 + 0.84 + 0.93}{3} = 0.863
$$

××ª××™× ×›××©×¨ ×—×©×•×‘ ×œ× ×• ×œ×”×ª×™×™×—×¡ ×œ×›×œ ×§×˜×’×•×¨×™×” **×‘××•×¤×Ÿ ×©×•×•×”**.

××” ×–×” `weighted avg`?

- ××—×©×‘×™× ×××•×¦×¢ ×©×œ ×”××“×“×™×, ××‘×œ ×›×œ ×§×˜×’×•×¨×™×” ××§×‘×œ×ª **××©×§×œ ×œ×¤×™ ×›××•×ª ×”×“×•×’×××•×ª ×©×œ×” (support)**.
- ×§×˜×’×•×¨×™×” ×©×™×© ×œ×” ×™×•×ª×¨ ×“×•×’×××•×ª ×ª×©×¤×™×¢ ×™×•×ª×¨ ×¢×œ ×”×××•×¦×¢.

ğŸ“Œ ×“×•×’××”:
×× × ×—×©×‘ ××ª `weighted avg` ×œÖ¾precision:

$$
\text{weighted avg precision} = \frac{(17 \times 0.82) + (19 \times 0.84) + (14 \times 0.93)}{50} = 0.858
$$

××ª××™× ×›××©×¨ ×¨×•×¦×™× ×œ×§×‘×œ ××“×“ ×›×•×œ×œ ×©××™×™×¦×’ **××ª ×”××¦×™××•×ª ×©×œ ×”×”×ª×¤×œ×’×•×ª** ×‘×“××˜×”.

×˜×‘×œ×ª ×¡×™×›×•×:

| ×¡×•×’ ×××•×¦×¢       | ××™×š ××—×©×‘×™×?                   | ××ª×™ ××ª××™×?                         |
|------------------|-------------------------------|-------------------------------------|
| `macro avg`      | ×××•×¦×¢ ×¤×©×•×˜ ×©×œ ×›×œ ×”×§×˜×’×•×¨×™×•×ª     | ×›×©××ª×” ×¨×•×¦×” ×œ×ª×ª ×—×©×™×‘×•×ª ×©×•×•×” ×œ×›×•×œ×Ÿ   |
| `weighted avg`   | ×××•×¦×¢ ××©×•×§×œ×œ ×œ×¤×™ ××¡×¤×¨ ×“×•×’×××•×ª | ×›×©××ª×” ×¨×•×¦×” ×œ×©×§×£ ××ª ×”××¦×™××•×ª ×‘×“××˜×”   |


×§×•×“ ×¤×™×™×ª×•×Ÿ ×œ×”×¦×’×ª ××“×“×™ ×‘×™×¦×•×¢ **×¢×œ ×—×œ×§ ×”×˜×¡×˜:**

```python
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

colors = np.array([200, 50, 220, 240, 250, 230, 30, 40, 20])
sizes = np.array([7, 7, 6, 9, 8, 9, 12, 13, 11])
weights = np.array([150, 160, 140, 170, 165, 180, 120, 130, 115])
fruit_types = np.array(['apple', 'apple', 'apple', 'orange', 'orange', 'orange', 'banana', 'banana', 'banana'])

# Create feature matrix
X = np.column_stack((colors, sizes, weights))
y = fruit_types

# ××™××•×Ÿ ×”××•×“×œ
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
model = KNeighborsClassifier(n_neighbors=3)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Calculate and display classification report (precision, recall, f1-score, support)
print("Classification Report:")
report = classification_report(y_test, y_pred)
print(report)

# Calculate and display confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)

# Create prettier confusion matrix display with labels
fruit_labels = ['apple', 'orange', 'banana']
cm_df = pd.DataFrame(cm,
                    index=[f'Actual: {label}' for label in fruit_labels],
                    columns=[f'Predicted: {label}' for label in fruit_labels])
print("\nConfusion Matrix (labeled):")
print(cm_df)

# Visualize confusion matrix with heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=fruit_labels,
            yticklabels=fruit_labels)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Fruit Classification')
plt.tight_layout()
plt.show()
```

## ×©×™×˜×•×ª ×œ×‘×—×™×¨×ª K ××•×¤×˜×™××œ×™

### 1. Elbow Method

×©×™×˜×ª ×”-Elbow ××‘×•×¡×¡×ª ×¢×œ ×‘×“×™×§×ª ×©×™×¢×•×¨ ×”×©×’×™××” ×©×œ ×”××•×“×œ ×¢× ×¢×¨×›×™ K ×©×•× ×™×, ×•×—×™×¤×•×© × ×§×•×“×ª "××¨×¤×§" ×©×‘×” ×ª×•×¡×¤×ª ×¢×¨×›×™ K × ×•×¡×¤×™× ××™× ×” ××©×¤×¨×ª ××©××¢×•×ª×™×ª ××ª ×”×‘×™×¦×•×¢×™×:

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# ×”×›× ×ª ×”× ×ª×•× ×™×
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ×‘×“×™×§×ª ×“×™×•×§ ×¢×‘×•×¨ ×¢×¨×›×™ K ×©×•× ×™×
k_range = range(1, 31)
scores = []

for k in k_range:
    model = KNeighborsClassifier(n_neighbors=k)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    scores.append(accuracy_score(y_test, y_pred))  # accuracy = (TP + TN) / (TP + TN + FP + FN)

# ×”×¦×’×ª ×”×ª×•×¦××•×ª ×‘×’×¨×£
plt.figure(figsize=(10, 6))
plt.plot(k_range, scores, marker='o')
plt.title('KNN: Accuracy for different K values')
plt.xlabel('K Value')
plt.ylabel('Accuracy')
plt.xticks(k_range[::2])  # ×”×¦×’×ª ×¢×¨×›×™ K ×–×•×’×™×™× ×‘×œ×‘×“ ×œ× ×•×—×•×ª
plt.grid(True)
plt.show()

# ××¦×™××ª ×¢×¨×š K ×”××•×¤×˜×™××œ×™
optimal_k = k_range[np.argmax(scores)]
print(f"×¢×¨×š K ×”××•×¤×˜×™××œ×™ ×”×•×: {optimal_k} ×¢× ×“×™×•×§ ×©×œ: {max(scores):.4f}")
```

<img src="knn4.png" style="width:60%;"/>

×‘×©×™×˜×ª ×”-Elbow, ×× ×• ××—×¤×©×™× ××ª ×”× ×§×•×“×” ×©×‘×” ×”×©×™×¤×•×¨ ×‘×“×™×•×§ ××ª×—×™×œ ×œ×”×ª××ª×Ÿ ××©××¢×•×ª×™×ª, ×›××• "××¨×¤×§" ×‘×’×¨×£.

### 2. Cross-Validation (××™××•×ª ×¦×•×œ×‘)

××™××•×ª ×¦×•×œ×‘ ××—×œ×§ ××ª ×”× ×ª×•× ×™× ×œ××¡×¤×¨ "×§×™×¤×•×œ×™×" (folds), ×•××‘×¦×¢ ××™××•×Ÿ ×•× ×™×‘×•×™ ×¢×œ ×›×œ ×§×™×¤×•×œ, ×××¦×¢ ××ª ×”×ª×•×¦××•×ª ×›×“×™ ×œ×§×‘×œ ×”×¢×¨×›×” ××“×•×™×§×ª ×™×•×ª×¨ ×©×œ ×‘×™×¦×•×¢×™ ×”××•×“×œ:

```python
from sklearn.model_selection import cross_val_score

# ×‘×“×™×§×ª ×‘×™×¦×•×¢×™× ×¢× ××™××•×ª ×¦×•×œ×‘ ×¢×‘×•×¨ ×¢×¨×›×™ K ×©×•× ×™×
k_range = range(1, 31)
cv_scores = []

for k in k_range:
    model = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(model, X, y, cv=10, scoring='accuracy')  # 10-fold CV
    # accuracy = (TP + TN) / (TP + TN + FP + FN)
    cv_scores.append(scores.mean())

# ×”×¦×’×ª ×”×ª×•×¦××•×ª ×‘×’×¨×£
plt.figure(figsize=(10, 6))
plt.plot(k_range, cv_scores, marker='o')
plt.title('KNN: CV Accuracy for different K values')
plt.xlabel('K Value')
plt.ylabel('CV Accuracy')
plt.xticks(k_range[::2])
plt.grid(True)
plt.show()

# ××¦×™××ª ×¢×¨×š K ×”××•×¤×˜×™××œ×™
optimal_k_cv = k_range[np.argmax(cv_scores)]
print(f"×¢×¨×š K ×”××•×¤×˜×™××œ×™ (CV) ×”×•×: {optimal_k_cv} ×¢× ×“×™×•×§ ×××•×¦×¢ ×©×œ: {max(cv_scores):.4f}")
```

<img src="knn5.png" style="width:60%;"/>

# ×©×™×˜×•×ª ×œ×‘×—×™×¨×ª ×¤×¨××˜×¨×™× ××•×¤×˜×™××œ×™×™×

### GridSearchCV with KNN â€“ Explanation & Example

`GridSearchCV` from Scikit-learn helps you **find the best parameters** for your model  
by searching through all possible combinations (a "grid") of parameters.

#### ğŸ“¦ In the context of KNN:

You might want to try different values for:

- `n_neighbors`: Number of neighbors (K)
- `weights`: 
  - `'uniform'` â€” all neighbors have equal weight  
  - `'distance'` â€” closer neighbors get more weight
- `metric`: 
  - `'euclidean'` â€” standard distance  
  - `'manhattan'` â€” city block distance. It measures the distance between two points by only moving horizontally and vertically, like you would in a city with square blocks
    abs(x1-x2) + abs(y1-y2)

#### âš™ï¸ How it works:

1. You define a **grid of parameters** to test
2. `GridSearchCV` trains the model using **cross-validation** for each combination
3. It evaluates each setup using a scoring metric (e.g., accuracy)
4. It returns the **best parameter combination** based on results

#### ğŸ§  Python Example:

```python
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

# Example data (X, y)
# Assume X and y are already defined

# Base model
knn = KNeighborsClassifier()

# Grid of parameters to try
param_grid = {
    'n_neighbors': list(range(1, 31)),
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}

# Grid Search with 5-fold cross-validation
grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')
grid.fit(X, y)

# Show best results
print("Best parameters:", grid.best_params_)
print("Best accuracy:", grid.best_score_)
```

Demo output:
```
GridSearchCV completed in 3.64 seconds
Best parameters: {'knn__n_neighbors': np.int64(11), 'knn__p': 2, 'knn__weights': 'distance'}
  p stands for the Minkowski distance formula:
  if p = 1 â†’ Manhattan (city block) distance
  if p = 2 â†’ Euclidean distance
Best accuracy: 0.8239
```



### ×›×™×¦×“ ×œ×‘×—×•×¨ ××ª K ×”××ª××™×?

1. **×’×•×“×œ ×”××“×’×**: ×›×›×œ ×©××“×’× ×”××™××•×Ÿ ×’×“×•×œ ×™×•×ª×¨, × ×™×ª×Ÿ ×œ×”×©×ª××© ×‘-K ×’×“×•×œ ×™×•×ª×¨.
   
2. **×¨××ª ×”×¨×¢×© ×‘× ×ª×•× ×™×**: 
   - ×œ× ×ª×•× ×™× ×¢× ××¢×˜ ×¨×¢×©: K ×§×˜×Ÿ ×™×•×ª×¨ (1-5)
   - ×œ× ×ª×•× ×™× ×¢× ×”×¨×‘×” ×¨×¢×©: K ×’×“×•×œ ×™×•×ª×¨ (×œ×”×¤×—×™×ª ××ª ×”×©×¤×¢×ª ×”×¨×¢×©)

3. **××•×¨×›×‘×•×ª ×”×’×‘×•×œ×•×ª ×‘×™×Ÿ ×”××—×œ×§×•×ª**:
   - ×’×‘×•×œ×•×ª ×¤×©×•×˜×™×: K ×’×“×•×œ ×™×•×ª×¨
   - ×’×‘×•×œ×•×ª ××•×¨×›×‘×™×: K ×§×˜×Ÿ ×™×•×ª×¨

4. **×›×œ×œ ××¦×‘×¢**: ×œ×¢×ª×™× ×§×¨×•×‘×•×ª ××•××œ×¥ ×œ×”×ª×—×™×œ ×¢× $K = \sqrt{n}$ ×›××©×¨ n ×”×•× ××¡×¤×¨ ×”×“×•×’×××•×ª ×‘××“×’× ×”××™××•×Ÿ.

5. **×¢×¨×›×™× ××™-×–×•×’×™×™×**: ×¢×‘×•×¨ ×‘×¢×™×•×ª ×¡×™×•×•×’ ×‘×™× ××¨×™, ×›×“××™ ×œ×‘×—×•×¨ ×¢×¨×›×™ K ××™-×–×•×’×™×™× ×›×“×™ ×œ×× ×•×¢ ×ª×™×§×•.

6. ## ×¡×™×›×•×

×‘×—×™×¨×ª ×¢×¨×š K ×”××•×¤×˜×™××œ×™ ×”×™× ××¤×ª×— ×œ×”×¦×œ×—×ª ××œ×’×•×¨×™×ª× KNN:

1. **×©×™×˜×•×ª ××•××œ×¦×•×ª ×œ×‘×—×™×¨×ª K**:
   - ×¢×‘×•×¨ ××“×’××™× ×§×˜× ×™×: ××™××•×ª ×¦×•×œ×‘ (cross-validation)
   - ×¢×‘×•×¨ ××“×’××™× ×’×“×•×œ×™×: ×©×™×˜×ª Elbow ××• GridSearchCV
   - ×©×™××•×© ×‘-K ××™-×–×•×’×™ ×œ×× ×™×¢×ª ×ª×™×§×• ×‘×‘×¢×™×•×ª ×‘×™× ××¨×™×•×ª

2. **×˜×™×¤×•×œ ×‘××§×¨×™ ×ª×™×§×•**:
   - ×©×™××•×© ×‘××©×§×•×œ×•×ª ××¨×—×§ (`weights='distance'`)
   - ×‘×—×™×¨×ª K ××™-×–×•×’×™
   - ×”×¤×—×ª×ª K ××• ×”×’×“×œ×ª×• ×‘××§×¨×” ×”×¦×•×¨×š

## × ×¡×¤×— ×': ×“×•×’×× ×¢× fetch_openml

<a href="A10-fetch_openml.md">read here about fetch_openml</a>


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV, train_test_split, learning_curve
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import fetch_openml
from sklearn.pipeline import Pipeline
import time
import warnings

warnings.filterwarnings('ignore')

# Load or create dataset
print("Loading dataset...")

# fetch_openml has different datasets like:
# mnist_784, fashion-mnist, covertype, shuttle, letter
dataset = fetch_openml('fashion-mnist', version=1, as_frame=False)
X, y = dataset.data, dataset.target

# Take a subset for faster computation if needed
subset_size = 5000
if X.shape[0] > subset_size:
    indices = np.random.choice(X.shape[0], subset_size, replace=False)
    X, y = X[indices], y[indices]
print(f"Dataset shape: {X.shape}, with {len(np.unique(y))} classes")

# Split the data into training, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

print(f"Training set: {X_train.shape}")
print(f"Validation set: {X_val.shape}")
print(f"Test set: {X_test.shape}")

# Create a pipeline that includes scaling and KNN
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('knn', KNeighborsClassifier())
])

reduced_param_grid = {
    'knn__n_neighbors': np.arange(1, 31, 2),  # K values from 1 to 30 (odd numbers)
    'knn__weights': ['uniform', 'distance'],  # Weight function
    'knn__p': [1, 2]  # 1 for Manhattan, 2 for Euclidean
}

# Create GridSearchCV
print("Setting up GridSearchCV...")
grid_search = GridSearchCV(
    pipeline,
    reduced_param_grid,  # Use reduced_param_grid for faster execution
    cv=5,  # 5-fold cross-validation
    n_jobs=-1,  # Use all available cores
    verbose=1,
    scoring='accuracy',
    return_train_score=True
)

# Fit the model
print("Training model with GridSearchCV...")
start_time = time.time()
grid_search.fit(X_train, y_train)
end_time = time.time()
print(f"GridSearchCV completed in {end_time - start_time:.2f} seconds")

# Get the best parameters and score
print(f"Best parameters: {grid_search.best_params_}")
print(f"Best cross-validation score: {grid_search.best_score_:.4f}")

# Evaluate on validation set
best_model = grid_search.best_estimator_
val_predictions = best_model.predict(X_val)
val_accuracy = accuracy_score(y_val, val_predictions)
print(f"Validation accuracy: {val_accuracy:.4f}")

# Evaluate on test set
test_predictions = best_model.predict(X_test)
test_accuracy = accuracy_score(y_test, test_predictions)
print(f"Test accuracy: {test_accuracy:.4f}")

# Print classification report
print("\nClassification Report:")
print(classification_report(y_test, test_predictions))

### Visualize

```

output:
```
Loading dataset...
Dataset shape: (5000, 784), with 10 classes
Training set: (3000, 784)
Validation set: (1000, 784)
Test set: (1000, 784)
Setting up GridSearchCV...
Training model with GridSearchCV...
Fitting 5 folds for each of 60 candidates, totalling 300 fits
GridSearchCV completed in 50.65 seconds
Best parameters: {'knn__n_neighbors': np.int64(5), 'knn__p': 1, 'knn__weights': 'distance'}
Best cross-validation score: 0.7917
Validation accuracy: 0.7870
Test accuracy: 0.8120

Classification Report:
              precision    recall  f1-score   support

           0       0.77      0.84      0.81       108
           1       1.00      0.92      0.96        92
           2       0.73      0.59      0.65       103
           3       0.86      0.84      0.85        95
           4       0.60      0.84      0.70        90
           5       0.97      0.81      0.88       122
           6       0.57      0.48      0.53        95
           7       0.81      0.92      0.86       104
           8       0.97      0.92      0.94        97
           9       0.88      0.95      0.91        94
```

<a href="9zz-full-demo.py"/>see complete code...</a>


<img src="knn-ex1.png" />

<img src="knn-ex2.png" />

<img src="knn-ex3.png" />




