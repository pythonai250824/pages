# ×”×’×“×¨×•×ª SVM, Kernel, Kernel Function, ×•-Kernel Trick

## Support Vector Machine (SVM)
×–×”×• ××œ×’×•×¨×™×ª× ×œ××™×“×” ××•× ×—×™×ª ×”××©××© ×œ×¡×™×•×•×’ (classification) ×•×¨×’×¨×¡×™×” (regression). ×”××œ×’×•×¨×™×ª× ××—×¤×© ×”×™×¤×¨×¤×œ×Ÿ ××•×¤×˜×™××œ×™ (××™×©×•×¨ ×”×¤×¨×“×”) ×©×™×¤×¨×™×“ ×‘×™×Ÿ ×§×‘×•×¦×•×ª ×©×•× ×•×ª ×©×œ × ×ª×•× ×™×. ×”×™×¤×¨×¤×œ×Ÿ ××•×¤×˜×™××œ×™ ×”×•× ×–×” ×©×™×•×¦×¨ ××ª ×”××¨×•×•×— (margin) ×”××¨×‘×™ ×‘×™×Ÿ ×”× ×§×•×“×•×ª ×”×§×¨×•×‘×•×ª ×‘×™×•×ª×¨ ××›×œ ×§×‘×•×¦×”, ×”×™×“×•×¢×•×ª ×›×•×•×§×˜×•×¨×™ ×ª××™×›×” (support vectors).

××ª××˜×™×ª, ×¢×‘×•×¨ × ×ª×•× ×™× ×œ×™× ××¨×™×™×, ×”××•×“×œ ××™×•×¦×’ ×¢×œ ×™×“×™:
- $f(x) = w^T x + b$
- ×›××©×¨ $w$ ×”×•× ×•×§×˜×•×¨ ×”××©×§×œ×•×ª
- x ×”×•× ×•×§×˜×•×¨ ×ª×›×•× ×•×ª ×”×§×œ×˜
- b ×”×•× ×¢×¨×š ×”×”×¡×˜×” (bias)
- ×”× ×•×¡×—× ××ª××™××” ×œ×”×¨×‘×” ××™××“×™× ×•×œ× ×¨×§ ×œ×“×•-××™××“

# ××™×š ××•×¦××™× ××ª ×•×§×˜×•×¨ ×”××©×§×œ×™× \( w \) ×‘Ö¾SVM

## ğŸ¯ ×”××˜×¨×” ×©×œ SVM
×œ××¦×•× ××ª ×”×§×• (××• ×”×”×™×¤×¨Ö¾×¤×œ×™×™×Ÿ) ×©××¤×¨×™×“ ×”×›×™ ×˜×•×‘ ×‘×™×Ÿ ×©×ª×™ ×§×‘×•×¦×•×ª, ×›×š ×©×”××¨×—×§ ××”× ×§×•×“×•×ª ×”×§×¨×•×‘×•×ª ×‘×™×•×ª×¨ ××›×œ ×¦×“ (×”Ö¾**support vectors**) ××œ ×”×§×• ×™×”×™×” **×”×›×™ ×’×“×•×œ ×©××¤×©×¨**


## ğŸ”¢ ××™×š ××•×¦××™× ××ª \( w \)?
×”××•×“×œ ××’×“×™×¨ ×‘×¢×™×” ××ª××˜×™×ª ×©×œ **××•×¤×˜×™××™×–×¦×™×”** (××¦×™××ª ××§×¡×™××•×/××™× ×™××•×)

### 1. × ×•×¡×—×ª ×”×”×™×¤×¨Ö¾×¤×œ×™×™×Ÿ:
$$
f(x) = w^T x + b
$$

××:

$$f(x) \geq 1$$

â†’ ×”×“×’×™××” ×©×™×™×›×ª ×œ××—×œ×§×” ×”×—×™×•×‘×™×ª (label = +1)

$$f(x) \leq -1$$ 

â†’ ×”×“×’×™××” ×©×™×™×›×ª ×œ××—×œ×§×” ×”×©×œ×™×œ×™×ª (label = -1)

### 2. ×ª× ××™ ×”×”×¤×¨×“×”:
×œ×›×œ ×“×•×’××” Xi Yi:

$$
y_i (w^T x_i + b) \geq 1
$$

**××” ×–×” Yi**

×”- Yi ×–×” ×”×ª×•×•×™×ª (label) ×©×œ ×”×“×•×’××” ×”Ö¾ i

×›×œ Xi ×”×•× ×•×§×˜×•×¨

×›×œ Yi ×”×•× ××¡×¤×¨ ×©××•××¨ ×œ××™×–×” ×§×‘×•×¦×” ×©×™×™×›×ª ×”×“×•×’××, ×œ×§×‘×•×¦×” ×”×—×™×•×‘×™×ª ××• ×œ×§×‘×•×¦×” ×”×©×œ×™×œ×™×ª

### 3. ×¤×•× ×§×¦×™×™×ª ×”××˜×¨×” (Objective Function):
×›×“×™ ×œ××§×¡× ××ª ×”××¨×—×§ ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª, × ××–×¢×¨ ××ª ×’×•×“×œ \( w \):

$$
\min \left( \frac{1}{2} \||w\||^2 \right)
$$

$$
\|w\| = \sqrt{w_1^2 + w_2^2 + \dots + w_n^2}
$$

×ª×—×ª ×”×”×’×‘×œ×”:

$$
y_i (w^T x_i + b) \geq 1 \quad \forall i
$$


â“ ×œ××” ×¦×¨×™×š ×œ×”×§×˜×™×Ÿ ××ª \||w\||  ×‘Ö¾SVM?

âœ¨ ×”×”×¡×‘×¨: ×›×œ ×”×¡×•×“ × ××¦× ×‘Ö¾**Margin** â€“ ×”××¨×•×•×— ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª

**×”××¨×—×§ ×©×œ × ×§×•×“×” ××”××™×©×•×¨:**
×œ×¤×™ ×”× ×•×¡×—×”:

$$
\text{Distance from hyperplane} = \frac{|w^T xi + b|}{\||w\||}
$$

**×”××˜×¨×” ×©×œ SVM:**
×œ××¦×•× ××™×©×•×¨ ×©××¤×¨×™×“ ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª ×¢× **×”××¨×—×§ ×”×›×™ ×’×“×•×œ ××”× ×§×•×“×•×ª ×”×§×¨×•×‘×•×ª ×‘×™×•×ª×¨** â€” ×›×œ×•××¨, ××¨×•×•×— (margin) ××§×¡×™××œ×™

**×ª× ××™ ×”×”×¤×¨×“×”:**

$$
y_i(w^T x_i + b) \geq 1
$$

×”× ×§×•×“×•×ª ×”×›×™ ×§×¨×•×‘×•×ª ×œ××™×©×•×¨ ×”×Ÿ ×”Ö¾**Support Vectors**, ×©××§×™×™××•×ª:

$$
y_i(w^T x_i + b) = 1
$$

**×”××¨×—×§ ×©×œ×”×Ÿ ××”××™×©×•×¨:**

$$
\text{margin} = \frac{1}{\||w\||}
$$


### ×•×œ×›×Ÿ:
- ×›×›×œ ×©Ö¾ ||w||  **×§×˜×Ÿ ×™×•×ª×¨**, ×”××¨×•×•×— **×’×“×•×œ ×™×•×ª×¨**.
- ×›×œ×•××¨: ×× × ×§×˜×™×Ÿ ××ª ||w\||, ×× ×—× ×• **××¨×—×™×§×™×** ××ª ×”××™×©×•×¨ ××”× ×§×•×“×•×ª ×”×§×¨×•×‘×•×ª ×‘×™×•×ª×¨ â€” ×•×–×” ×‘×“×™×•×§ ××” ×©×× ×—× ×• ×¨×•×¦×™×!

**×•×œ×›×Ÿ ×‘×¤×•× ×§×¦×™×™×ª ×”××˜×¨×” ×©×œ SVM:
×× ×—× ×• **×××–×¢×¨×™×** ××ª:**

$$
\frac{1}{2} \||w\||^2
$$


×›×“×™ ×œ××¦×•× ××ª ×”×”×™×¤×¨Ö¾×¤×œ×™×™×Ÿ ×¢× **margin ××§×¡×™××œ×™** ×•×œ×•×•×“× ×”×¤×¨×“×” ×˜×•×‘×” ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª.

**×œ××” ×××–×¢×¨×™× ××ª**

$$ 
\frac{1}{2} \||w\||^2 
$$

×•×œ× ×¤×©×•×˜ ××ª 

$$ 
\||w\||
$$

×‘Ö¾SVM?

**×”××˜×¨×” ×”××§×•×¨×™×ª:**
×× ×—× ×• ×¨×•×¦×™× ×œ××§×¡× ××ª ×”××¨×•×•×— (**margin**) ×‘×™×Ÿ ×©×ª×™ ×”×§×‘×•×¦×•×ª.

**×”××¨×•×•×— ××•×’×“×¨ ×›:**

$$
\text{margin} = \frac{1}{\|w\|}
$$

×›×“×™ **×œ××§×¡×** ××ª ×”××¨×•×•×— â€” ×¦×¨×™×š **×œ××–×¢×¨** ××ª:

$$
\|w\|
$$

××– ×œ××” ×××–×¢×¨×™× ×“×•×•×§× ××ª 

$$ 
\frac{1}{2} \|w\|^2 
$$

?

**×¡×™×‘×•×ª ××ª××˜×™×•×ª:**

**× ×’×–×¨×•×ª ×¤×©×•×˜×•×ª ×™×•×ª×¨**:

×× × ×’×“×™×¨ ××ª ×¤×•× ×§×¦×™×™×ª ×”××˜×¨×” ×›×š:

$$
\frac{1}{2} \|w\|^2
$$

××– ×”× ×’×–×¨×ª ×©×œ×” ×”×™× ×¤×©×•×˜ 

$$ 
w 
$$

×•××™×Ÿ ×¦×•×¨×š ×‘×©×•×¨×©×™× ××• × ×’×–×¨×•×ª ××•×¨×›×‘×•×ª

---



## ğŸ¤– ××™×š ×¤×•×ª×¨×™× ××ª ×–×” ×‘×¤×•×¢×œ?

ğŸ” ××” ×–×” ×”Ö¾Dual Problem ×‘Ö¾SVM?

×‘Ö¾SVM ×× ×¡×™× ×œ××¦×•× ××ª ×”×§×• ×”×›×™ ×˜×•×‘ ×©××¤×¨×™×“ ×‘×™×Ÿ ×§×‘×•×¦×•×ª
×™×© ×©×ª×™ ×“×¨×›×™× ×œ×¤×ª×•×¨ ××ª ×”×‘×¢×™×”:

âœ¨ Primal Problem (×‘×¢×™×” ××§×•×¨×™×ª):
- ××•×¦××™× ×™×©×™×¨×•×ª ××ª: `w`, `b`, ×•Ö¾ `Î¾` (×©×’×™××•×ª)
- ××ª××™××” ×‘×¢×™×§×¨ ×œ××§×¨×™× ×¤×©×•×˜×™× ××• ×œ×™× ××¨×™×™×

âœ¨ Dual Problem (×‘×¢×™×” ×›×¤×•×œ×”):
- ×¤×•×ª×¨×™× ×“×¨×š ××©×ª× ×™× ×—×“×©×™×: `Î±áµ¢`
- ×××¤×©×¨ ×©×™××•×© ×‘Ö¾**Kernel Trick**
- ××“×’×™×©×” ×¨×§ ××ª **×•×§×˜×•×¨×™ ×”×ª××™×›×”** (support vectors)
- Î±áµ¢ â‰¥ 0  
- Î£ Î±áµ¢ yáµ¢ = 0

ğŸ’¡ ×œ××” ×–×” ×˜×•×‘?
- ×××¤×©×¨ ×©×™××•×© ×‘×§×¨× ×œ×™× (×’× ×× ×”×“××˜×” ×œ× ×œ×™× ××¨×™)
- ××“×’×™×© ×¨×§ ×“×•×’×××•×ª ×—×©×•×‘×•×ª
- ×™×¢×™×œ ×™×•×ª×¨ ×›×©×™×© ×”×¨×‘×” ×××“×™×

**×¤×ª×¨×•×Ÿ:**

1. ××©×ª××©×™× ×‘×©×™×˜×” ××ª××˜×™×ª ×‘×©× **Lagrange Multipliers**
2. ×¤×•×ª×¨×™× ××ª ×”×‘×¢×™×” ×”×›×¤×•×œ×” (Dual Problem)
3. ×”×¤×ª×¨×•×Ÿ ××‘×•×¡×¡ ×¨×§ ×¢×œ ×”Ö¾**Support Vectors** (×”× ×§×•×“×•×ª ×”×›×™ ×§×¨×•×‘×•×ª ×œ×”×™×¤×¨Ö¾×¤×œ×™×™×Ÿ)
4. ××”× ××—×©×‘×™× ××ª \( w \) ×›×š:

$$
w = \sum_i \alpha_i y_i x_i
$$


×›××©×¨:

$$
\alpha_i
$$

-  ×”× ×¤×¨××˜×¨×™× ×©×§×•×‘×¢×™× ××ª ×—×©×™×‘×•×ª ×›×œ ×“×•×’××”
- ×¨×§ ×¢×‘×•×¨ ×”Ö¾support vectors ×™×© alpha ×©×•× ×” ×××¤×¡

**ğŸ’¡ ×¡×™×›×•×**
- ×× ×—× ×• ×œ× ××—×©×‘×™× ××ª \( w \) ×™×©×™×¨×•×ª, ××œ× ×¤×•×ª×¨×™× ×‘×¢×™×™×ª ××•×¤×˜×™××™×–×¦×™×”.
- ×”××˜×¨×” ×”×™× ×œ××¦×•× ××ª ×”×§×• ×©××¤×¨×™×“ ×”×›×™ ×˜×•×‘ ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª ×¢× **margin** ××§×¡×™××œ×™.
- ×”×ª×•×¦××”: ××©×•×•××” ×©××‘×•×¡×¡×ª ×¨×§ ×¢×œ ×”Ö¾support vectors.


## ğŸ§  ××™×š SVM ××ª××•×“×“ ×¢× ×™×•×ª×¨ ××©×ª×™ ×§×‘×•×¦×•×ª?

### ğŸ¯ ×”×‘×¢×™×”:
×”××•×“×œ ×”- "×§×œ××¡×™" × ×•×¢×“ ×œ×‘×¢×™×” ×©×œ **×©× ×™ ×¡×•×’×™× ×‘×œ×‘×“**:
- ××—×œ×§×” ×—×™×•×‘×™×ª: \( +1 \)
- ××—×œ×§×” ×©×œ×™×œ×™×ª: \( -1 \)

××‘×œ ××” ×¢×•×©×™× ×›×©×™×© **×©×œ×•×© ×§×‘×•×¦×•×ª ××• ×™×•×ª×¨**? (×œ××©×œ A, B, C)

**âœ… ×¤×ª×¨×•× ×•×ª × ×¤×•×¦×™×:**

**1. One-vs-Rest (OvR) â€“ "××—×“ ××•×œ ×›×œ ×”×©××¨"**

- ×× ×™×© 3 ×§×‘×•×¦×•×ª (A, B, C) â†’ × ×‘× ×” 3 ××•×“×œ×™×:
  - ××•×“×œ 1: A ××•×œ (B ×•Ö¾C)
  - ××•×“×œ 2: B ××•×œ (A ×•Ö¾C)
  - ××•×“×œ 3: C ××•×œ (A ×•Ö¾B)

- ×›×œ ××•×“×œ ××××Ÿ SVM ×‘×™× ××¨×™
- ×›×©×‘×•×“×§×™× ×“×•×’××” ×—×“×©×”:
  - ××¨×™×¦×™× ××ª ×©×œ×•×©×ª ×”××•×“×œ×™×
  - ×‘×•×—×¨×™× ××ª ×”×§×‘×•×¦×” ×¢× ×”×¦×™×•×Ÿ ×”×’×‘×•×” ×‘×™×•×ª×¨

**2. One-vs-One (OvO) â€“ "×›×œ ×–×•×’ ××•×œ ×–×•×’"**

- × ×‘× ×” SVM ×¢×‘×•×¨ ×›×œ **×–×•×’ ×§×‘×•×¦×•×ª**
- ×œ×“×•×’××”, ×¢×‘×•×¨ ×§×‘×•×¦×•×ª A, B, C â†’ × ×‘× ×”:
  - A ××•×œ B
  - A ××•×œ C
  - B ××•×œ C
- ×¢×‘×•×¨ \( k \) ×§×‘×•×¦×•×ª ×™×©:


$$
\frac{k(k - 1)}{2}
$$

  ××•×“×œ×™× ×©×•× ×™×

- ×›×©×‘×•×“×§×™× ×“×•×’××” ×—×“×©×”:
  - ×›×œ ××•×“×œ × ×•×ª×Ÿ "×”×¦×‘×¢×”"
  - ×”×§×‘×•×¦×” ×©×–×•×›×” ×‘×”×›×™ ×”×¨×‘×” ×”×¦×‘×¢×•×ª ×”×™× ×”×–×•×›×”

**ğŸ¤– ×‘×¤×•×¢×œ â€“ ×¢× Scikit-learn:**

- ×× ×ª×©×ª××© ×‘Ö¾`SVC` (×¡×¤×¨×™×™×ª `sklearn.svm`) â€” ××™×Ÿ ×¦×•×¨×š ×œ×˜×¤×œ ×‘×–×” ×™×“× ×™×ª!
- ×›×‘×¨×™×¨×ª ××—×“×œ, ×”××œ×’×•×¨×™×ª× ××¤×¢×™×œ **One-vs-One** ×‘××•×¤×Ÿ ××•×˜×•××˜×™

**ğŸ’¡ ×¡×™×›×•×:**

| ××¡' ×§×‘×•×¦×•×ª | ×¤×ª×¨×•×Ÿ SVM               |
|------------|-------------------------|
| 2          | SVM ×¨×’×™×œ                |
| >2         | One-vs-Rest ××• One-vs-One |



## ×“×•×’×× ×‘×¤×™×™×ª×•×Ÿ ×¢×‘×•×¨ 3 ×§×‘×•×¦×•×ª

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.preprocessing import StandardScaler
from itertools import combinations

# Create a dataset for apples, bananas, and oranges
# Features: sweetness (x-axis) and weight (y-axis)
apples = np.array([[3, 150], [4, 130], [2, 160], [3, 140], [3.5, 145]])
bananas = np.array([[7, 120], [6, 110], [8, 115], [7.5, 125], [6.5, 118]])
oranges = np.array([[5, 180], [4.5, 195], [5.5, 185], [6, 175], [4.8, 190]])

# Combine features and create labels (0 for apples, 1 for bananas, 2 for oranges)
X = np.vstack([apples, bananas, oranges])
y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2])

# Scale the features (important for SVM)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Create and train the SVM model
# For multiclass problems, SVM creates multiple binary classifiers (one-vs-one by default)
clf = svm.SVC(kernel='linear', C=1000, decision_function_shape='ovr')
clf.fit(X_scaled, y)

# Create a test point
test_point = np.array([[5, 150]])  # A point with sweetness=5, weight=150
test_point_scaled = scaler.transform(test_point)
prediction = clf.predict(test_point_scaled)[0]
class_names = ["Apple", "Banana", "Orange"]
predicted_class = class_names[prediction]

print(f"Test point: Sweetness={test_point[0][0]}, Weight={test_point[0][1]}")
print(f"Predicted class: {predicted_class}")
```

<img src="svm8.png" style="width: 80%" />

Output:
```
Test point: Sweetness=5, Weight=150
Predicted class: Apple

Decision function values for test point:
Apple: 2.15989923461655
Banana: -0.18143598271709815
Orange: 1.071382786408734

the negative value for Banana simply indicates that the binary classifier for 
  "Banana vs. not-Banana" believes your test point is in the "not-Banana" category (rest)
  because we are using One-vs-Rest (OvR)
```