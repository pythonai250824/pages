# ğŸŒŸ ×¨×’×¨×¡×™×” ×œ×•×’×™×¡×˜×™×ª ×¢× ×›××” ××©×ª× ×™× ×•×›××” ×§×œ××¡×™×

## ğŸ“˜ ××” ×–×” ×¨×’×¨×¡×™×” ×œ×•×’×™×¡×˜×™×ª?
×¨×’×¨×¡×™×” ×œ×•×’×™×¡×˜×™×ª ×”×™× ×©×™×˜×” ×¡×˜×˜×™×¡×˜×™×ª ×œ×—×™×–×•×™ ××©×ª× ×” ×ª×œ×•×™ ×‘×™× ××¨×™ ××• ×¨×‘Ö¾×§×˜×’×•×¨×™ (×›××•: ×›×Ÿ/×œ×, ×¡×•×’ ×¤×¨×—), ×¢×œ ×¡××š ×¢×¨×›×™× ×©×œ ××©×ª× ×™× ××¡×‘×™×¨×™× (×ª×›×•× ×•×ª)

×‘××§×¨×” ×©×œ ××¡×¤×¨ ××©×ª× ×™× ××¡×‘×™×¨×™× (features), ××“×•×‘×¨ ×‘×¨×’×¨×¡×™×” ×œ×•×’×™×¡×˜×™×ª ×¢× **×›××” × ×¢×œ××™×**

---

## âœï¸ ××•×©×’×™× ×‘×¡×™×¡×™×™×:

- **××©×ª× ×” ×ª×œ×•×™ (Y):** ××” ×©×× ×—× ×• ×× ×¡×™× ×œ×—×–×•×ª (×œ××©×œ ×¡×•×’ ×¤×¨×—)
- **××©×ª× ×™× ×‘×œ×ª×™ ×ª×œ×•×™×™× (Xâ‚, Xâ‚‚, Xâ‚ƒ...):** ×”×ª×›×•× ×•×ª ×©××¡×‘×™×¨×•×ª ××ª Y (×œ××©×œ ××•×¨×š ×¢×œ×™ ×›×•×ª×¨×ª)
- **Î² (×‘×˜×):** ××§×“××™× ×©× ×œ××“×™× ×¢×œ ×™×“×™ ×”××•×“×œ. ×›×œ ××©×ª× ×” ××§×‘×œ Î² ××©×œ×•.

---

## ğŸ“ × ×•×¡×—×” ××ª××˜×™×ª (Softmax Multiclass):

×× ×™×© ×œ× ×• \( K \) ×§×˜×’×•×¨×™×•×ª ×•Ö¾\( n \) ××©×ª× ×™×:

$$
P(y = k \mid x) = \frac{e^{\beta_{k0} + \beta_{k1}x_1 + \beta_{k2}x_2 + \dots + \beta_{kn}x_n}}{\sum_{j=1}^K e^{\beta_{j0} + \beta_{j1}x_1 + \beta_{j2}x_2 + \dots + \beta_{jn}x_n}}
$$


×›××©×¨ ×™×© ×œ× ×• **K ×§×˜×’×•×¨×™×•×ª** ×©×•× ×•×ª, ×× ×—× ×• ××©×ª××©×™× ×‘×¤×•× ×§×¦×™×™×ª Softmax ×›×“×™ ×œ×—×©×‘ ××ª ×”×”×¡×ª×‘×¨×•×ª ×©×”×“×•×’××” ×©×œ× ×• ×©×™×™×›×ª ×œ××—×œ×§×” <img src="https://latex.codecogs.com/gif.latex?k"/>.
  
- <img src="https://latex.codecogs.com/gif.latex?x%20=%20[x_1,%20x_2,%20...,%20x_n]"/>: ×•×§×˜×•×¨ ×”×ª×›×•× ×•×ª ×©×œ ×”×“×•×’××” ×©×œ× ×•
- <img src="https://latex.codecogs.com/gif.latex?\beta_{k0}"/>: bias (×”×™×¡×˜) ×œ××—×œ×§×” <img src="https://latex.codecogs.com/gif.latex?k"/>
- <img src="https://latex.codecogs.com/gif.latex?\beta_{k1},%20\dots,%20\beta_{kn}"/>: ××©×§×œ×™× ×©×œ ×›×œ ×ª×›×•× ×” ×¢×‘×•×¨ ××—×œ×§×” <img src="https://latex.codecogs.com/gif.latex?k"/>
- <img src="https://latex.codecogs.com/gif.latex?e"/>: ×‘×¡×™×¡ ×”×œ×•×’×¨×™×ª× ×”×˜×‘×¢×™ (â‰ˆ 2.718)
- <img src="https://latex.codecogs.com/gif.latex?K"/>: ××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª
- ×”××›× ×” ×›×•×œ×œ ××ª ×›×œ ×”××—×œ×§×•×ª ×”××¤×©×¨×™×•×ª <img src="https://latex.codecogs.com/gif.latex?j%20=%201%20\dots%20K"/>
  

××” ×”× ×•×¡×—×” ×¢×•×©×”?

- ××—×©×‘×ª **"× ×™×§×•×“" (score)** ×œ×›×œ ×§×˜×’×•×¨×™×” ×œ×¤×™ ××•×“×œ ×œ×™× ××¨×™
- ×××™×¨×” ××ª ×›×œ ×”× ×™×§×•×“×™× ×œ×”×¡×ª×‘×¨×•×™×•×ª (×‘×™×Ÿ 0 ×œ-1) ×‘×¢×–×¨×ª ×¤×•× ×§×¦×™×™×ª Softmax
- ××‘×˜×™×—×” ×©×›×œ ×”×”×¡×ª×‘×¨×•×™×•×ª ×‘×™×—×“ ×™×ª× ×• ×‘×“×™×•×§ **1**
- ×”×§×˜×’×•×¨×™×” ×¢× ×”×”×¡×ª×‘×¨×•×ª ×”×’×‘×•×”×” ×‘×™×•×ª×¨ ×”×™× ×–×• ×©×”××•×“×œ ×™× ×‘×

### ğŸ’¬ ×“×•×’××” ××™×œ×•×œ×™×ª:

> ×× ×× ×—× ×• ×¨×•×¦×™× ×œ× ×‘× ××™×–×” ×¡×•×’ ×¤×¨×™ ×‘×ª××•× ×”: ×ª×¤×•×—, ×‘× × ×” ××• ×ª×¤×•×– â€”  
> Softmax ×ª×—×©×‘ ××ª ×”×”×¡×ª×‘×¨×•×™×•×ª ×œ×›×œ ××—×ª ××”××¤×©×¨×•×™×•×ª ×œ×¤×™ ×”×ª×›×•× ×•×ª (×¦×‘×¢, ×’×•×“×œ, ××©×§×œ),  
> ×•×ª×—×–×™×¨:  
> - ×ª×¤×•×—: 0.1  
> - ×‘× × ×”: 0.3  
> - ×ª×¤×•×–: 0.6 âœ… â† ×”×›×™ ×¡×‘×™×¨

- ×–×•×”×™ ×¤×•× ×§×¦×™×™×ª **Softmax** â€“ ×××™×¨×” ××ª ×”×¦×™×•× ×™× ×œ×”×¡×ª×‘×¨×•×™×•×ª.
- ×›×œ ×§×˜×’×•×¨×™×” ××§×‘×œ×ª ×”×¡×ª×‘×¨×•×ª, ×•×”×—×–×•×™×” ×”×™× ×–×• ×¢× ×”×”×¡×ª×‘×¨×•×ª ×”×’×‘×•×”×” ×‘×™×•×ª×¨.

## ğŸŒ¸ ×“×•×’××” â€“ ×¢×œ ×‘×¡×™×¡ × ×ª×•× ×™ Iris:

<img src="iris.jpg" style="width: 60%" />


× × ×¡×” ×œ×—×–×•×ª ××ª ×¡×•×’ ×”×¤×¨×— (setosa, versicolor, virginica) ×¢×œ ×¡××š 4 ××©×ª× ×™×:
- sepal length (Xâ‚)
- sepal width (Xâ‚‚)
- petal length (Xâ‚ƒ)
- petal width (Xâ‚„)

×›×œ ×¡×•×’ ×¤×¨×— ×™×§×‘×œ ××§×“××™× ××©×œ×•:

×œ××©×œ:

$$
P(y = \text{setosa}) = \frac{e^{\beta_{0} + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4}}{\text{sum of all classes}}
$$

### ×œ×•×’×™×¡×˜×™×ª ××¨×•×‘×ª ××©×ª× ×™× ×¢× softmax

### ğŸ¯ ××˜×¨×”:
×œ×—×–×•×ª ××™×–×” ×¤×¨×— ×–×” (Setosa / Versicolor / Virginica) ×œ×¤×™ ×”×ª×›×•× ×•×ª ×”×‘××•×ª:

- `xâ‚` = sepal length = 5.0  
- `xâ‚‚` = sepal width = 3.0  
- `xâ‚ƒ` = petal length = 1.5  
- `xâ‚„` = petal width = 0.2  

### ğŸ§  × × ×™×— ××ª ×”××§×“××™× ×”×‘××™×:

| Class       | Î²â‚€ (bias) | Î²â‚  | Î²â‚‚  | Î²â‚ƒ   | Î²â‚„   |
|-------------|-----------|-----|-----|------|------|
| Setosa      | 2.0       | 1.0 | 0.5 | -0.5 | -1.0 |
| Versicolor  | 1.0       | 0.5 | 0.2 |  0.1 |  0.3 |
| Virginica   | -0.5      | 0.3 | 0.1 |  0.8 |  0.9 |

### âœï¸ × ×—×©×‘ ××ª ×”×¦×™×•× ×™× (logits):

$$
z_{setosa} = 2.0 + 1.0 \cdot 5.0 + 0.5 \cdot 3.0 - 0.5 \cdot 1.5 - 1.0 \cdot 0.2 = 7.55
$$

$$
z_{versicolor} = 1.0 + 0.5 \cdot 5.0 + 0.2 \cdot 3.0 + 0.1 \cdot 1.5 + 0.3 \cdot 0.2 = 4.31
$$

$$
z_{virginica} = -0.5 + 0.3 \cdot 5.0 + 0.1 \cdot 3.0 + 0.8 \cdot 1.5 + 0.9 \cdot 0.2 = 2.68
$$

### ğŸ§® ××—×©×‘×™× softmax:

$$
P(k) = \frac{e^{z_k}}{\sum_j e^{z_j}}
$$

$$
e^{7.55} â‰ˆ 1902.56
$$

$$ 
e^{4.31} â‰ˆ 74.46 
$$

$$ 
e^{2.68} â‰ˆ 14.61 
$$
- ×¡×š ×”×›×•×œ: \( 1991.63 \)


**×”×¡×ª×‘×¨×•×™×•×ª ×œ×›×œ ×¤×¨×—:**

Setosa

$$
\frac{1902.56}{1991.63} = **0.955**  
$$

Versicolor

$$
\frac{74.46}{1991.63} = 0.037
$$

Virginica

$$
\frac{14.61}{1991.63} = 0.007
$$

### âœ… ××¡×§× ×”:

> ×”××•×“×œ ×—×•×–×” ×©×”×¤×¨×— ×”×•× **Setosa** ×¢× ×”×¡×ª×‘×¨×•×ª ×©×œ **95.5%**  
> ×•×œ×›×Ÿ ×–×” ××” ×©×ª×—×–×™×¨ ×”×¤×•× ×§×¦×™×” `predict` ×©×œ ×”××•×“×œ ×¢×‘×•×¨ ×”×¤×¨××˜×¨×™× ×”× ×•×›×—×™×™×


---


## ğŸ” LogisticRegressionCV â€“ Key Parameters Explained

`LogisticRegressionCV` is a version of logistic regression that automatically performs cross-validation to choose the best regularization parameter (`C`). It is ideal for both binary and multiclass classification

C is a hyperparameter that controls the strength of regularization in logistic regression

C controls how much we trust our training data versus how much we want to simplify the model to avoid overfitting

### ğŸ”§ Important Parameters:

- **`solver`** â€“ Optimization algorithm used for fitting the model:
  - `'lbfgs'` â€“ Default. Efficient for multiclass problems and large datasets.
  - `'liblinear'` â€“ Good for small datasets and binary classification; supports L1 penalty.
  - `'saga'` â€“ Handles large datasets; supports both L1 and L2 penalties; supports `multinomial`.
  - `'newton-cg'`, `'sag'` â€“ Also suitable for multiclass, but less commonly used.

- **`multi_class`** â€“ Defines the strategy for handling multiple classes:
  - `'ovr'` â€“ "One-vs-Rest": Trains one binary classifier per class. Suitable for binary and multiclass (less preferred).
  - `'multinomial'` â€“ Performs true multiclass classification using softmax. More accurate when the number of classes > 2 (requires `lbfgs`, `saga`, `newton-cg`, or `sag`).

- **`cv`** â€“ Number of cross-validation folds (default is 5). For example, `cv=5` splits the training data into 5 parts and uses each part as validation once.

- **`max_iter`** â€“ Maximum number of iterations taken by the solver to converge. If your model does not converge, increase this value (e.g., 500, 1000).

### ğŸ¯ What is Binary vs Multiclass Classification?

- **Binary classification**: The target `y` has **only 2 classes**  
  Example: `['yes', 'no']`, `['spam', 'not spam']`, `['approved', 'denied']`

- **Multiclass classification**: The target `y` has **more than 2 classes**  
  Example: `['studio', 'apartment', 'garden', 'penthouse']`

### âš™ï¸ `multi_class` â€“ How to Handle Multiple Classes

- `'ovr'` (One-vs-Rest):
  - Trains one binary model **per class**
  - Simpler, but less accurate in real multiclass problems
  - Example: for 4 classes, builds 4 separate models

- `'multinomial'` (recommended):
  - Uses **softmax** to handle all classes in one model
  - Better for accuracy, especially when classes are related
  - Requires solver: `'lbfgs'`, `'saga'`, `'newton-cg'`, or `'sag'`

### ğŸ› ï¸ `solver` â€“ Optimization Algorithm

The solver handles how the model is trained. Each one has different features:

| Solver      | Supports L1 | Supports L2 | Works with Multinomial | Notes                      |
|-------------|-------------|-------------|--------------------------|-----------------------------|
| `'lbfgs'`   | âŒ          | âœ…          | âœ…                       | Fast, stable, default choice |
| `'liblinear'` | âœ…        | âœ…          | âŒ                       | Binary only, good for small data |
| `'saga'`    | âœ…          | âœ…          | âœ…                       | Best for large datasets & L1 |
| `'newton-cg'` | âŒ        | âœ…          | âœ…                       | Accurate but slower |
| `'sag'`     | âŒ          | âœ…          | âœ…                       | For large datasets, not sparse |

### ğŸ§® `penalty='l1'` â€“ L1 Regularization (a.k.a. Lasso)

- L1 pushes some coefficients to **zero** â†’ simplifies the model
- You **donâ€™t apply it manually** â€” you specify it in the model:

```python
LogisticRegressionCV(penalty='l1', solver='saga')
```

---

## ğŸ§ª ×§×•×“ ×¤×™×™×ª×•×Ÿ ×œ×“×•×’××”:

```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegressionCV
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix

# Load dataset
df = pd.read_csv("iris.csv")
X = df.drop(columns="species")  # Features
y = df["species"]               # Target (3 flower types)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Normalize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train logistic regression model
model = LogisticRegressionCV(
    solver='lbfgs',             # Efficient for multiclass
    multi_class='multinomial', # Softmax-based multiclass prediction
    cv=5,
    max_iter=500
)
model.fit(X_train_scaled, y_train)

# Predict on test set and print report
y_pred = model.predict(X_test_scaled)
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Print learned coefficients
print("\nModel Coefficients (per class):")
coef_df = pd.DataFrame(model.coef_, columns=X.columns, index=model.classes_)
print(coef_df)

# Predict a specific custom case
sample = np.array([[5.0, 3.0, 1.5, 0.2]])  # Example input
sample_scaled = scaler.transform(sample)
predicted_class = model.predict(sample_scaled)[0]
predicted_proba = model.predict_proba(sample_scaled)

print(f"\nPrediction for input [5.0, 3.0, 1.5, 0.2]: {predicted_class}")
probs = predicted_proba[0]
class_labels = model.classes_

print("Class probabilities:")
for label, prob in zip(class_labels, probs):
    print(f"  {label}: {prob:.4f}")

# Plot confusion matrix heatmap
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=model.classes_,
            yticklabels=model.classes_)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix Heatmap")
plt.tight_layout()
plt.show()
```

<img src="log4.png" />

output:
```
lassification Report:
              precision    recall  f1-score   support

      setosa       1.00      1.00      1.00        19
  versicolor       1.00      1.00      1.00        13
   virginica       1.00      1.00      1.00        13

    accuracy                           1.00        45
   macro avg       1.00      1.00      1.00        45
weighted avg       1.00      1.00      1.00        45


Model Coefficients (per class):
            sepal_length  sepal_width  petal_length  petal_width
setosa         -2.379596     2.721570     -5.890557    -5.505229
versicolor      1.922642    -0.167737     -2.672822    -2.181061
virginica       0.456954    -2.553833      8.563378     7.686290

Prediction for input [5.0, 3.0, 1.5, 0.2]: setosa
Class probabilities:
  setosa: 0.9980
  versicolor: 0.0020
  virginica: 0.0000
```

---

# ğŸ  ×ª×¨×’×™×œ: ×—×™×–×•×™ ×¡×•×’ ×”×“×™×¨×” ×‘×¢×–×¨×ª ×¨×’×¨×¡×™×” ×œ×•×’×™×¡×˜×™×ª ××¨×•×‘×ª ××©×ª× ×™×

## ğŸ¯ ××˜×¨×ª ×”×ª×¨×’×™×œ:
×œ×‘× ×•×ª ××•×“×œ ×¨×’×¨×¡×™×” ×œ×•×’×™×¡×˜×™×ª ××¨×•×‘×ª ××©×ª× ×™× (Multivariable Logistic Regression) ×©×™×—×–×” ××ª **×¡×•×’ ×”×“×™×¨×”** ×¢×œ ×¤×™ ×××¤×™×™× ×™× ×›××•×ª×™×™× ×©×œ ×”× ×›×¡.



## ğŸ§  ××©×ª× ×™×:

| ××©×ª× ×”            | ×ª×™××•×¨                                   |
|------------------|-------------------------------------------|
| `area`           | ×©×˜×— ×”×“×™×¨×” ×‘××´×¨                           |
| `rooms`          | ××¡×¤×¨ ×—×“×¨×™×                                |
| `age`            | ×’×™×œ ×”×‘× ×™×™×Ÿ ×‘×©× ×™×                         |
| `distance`       | ××¨×—×§ ×××¨×›×– ×”×¢×™×¨ ×‘×§\"×                   |
| `type`           | ×¡×•×’ ×”×“×™×¨×” (×”××©×ª× ×” ×©× ×¨×¦×” ×œ×—×–×•×ª): `studio`, `apartment`, `penthouse`, `garden`


## ğŸ“‹ ×˜×‘×œ×ª × ×ª×•× ×™×:

<img src="apart.jpg"  style="width: 60%" />

× ××¦× ×‘×§×•×‘×¥ apartments.csv 

## ğŸ“Œ ××” ×¢×œ×™×š ×œ×¢×©×•×ª?

1. **×™×™×‘× ××ª ×”× ×ª×•× ×™×** ×œÖ¾`pandas.DataFrame`
2. ×”×¤×¨×“ ×‘×™×Ÿ `type` (×”××©×ª× ×” ×”×ª×œ×•×™) ×œ×‘×™×Ÿ ×©××¨ ×”××©×ª× ×™× (`X`)
3. ×—×œ×§ ××ª ×”× ×ª×•× ×™× ×œÖ¾Train/Test
4. × ×¨××œ ×‘×××¦×¢×•×ª StandardScaler
5. ××™××Ÿ ××•×“×œ `LogisticRegressionCV` ×¢×:
   - `multi_class='multinomial'`
   - `solver='lbfgs'`
   - `cv=5`
   5. ×”×“×¤×¡:
   - ×“×™×•×§ ×”××•×“×œ (`accuracy`)
   - ××ª ×”- confusion matrix
   - ××ª ×”- heat map
   - ×ª×—×–×™×ª ×œ×“×•×’××”: ×“×™×¨×” ×¢× 90 ××´×¨, 4 ×—×“×¨×™×, ×’×™×œ 10, ××¨×—×§ 4 ×§×´×

