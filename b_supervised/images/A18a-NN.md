## למידה עמוקה (Deep Learning)

<img src="deep2.jpg" align="right"/>

למידה עמוקה היא תחום בתוך למידת מכונה שמתמקד באלגוריתמים שמדמים את מבנה ותפקוד המוח האנושי – ובעיקר רשתות עצביות מלאכותיות
המטרה היא לגרום למחשב ללמוד בצורה היררכית מתוך הדאטה, מבלי שנצטרך להגדיר ידנית את המאפיינים החשובים

### דוגמה:

במקום להגדיר שכל תמונה של חתול כוללת שפם, אוזניים וזנב – המחשב לומד לבד את המאפיינים שמבדילים חתול מכלב

### מה זה אומר בפועל?

* רשתות עצביות עמוקות לומדות באופן הדרגתי – כל שכבה מזהה תבניות מורכבות יותר
* המודל לומד מתוך כמויות עצומות של דאטה, תוך כדי התאמה עצמאית של המשקלים
* תהליכי הלמידה כוללים סיווג (classification), חיזוי (regression) ואשכולים (clustering)

## שימושים בלמידה עמוקה

### 🎨 זיהוי תמונות

רשתות מסוג **CNN** מצליחות לזהות אובייקטים, פרצופים וסצנות – ברמת דיוק כמעט אנושית

### 🗣️ זיהוי דיבור

מערכות כמו **Siri**, **Google Assistant** ו־**Alexa** מבוססות למידה עמוקה כדי להבין ולנתח שפה מדוברת

### 📝 עיבוד שפה טבעית (NLP)

משימות כמו תרגום, סיכום טקסטים, ניתוח רגשות – מתבצעות על ידי מודלים עמוקים כמו **טרנספורמרים**

 **טרנספורמרים** סוג של ארכיטקטורת רשת עצבית שנועדה לעבד טקסטים בצורה חכמה ומדויקת
הוא שונה ממודלים קודמים בכך שהוא לא קורא מילה־אחר־מילה אלא מסתכל על כל המשפט בו זמנית

איך זה עובד?
הטרנספורמר משתמש במנגנון שנקרא **Attention** – כלומר "קשב"
במקום להתייחס לכל מילה באותה צורה, הוא שם יותר תשומת לב למילים החשובות בהקשר של המשפט

דוגמה במשפט:

"החתול רדף אחרי העכבר כי הוא היה רעב"

הטרנספורמר יכול להבין ש־"הוא" מתייחס ל־"החתול" ולא לעכבר כי הוא לומד את ההקשרים בתוך המשפט ולא רק את הסדר

למה זה חזק?

אפשר לאמן טרנספורמרים על כמויות עצומות של טקסט

הם עובדים מעולה גם בתרגום, גם בסיכום, וגם בהשלמת טקסט

המודלים הכי מתקדמים היום – כמו BERT, GPT, T5 – מבוססים על טרנספורמרים

**T5 Text-To-Text Transfer Transformer** 
מדובר בטרנספורמר מתקדם שפותח על ידי גוגל, שמבצע המון משימות שונות על טקסט 

### 🏥 בריאות

זיהוי מחלות מתמונות רנטגן, ניתוח MRI, ובניית תוכניות טיפול מותאמות אישית

### 🚗 רכבים אוטונומיים

רכבים חכמים משתמשים בלמידה עמוקה כדי לזהות תמרורים, הולכי רגל, כבישים ולנווט באופן עצמאי

## רשת עצבית מלאכותית (ANN - Artificial Neural Network)

רשת עצבית מלאכותית מדמה את פעולת הנוירונים במוח – כל **נוירון ברשת** מקבל קלטים, משקלות, מפעיל פונקציה ומעביר פלט
הרשת בנויה מכמה שכבות שכל אחת מהן אחראית על שלב אחר בעיבוד

### שלוש שכבות עיקריות:

* **שכבת קלט (Input Layer)** – מקבלת את הנתונים (features)
* **שכבה מוסתרת (Hidden Layer)** – מכילה את נוירוני הרשת שמבצעים את החישוב בפועל
* **שכבת פלט (Output Layer)** – מחזירה את התוצאה הסופית של המודל

### דוגמה:

תחזית מחירי דירות לפי שטח, מספר חדרים, מרחק מהעיר וגיל הדירה

<img src="deep1.png" style="width: 100%" />

## נוירון ברשת

### 🍖 מהו נוירון ביולוגי?

* נוירון במוח מקבל אותות חשמליים / כימיים ומעביר אותם הלאה
* המוח הוא "קופסה שחורה" שלא מבינה כלום בלי הקלטים מהנוירונים

### 🧠 מהו נוירון ברשת?

נוירון ברשת (Node) הוא יחידה בסיסית שמעבירה מידע בשכבות של הרשת
הוא מקבל קלטים (input), מחבר אותם עם משקלות (weights), מפעיל פונקציה, ומחזיר פלט

<img src="deep3.jpg" style="width: 80%" />

## רכיבי נוירון ברשת

### 📥 קלטים (Inputs)

כל נוירון ברשת מקבל ערכים מהשכבה הקודמת או מהדאטה עצמו

### ⚖️ משקלות (Weights)

לכל קלט יש משקל – כלומר כמה הוא חשוב
המודל לומד את המשקלות תוך כדי תהליך הלמידה

### 🔀 פונקציית הפעלה (Activation Function)

פונקציה מתמטית שמוסיפה עיקול (לא לינאריות) למודל
היא מאפשרת למודל ללמוד תבניות מורכבות

## סוגי בעיות לפי סוג הפלט

* 🔢 בעיה רציפה → פלט מספרי רציף (כמו מחיר)
* ❓ בעיה בינארית → פלט של כן / לא (0 או 1)
* 🧮 בעיה קטגורית → פלט מסוג מסוים (למשל: צבע = אדום, ירוק, כחול)

## שלבי הפעולה של נוירון ברשת

### 🔁 בכל איטרציה:

1. קבלת קלטים עם משקלות
2. כפל כל קלט במשקל שלו
3. סכימה של כל הקלטים הכפולים
4. הפעלת פונקציית הפעלה על התוצאה
5. הפלט מועבר לשכבה הבאה או מהווה תחזית סופית

💡 חשוב לבצע **נירמול** (Normalization) לקלטים כדי לשפר את ביצועי המודל

<img src="deep4.png" style="width: 100%" />

## איך נקבעים המשקלות הראשוניים?

המשקלות נבחרים אקראית בתחילת הדרך
שתי שיטות עיקריות:

* 🎲 התפלגות אחידה (Uniform): טווח אקראי כמו בין ‎-1 ל־1
* 🎲 התפלגות נורמלית (Normal): ממוצע 0 וסטיית תקן קטנה

💡 גם אם נזין לכל הנוירונים ברשת את אותם הקלטים – הם יפיקו פלטים שונים בגלל משקלות שונים

## פונקציית הפעלה (Activation Function)

כדי שהנוירון ברשת יוכל לזהות דפוסים מורכבים בדאטה, הוא צריך פונקציית הפעלה – שהיא בעצם פונקציה מתמטית שמוסיפה **לא־לינאריות** לפלט של הנוירון
אם לא נשתמש בפונקציית הפעלה, כל הרשת תתנהג כמו רגרסיה ליניארית פשוטה, ולא תוכל לפתור בעיות מורכבות

### 🎯 למה זה חשוב?

כי העולם לא לינארי – ולכן גם המודלים שלנו צריכים להכיל עקומות ודפוסים לא ישרים

## סוגים של פונקציות הפעלה

### ✅ Threshold Function

מוציאה ערכים של 0 או 1 לפי סף מסוים (למשל 0)

* אם סכום המשקלים והקלטים גדול או שווה ל־0 → הפלט יהיה 1
* אחרת → הפלט יהיה 0

<img src="deep5.png" style="width: 40%" />

### ✅ Sigmoid Function

הפונקציה הלוגיסטית – ממירה כל מספר לפלט רציף בין 0 ל־1
מצוינת למודלים של סיווג בינארי, כי אפשר להבין את הפלט כסבירות (הסתברות)

<img src="deep7.png" style="width: 40%" />

### ✅ ReLU (Rectified Linear Unit)

הכי נפוצה ברשתות עמוקות – מחזירה את הקלט אם הוא חיובי, ואם לא אז 0
פשוטה ומהירה – ולכן נפוצה בשכבות נסתרות

<img src="deep6.png" style="width: 40%" />

### ✅ Tanh (Hyperbolic Tangent)

ממירה את הקלט לפלט בין ‎-1 ל־1 – כלומר מרכז את הקלט סביב 0
מתאימה לדאטה שמכיל גם ערכים שליליים

<img src="deep8.png" style="width: 40%" />


### ✅ Softmax

ממירה את כל הפלטים של שכבת הפלט **להסתברויות** כך שהסכום שלהן יהיה 1
כל נוירון בשכבת הפלט מקבל ציון (logit) וה־Softmax ממירה את כל הציונים האלו לפלטים שנראים כמו הסתברות
הפלט של Softmax הוא **וקטור של הסתברויות** – לא מספר אחד

### 🎯 דוגמה:

אם המודל מנסה לסווג תמונה לחיה מתוך {חתול, כלב, סוס} והוא מחזיר:

* חתול → 0.7
* כלב → 0.2
* סוס → 0.1

אז הפלט הוא: `[0.7, 0.2, 0.1]` והסכום הוא 1

לא שהפלט הוא תמיד 1 – אלא שסך כל ההסתברויות בקטגוריות שווה ל־1ממירה את כל הפלטים של שכבת הפלט **להסתברויות** כך שהסכום שלהן יהיה 1
מצוינת לסיווג רב־קטגורי (Multi-Class Classification)

<img src="deep9.png" style="width: 40%" />

### ✅ Linear

משאירה את הפלט כמו שהוא – משמשת לרוב ברגרסיה (כאשר הפלט הוא מספר רציף ולא קטגוריה)

## מהי שכבה חבויה (Hidden Layer)

שכבה חבויה היא אחת מהשכבות הפנימיות של רשת נוירונים  
היא לא שכבת הקלט ולא שכבת הפלט אלא נמצאת ביניהן  
לכן קוראים לה שכבה חבויה כי היא לא חשופה לקלט המקורי ולא מוציאה את הפלט הסופי

<img src="deep11.png" style="width: 60%" />

### מה קורה בתוך שכבה חבויה

- כל נוירון בשכבה מקבל מספר קלטים מהשכבה הקודמת  
- לכל קלט יש משקל שמייצג עד כמה הוא חשוב  
- כל נוירון מבצע חישוב של סכום משוקלל ואז מפעיל עליו פונקציית הפעלה (Activation Function)  
- הפלט מכל נוירון עובר לשכבה הבאה

### למה צריך שכבות חבויות

- הן אלו שמבצעות את הלמידה האמיתית  
- הן מאפשרות למודל להבין קשרים מורכבים ודפוסים לא לינאריים  
- ככל שמוסיפים יותר שכבות חבויות ויותר נוירונים המודל יכול ללמוד בצורה עמוקה יותר  
אבל גם נהיה יותר איטי ועלול להיכנס לאוברפיטינג

## תהליך למידה אחורית (Backpropagation)

### 🧠 מה זה בעצם?

אחרי שהרשת מנסה לנבא משהו – היא בודקת כמה היא טעתה, ואז חוזרת אחורה ומעדכנת את המשקלים כדי לטעות פחות בפעם הבאה
זה כמו מורה שמתקן את עצמו לפי התוצאה של הבחינה

### ✏️ איך זה עובד בפועל?

1. מחשבים את הפלט של הנוירון כמו שלמדנו (סכום קלטים × משקלים → פונקציית הפעלה)
2. מחשבים את **שגיאת התחזית** (כמה רחוק הפלט מהתוצאה הנכונה)
3. מחשבים **נגזרת** של פונקציית העלות לפי כל משקל → כדי לדעת באיזה כיוון צריך לשנות אותו
4. מעדכנים את המשקלים בהתאם למהירות הלמידה (learning rate)
5. חוזרים על זה שוב ושוב עם כל הדאטה

### 💡 דוגמה:

אם אנחנו חוזים תוצאה – המודל ינסה לצמצם את השגיאה בין התחזית לערך האמיתי

<img src="deep10.png" style="width: 100%" />

## סוגי פונקציות עלות (Cost Functions)

* 🔢 בעיה רציפה (רגרסיה) → משתמשים ב־**MSE (Mean Squared Error)**
* 🧮 בעיה קטגורית (סיווג) → משתמשים ב־**Categorical Cross-Entropy**

**מה זה Categorical Cross-Entropy?**


זו פונקציית עלות שמתאימה לבעיות שבהן הפלט הוא קטגוריה אחת מתוך כמה אפשרויות
לדוגמה: סיווג תמונה לאחת מתוך {חתול, כלב, סוס}

**🎯 מה היא בודקת?**

עד כמה הפלט של המודל דומה לקטגוריה הנכונה
אם המודל ניחש נכון עם סבירות גבוהה → ערך נמוך
אם ניחש לא נכון או היה לא בטוח → ערך גבוה

**דוגמה מספרית:**

אם האמת היא "חתול" והמודל מחזיר את ההסתברויות:

* חתול → 0.8
* כלב → 0.15
* סוס → 0.05

אז הקרוס־אנטרופי יהיה נמוך – כי המודל היה בטוח ונכון

אם הפלט היה:

* חתול → 0.3
* כלב → 0.4
* סוס → 0.3

אז הקרוס־אנטרופי יהיה גבוה – כי המודל לא בטוח ולא מדויק

🧮 חישוב בפועל:

אם הקטגוריה הנכונה היא "חתול" (כלומר התווית היא [1, 0, 0]) והמודל נתן:

חתול → 0.3

כלב → 0.4

סוס → 0.3

אז החישוב יהיה:

Cross-Entropy = -log(0.3) ≈ 1.204

💡 כל נוירון ברשת עושה לעצמו backpropagation – אבל כל הרשת מתעדכנת בצורה **מתואמת** כדי לצמצם את הטעות הכוללת

## שיטות עדכון משקלות

* 🔁 Batch Gradient Descent → מחשב את הטעות על כל הדאטה ואז מעדכן
* 🔀 Stochastic Gradient Descent (SGD) → מעדכן משקלות כל פעם אחרי דוגמה אחת בלבד
* ⚖️ Mini-Batch Gradient Descent → פשרה בין השניים – מחשב על קבוצות קטנות בכל פעם


