# NLP FETAURES

## ×ž×”×• ×§×•×¨×¤×•×¡ (Corpus)

×§×•×¨×¤×•×¡ ×”×•× ××•×¡×£ ×’×“×•×œ ×©×œ ×˜×§×¡×˜×™× ×”×ž×©×ž×© ×œ×œ×™×ž×•×“ ×•× ×™×ª×•×— ×©×¤×” ×˜×‘×¢×™×ª. ×–×” ×™×›×•×œ ×œ×”×™×•×ª ×˜×§×¡×˜×™× ×¡×¤×¨×•×ª×™×™×, ×—×“×©×•×ª, ×ª×ž×œ×•×œ×™× ×©×œ ×“×™×‘×•×¨, ××ª×¨×™ ××™× ×˜×¨× ×˜ ×•×¢×•×“

×‘×§×•× ×˜×§×¡×˜ ×©×œ NLP, ×§×•×¨×¤×•×¡ ×ž×©×ž×© ×œ××™×ž×•×Ÿ, ×‘×“×™×§×” ×•×”×¢×¨×›×” ×©×œ ×ž×•×“×œ×™× â€“ ×œ×“×•×’×ž×”, ×ž×•×“×œ×™× ×©×œ ×—×œ×§×™ ×“×™×‘×¨ (POS) ××• ×–×™×”×•×™ ×™×©×•×™×•×ª (NER)

#### ×¡×•×’×™ ×§×•×¨×¤×•×¡×™×:

* **×§×•×¨×¤×•×¡ ×œ× ×ž×¡×•×ž×Ÿ** (raw corpus): ×˜×§×¡×˜ ×¨×’×™×œ ×œ×œ× ×ª×™×•×’
* **×§×•×¨×¤×•×¡ ×ž×¡×•×ž×Ÿ** (annotated corpus): ×›×•×œ×œ ×ž×™×“×¢ × ×•×¡×£ ×›×ž×• ×—×œ×§×™ ×“×™×‘×¨, ×ª×—×‘×™×¨, ×™×©×•×™×•×ª ×•×›×•'

ðŸ“Œ ×”- SpaCy ××•×ž× ×” ×¢×œ ×§×•×¨×¤×•×¡×™× ×ž×¡×•×ž× ×™×, ×ž×” ×©×ž××¤×©×¨ ×œ×” ×œ×”×‘×™×Ÿ ×”×§×©×¨×™× ×•×œ×–×”×•×ª ×ž×‘× ×™× ×œ×©×•× ×™×™× ×ž×•×¨×›×‘×™×

**×“×•×’×ž× 1**

```python
import spacy
nlp = spacy.load('en_core_web_sm')
doc = nlp("The quick brown fox jumped over the lazy dog's back.")

print(doc[4].text, doc[4].pos_, doc[4].tag_, spacy.explain(doc[4].tag_))
```

Output:
```
jumped VERB VBD verb, past tense
```

SpaCy decided that the token jumped is a verb with the VBD tag, meaning it's a verb in past tense

**×“×•×’×ž× 2**

```python
doc1 = nlp(u'I read books on NLP.')
doc2 = nlp(u'I read a book on NLP.')

print(f'{doc1[1].text:10} {doc1[1].pos_:8} {doc1[1].tag_:6} {spacy.explain(doc1[1].tag_)}')
print()
print(f'{doc2[1].text:10} {doc2[1].pos_:8} {doc2[1].tag_:6} {spacy.explain(doc2[1].tag_)}')
```

Output:
```
read       VERB     VBP    verb, non-3rd person singular present

read       VERB     VBD    verb, past tense
```

ðŸ“Œ SpaCy is smart enough to understand from the textual context that the first read token is in present tense, while the second read is in past tense

## Named Entity Recognition (NER)

×”- NER ×”×•× ×ª×”×œ×™×š ×©×‘×• ×ž×–×”×™× ×™×©×•×™×•×ª ×‘×©× ×‘×˜×§×¡×˜ (×›×ž×• ×©×ž×•×ª ×©×œ ×× ×©×™×, ×ž×§×•×ž×•×ª, ×—×‘×¨×•×ª, ×¡×›×•×ž×™×, ×ª××¨×™×›×™× ×•×›×•')

#### â€¢ **Named Entities**  
×‘× ×™×™×ª ×¨×©×™×ž×” ×©×œ ×™×©×•×™×•×ª ×‘×©× ×ž×ª×•×š ×”×˜×§×¡×˜, ×›×©×”×ž×¤×ª×— ×”×•× ×¡×•×’ ×”×™×©×•×ª ×•×”×¢×¨×›×™× ×”× ×”×ž×•×¤×¢×™× ×©×–×•×”×•  
×œ×“×•×’×ž×”:
- **Persons (PER)** â€“ ×©×ž×•×ª ×©×œ ×× ×©×™× ×›×ž×•: `"Albert Einstein"`, `"Marie Curie"`
- **Locations (LOC)** â€“ ×ž×§×•×ž×•×ª ×’×™××•×’×¨×¤×™×™× ×›×ž×•: `"Paris"`, `"Mount Everest"`

#### â€¢ **Recognition**  
×–×™×”×•×™ ×‘×¤×•×¢×œ ×©×œ ×”×ž×™×œ×™× ××• ×”×‘×™×˜×•×™×™× ×‘×˜×§×¡×˜ ×©×ž×™×™×¦×’×™× ×™×©×•×™×•×ª ×‘×©×  
×œ×“×•×’×ž×”: ×‘×ž×©×¤×˜ `"Barack Obama visited Paris"` â€“ × ×–×”×” ××ª `"Barack Obama"` ×•××ª `"Paris"` ×›×™×©×•×™×•×ª

#### â€¢ **Classification**  
×©×™×•×š ×›×œ ×™×©×•×ª ×©× ×ž×¦××” ×œ×§×˜×’×•×¨×™×” ×ž×ª××™×ž×” (×›×ž×• `PERSON`, `LOCATION`, `ORG`, `DATE` ×•×›×•')


### ×©×œ×‘×™×:

1. **Tokenization** â€“ ×¤×™×¦×•×œ ×”×˜×§×¡×˜ ×œ×ž×™×œ×™×
2. **Feature Extraction** â€“ ×”×¤×§×ª ×ž××¤×™×™× ×™× ×›×ž×• ×¦×•×¨×ª ×ž×™×œ×”, ××•×ª×™×•×ª ×’×“×•×œ×•×ª, ×¡×™×•×ž×•×ª
3. **Model Training** â€“ ××™×ž×•×Ÿ ×ž×•×“×œ ×¢×œ ×˜×§×¡×˜×™× ×ž×¡×•×ž× ×™× ×ž×¨××©
4. **Prediction** â€“ ×–×™×”×•×™ ×™×©×•×™×•×ª ×‘×˜×§×¡×˜ ×—×“×©

`Barack Obama was born on August 4, 1961, in Honolulu, Hawaii.`

#### 1. **Tokenization** â€“ ×¤×™×¦×•×œ ×”×˜×§×¡×˜ ×œ×ž×™×œ×™× (×˜×•×§× ×™×)  
×”- SpaCy ×ž×¤×¨×§×ª ××ª ×”×ž×©×¤×˜ ×œ×ž×™×œ×™× ×•×ª×•×•×™ ×¤×™×¡×•×§ â€“ ×›×ž×• `"Barack"`, `"Obama"`, `"August"`, `"4"`, `"1961"`, `","`, `"Hawaii"` ×•×›×•'

#### 2. **Feature Extraction** â€“ ×”×¤×§×ª ×ž××¤×™×™× ×™× ×ž×”×˜×•×§× ×™×  
×œ×›×œ ×ž×™×œ×” ×ž×—×•×œ×¦×™× ×ž××¤×™×™× ×™× ×›×ž×•:  
- ×”×× ×”××•×ª ×”×¨××©×•× ×” ×’×“×•×œ×”? (Barack)  
- ×”×× ×–×• ×ž×™×œ×” ×ž×¡×¤×¨×™×ª ××• ×ª××¨×™×š? (4, 1961)  
- ×”×× ×”×™× ×ž×ž×•×§×ž×ª ××—×¨×™ ×ž×™×œ×ª ×ž×¤×ª×— ×›×ž×• "born on"?  

×ž××¤×™×™× ×™× ××œ×• ×¢×•×–×¨×™× ×œ×ž×•×“×œ ×œ×”×‘×™×Ÿ ××™×œ×• ×ž×™×œ×™× ×¢×©×•×™×•×ª ×œ×”×™×•×ª ×™×©×•×™×•×ª

#### 3. **Model Training** â€“ (×›×‘×¨ ×‘×•×¦×¢ ×ž×¨××© ×¢×œ ×™×“×™ spaCy)  
×”×ž×•×“×œ ×©××•×ž×Ÿ ×¢×œ ×§×•×¨×¤×•×¡ ×’×“×•×œ ×™×•×“×¢ ×œ×–×”×•×ª ×ž×‘× ×™× ×©×œ ×©×ž×•×ª ×¤×¨×˜×™×™×, ×ª××¨×™×›×™×, ×¢×¨×™× ×•×¢×•×“

#### 4. **Prediction** â€“ ×—×™×–×•×™ ×”×™×©×•×™×•×ª ×‘×˜×§×¡×˜  
×”×ž×•×“×œ ×ž×–×”×” ××ª ×”×™×©×•×™×•×ª ×”×‘××•×ª:

```
Barack Obama â†’ PERSON â†’ ××“× (××ž×™×ª×™ ××• ×‘×“×™×•× ×™)
August 4, 1961 â†’ DATE â†’ ×ª××¨×™×š
Honolulu â†’ GPE â†’ ×ž×“×™× ×” / ×¢×™×¨ / ××–×•×¨ ×’×™××•×’×¨×¤×™
Hawaii â†’ GPE â†’ ×ž×“×™× ×” / ×¢×™×¨ / ××–×•×¨ ×’×™××•×’×¨×¤×™
```

ðŸ“Œ ×›×š spaCy ×ž×¦×œ×™×—×” ×œ×”×‘×™×Ÿ ×ž×ª×•×š ×”×”×§×©×¨ ×•×œ×¡×•×•×’ × ×›×•×Ÿ ×›×œ ×™×©×•×ª ×‘×˜×§×¡×˜

**×¤×™×™×ª×•×Ÿ**

```python
doc = nlp("Barack Obama was born on August 4, 1961, in Honolulu, Hawaii.")
for ent in doc.ents:
    print(ent.text, ent.label_, spacy.explain(ent.label_))
```

Output:
```
Barack Obama PERSON People, including fictional
August 4, 1961 DATE Absolute or relative dates or periods
Honolulu GPE Countries, cities, states
Hawaii GPE Countries, cities, states
```

ðŸ“Œ ×”- SpaCy ×ž×—×–×™×¨ ×™×©×•×™×•×ª ×¢× label ×›×ž×• `PERSON`, `DATE`, `GPE` ×•×ž××¤×©×¨ ×œ×”×¡×‘×™×¨ ×›×œ label ×‘××ž×¦×¢×•×ª `spacy.explain`

---

### ðŸ§  ×”×ª××ž×” ××™×©×™×ª ×©×œ NER ×‘Ö¾spaCy

#### ×œ×ž×” × ×¨×¦×” ×œ×”×•×¡×™×£ ×™×©×•×™×•×ª ×™×“× ×™×ª?
×œ×¤×¢×ž×™× spaCy ×œ× ×ž×–×”×” ×™×©×•×™×•×ª ×©×× ×—× ×• ×›×Ÿ ×¨×•×¦×™× ×œ×–×”×•×ª â€“ ×œ×“×•×’×ž×”, "Tesla" ×œ× ×ž×–×•×”×” ×›×‘×¨×™×¨×ª ×ž×—×“×œ ×›×™×©×•×ª ×ž×¡×•×’ ORG  
×‘×ž×§×¨×™× ×›××œ×” × ×•×›×œ ×œ×”×•×¡×™×£ ××ª ×”×™×©×•×ª ×™×“× ×™×ª ×œ×ž×¡×ž×š

#### ×”×©×œ×‘×™× ×œ×”×•×¡×¤×ª ×™×©×•×ª ×™×“× ×™×ª:
1. ×©×œ×™×¤×ª ×”×¢×¨×š ×”×ž×¡×¤×¨×™ (hash) ×©×œ ×”×ª×•×•×™×ª ×”×¨×¦×•×™×” (×œ×ž×©×œ "ORG")
2. ×™×¦×™×¨×ª Span ×ž×”×ž×™×œ×” ××• ×”×‘×™×˜×•×™ ×”×¨×¦×•×™
3. ×”×•×¡×¤×ª ×”Ö¾Span ×œ×¨×©×™×ž×ª ×”×™×©×•×™×•×ª ×‘×ž×¡×ž×š `doc.ents`

```python
import spacy
nlp = spacy.load('en_core_web_sm')

doc = nlp(u'Tesla to build a U.K. factory for $6 million')
for ent in doc.ents:
    print(ent.text, ent.label_, spacy.explain(ent.label_))  

# adding Tesla entity
from spacy.tokens import Span

ORG = doc.vocab.strings["ORG"]
new_ent = Span(doc, start=0, end=1, label=ORG)
doc.ents = list(doc.ents) + [new_ent]

for ent in doc.ents:
    print(ent.text, ent.label_, spacy.explain(ent.label_))  
```

Output:
```
-Before
U.K. GPE Countries, cities, states
$6 million MONEY Monetary values, including unit

-After
Tesla ORG Companies, agencies, institutions, etc.
U.K. GPE Countries, cities, states
$6 million MONEY Monetary values, including unit
```

### ×“×•×’×ž×” ×ž×ª×§×“×ž×ª â€“ Phrase Matcher
×× × ×¨×¦×” ×œ×–×”×•×ª ×‘×™×˜×•×™×™× ×©×œ×ž×™× ×›×ž×•:
- `dashboard website`
- `dashboard-website`

In this example, we demonstrate how to use SpaCyâ€™s `PhraseMatcher` to detect custom named entities that are not recognized by default

#### Step 1: Default Behavior â€“ No Entities Detected

```python
doc = nlp(u"Our company plans to introduce a new dashboard-website. If successful, the dashboard website\
 will be our main customer payed product")

for ent in doc.ents:
    print(ent.text, ent.label_, spacy.explain(ent.label_))  
```

Output:
```
No named entities found.
```

#### Step 2: Define Phrase Matcher Patterns

We want to recognize ```"dashboard website"``` and ```"dashboard-website"``` as **PRODUCT** entities. First, we use PhraseMatcher to define patterns for them

```python
from spacy.matcher import PhraseMatcher

matcher = PhraseMatcher(nlp.vocab)

phrase_list = ['dashboard website', 'dashboard-website']
phrase_patterns = [nlp(text) for text in phrase_list]

matcher.add('newproduct', phrase_patterns)

matches = matcher(doc)
matches
```

Output:
```
[(2689272359382549672, 7, 10), (2689272359382549672, 15, 17)]
```

âœ… We created two phrase patterns: one for "dashboard website" and one for "dashboard-website"

âœ… Added both patterns under the label 'newproduct'

âœ… SpaCy successfully matched both phrases within the doc and returned their span positions

#### Step 3: Add Matched Spans to `doc.ents`

After using the `PhraseMatcher` to find matching phrases in the text, we can now manually add them to SpaCyâ€™s `doc.ents` list so that they are treated as proper named entities.

```python
from spacy.tokens import Span

PROD = doc.vocab.strings['PRODUCT']

new_ents = [Span(doc, match[1], match[2], label=PROD) for match in matches]

doc.ents = list(doc.ents) + new_ents

for ent in doc.ents:
    print(ent.text, ent.label_, spacy.explain(ent.label_))  
```    

Output:
```
dashboard-website PRODUCT Objects, vehicles, foods, etc. (not services)
dashboard website PRODUCT Objects, vehicles, foods, etc. (not services)
```

---

### Sentence Segmentation

### ×¤×™×¦×•×œ ×œ×ž×©×¤×˜×™× (Sentence Segmentation) ðŸ§©

**×¤×™×¦×•×œ ×ž×©×¤×˜×™×** â€“ ×ª×”×œ×™×š ×‘Ö¾NLP ×©×ž×˜×¨×ª×• ×œ×—×œ×§ ×˜×§×¡×˜ ×©×œ× ×œ×ž×©×¤×˜×™× × ×¤×¨×“×™×. ×”×ž×˜×¨×” ×”×™× ×œ×–×”×•×ª ×ž×ª×™ ×ž×¡×ª×™×™× ×ž×©×¤×˜ ××—×“ ×•×ž×ª×—×™×œ ×ž×©×¤×˜ ×—×“×©

#### ðŸŸ£ ×”×’×™×©×” ×”× ×¤×•×¦×”

×‘×“×¨×š ×›×œ×œ, ×¤×™×¦×•×œ ×ž×©×¤×˜×™× ×ž×ª×‘×¦×¢ ×œ×¤×™ ×¡×™×ž× ×™ ×¤×™×¡×•×§ ×›×ž×•:
- × ×§×•×“×” `.`
- ×¡×™×ž×Ÿ ×©××œ×” `?`
- ×¡×™×ž×Ÿ ×§×¨×™××” `!`

#### ðŸŸ  ×ž×’×‘×œ×•×ª ×”×’×™×©×” ×”×¤×©×•×˜×”

×œ×ž×¨×•×ª ×©×”×©×™×˜×” ×”×–×• ×¤×©×•×˜×”, ×”×™× ×¢×œ×•×œ×” ×œ×”×™×•×ª **×œ× ×ž×“×•×™×§×ª** ×‘×’×œ×œ ×”×ž×•×¨×›×‘×•×ª ×©×œ ×”×©×¤×”:

- × ×§×•×“×•×ª ×ž×•×¤×™×¢×•×ª ×’× ×‘**×§×™×¦×•×¨×™×** (×œ×ž×©×œ: `×“"×¨`, `××¨×”"×‘`)
- **×ž×¡×¤×¨×™× ×¢×©×¨×•× ×™×™×** (×›×ž×• `3.14`)
- **×©×œ×•×© × ×§×•×“×•×ª** (`...`)

×¡×™×ž× ×™× ××œ×” ×œ× ×‘×”×›×¨×— ×ž×¡×ž× ×™× ×¡×•×£ ×ž×©×¤×˜.

#### ðŸŸ¢ ×¤×ª×¨×•× ×•×ª ×ž×ª×§×“×ž×™×

×ž×•×“×œ×™× ×ž×ª×§×“×ž×™× ×‘×•×—× ×™× ×’× ×’×•×¨×ž×™× × ×•×¡×¤×™×:

- **××•×ª×™×•×ª ×¨×™×©×™×•×ª** (×”×× ×”×ž×™×œ×” ×©××—×¨×™×” ×ž×ª×—×™×œ×” ×‘××•×ª ×’×“×•×œ×”)
- **×”×§×©×¨ ×ª×—×‘×™×¨×™**
- **×ž×™×œ×™× ×©×¤×•×ª×—×•×ª ×ž×©×¤×˜×™× ×‘×“×¨×š ×›×œ×œ**

#### ðŸ§  ×œ×ž×” ×–×” ×—×©×•×‘?

×¤×™×¦×•×œ × ×›×•×Ÿ ×œ×ž×©×¤×˜×™× ×”×•× ×‘×¡×™×¡ ×—×©×•×‘ ×œ×ž×©×™×ž×•×ª NLP ×ž×ª×§×“×ž×•×ª ×›×ž×•:
- ×ª×¨×’×•× ×ž×›×•× ×”
- ×¡×™×›×•× ×˜×§×¡×˜×™×
- × ×™×ª×•×— ×¡× ×˜×™×ž× ×˜

×‘×’×œ×œ ×©×¤×¢×•×œ×•×ª ××œ×• ×ž×‘×•×¦×¢×•×ª ×œ×¢×™×ª×™× ×§×¨×•×‘×•×ª **×‘×¨×ž×ª ×ž×©×¤×˜**, ×¤×™×¦×•×œ ×œ× ×ž×“×•×™×§ ×™×¤×’×¢ ×‘×“×™×•×§ ×”×ª×•×¦××•×ª

×›××©×¨ ×× ×• ×ž×¤×¦×œ×™× ×˜×§×¡×˜ ×œ×ž×©×¤×˜×™× ×‘×¢×–×¨×ª SpaCy, ×”×¤×œ×˜ × ×©×ž×¨ ×‘××•×‘×™×™×§×˜ ×‘×©× `doc.sents`  
×–×”×• ×œ× ×ž×ž×© ×¨×©×™×ž×”, ××œ× **generator** â€“ ××•×‘×™×™×§×˜ ×©×ž×—×–×™×¨ ×›×œ ×ž×©×¤×˜ ×‘× ×¤×¨×“ ×‘×œ×•×œ××”

```python
import spacy
nlp = spacy.load('en_core_web_sm')

doc = nlp('This is the first sentence. This is another sentence. This is the last sentence.')

for sent in doc.sents:
    print(sent)
```

Output:
```
This is the first sentence.
This is another sentence.
This is the last sentence.
```

×”×¢×¨×”: ×›×“×™ ×œ×”×©×ª×ž×© ×‘×™×›×•×œ×•×ª ×©×œ ×¨×©×™×ž×•×ª (×›×ž×• ×©×œ×™×¤×” ×œ×¤×™ ××™× ×“×§×¡), ×¦×¨×™×š ×œ×”×ž×™×¨ ××ª doc.sents ×œ×¨×©×™×ž×” ×™×“× ×™×ª:

```python
doc_sents = [sent for sent in doc.sents]

print(doc_sents[0])
```

Output:
```
This is the first sentence.
```

### ×›×œ×œ ×ž×•×ª×× ×œ×¤×™×¦×•×œ ×ž×©×¤×˜×™× ×œ×¤×™ ×ª×• ×ž×•×ª×× ××™×©×™×ª (×›×ž×• `;`) âœ‚ï¸

×‘Ö¾SpaCy ×‘×¨×™×¨×ª ×”×ž×—×“×œ ×œ×¤×™×¦×•×œ ×ž×©×¤×˜×™× ×ž×ª×‘×¦×¢×ª ×œ×¤×™ ×¡×™×ž× ×™ ×¤×™×¡×•×§ ×‘×¡×™×¡×™×™× ×›×ž×• × ×§×•×“×” (`.`).  
××š ×œ×¤×¢×ž×™× ×”×ž×©×¤×˜ ×›×•×œ×œ ×ª×•×•×™× × ×•×¡×¤×™× ×©×™×›×•×œ×™× ×œ×¡×ž×Ÿ ×¡×™×•× ×ž×©×¤×˜ â€“ ×œ×ž×©×œ **× ×§×•×“×”Ö¾×¤×¡×™×§** (`;`)

#### ×‘×¢×™×”:

×‘×“×•×’×ž×” ×”×‘××”, SpaCy ×œ× ×ž×¤×¦×œ ××ª ×”×ž×©×¤×˜ ×œ×©× ×™×™×, ×œ×ž×¨×•×ª ×©×™×© `;` ×©×ž×¤×¨×™×“ ×‘×™×Ÿ ×¨×¢×™×•× ×•×ª ×©×•× ×™×:

```python
doc = nlp('Management is doing things right; leadership is doing the right things. â€“Peter Drucker')

for sent in doc.sents:
    print(sent)
```

Output:
```
Management is doing things right; leadership is doing the right things.
â€“Peter Drucker
```

#### ×¤×™×ª×¨×•×Ÿ:

**× ×’×“×™×¨ ×¤×•× ×§×¦×™×” ×©×ª×—×¤×© ×ª×• ; ×•×ª×¡×ž×Ÿ ××ª ×”×˜×•×§×Ÿ ×”×‘× ××—×¨×™×• ×›×”×ª×—×œ×” ×©×œ ×ž×©×¤×˜ ×—×“×©**

×”×¤×•× ×§×¦×™×” ×ª×™×¨×©× ×œ×¨×¦×£ ×”×¤×¢×•×œ×•×ª (pipeline) ×©×œ SpaCy ×›Ö¾component ×—×“×©

```python
from spacy.language import Language

@Language.component('set_custom_boundaries')
def set_custom_boundaries(doc):
    for token in doc[:-1]:
        if token.text == ';':
            doc[token.i + 1].is_sent_start = True
    return doc
```

ðŸ“Œ ×ž×” ×”×¤×•× ×§×¦×™×” ×¢×•×©×”:

×¢×•×‘×¨×ª ×¢×œ ×›×œ ×”×˜×•×§× ×™× ×‘×ž×¡×ž×š (×—×•×¥ ×ž×”××—×¨×•×Ÿ)

×× ×”×™× ×ž×•×¦××ª `;` ×”×™× ×ž×¡×ž× ×ª ××ª ×”×˜×•×§×Ÿ ×”×‘× (token.i + 1) ×›×”×ª×—×œ×” ×©×œ ×ž×©×¤×˜ (is_sent_start = True)

×”×¤×•× ×§×¦×™×” ×ž×—×–×™×¨×” ××ª ×”Ö¾doc ×¢× ×”×¢×“×›×•×Ÿ

**×”- anotaton ×©×œ Language.component ðŸ§©**

×”Ö¾`@Language.component` ×”×•× ×§×™×©×•×˜ (decorator) ×‘Ö¾SpaCy ×©×ž××¤×©×¨ ×œ×¨×©×•× ×¤×•× ×§×¦×™×” ×ž×•×ª××ž×ª ××™×©×™×ª ×›×©×œ×‘ (component) ×‘Ö¾pipeline ×©×œ SpaCy.  
×‘××ž×¦×¢×•×ª×• ×× ×• ×ž×•×“×™×¢×™× ×œÖ¾SpaCy ×©×”×¤×•× ×§×¦×™×” ×©×™×¦×¨× ×• ×¦×¨×™×›×” ×œ×”×™×•×ª ×ž×•×¤×¢×œ×ª ×›×—×œ×§ ×ž×ª×”×œ×™×š ×”×¢×™×‘×•×“ ×©×œ ×”×˜×§×¡×˜.

ðŸ“Œ ×–×” ×©×™×ž×•×©×™ ×‘×ž×™×•×—×“ ×›×©×× ×—× ×• ×¨×•×¦×™× ×œ×”×•×¡×™×£ ×—×•×§×™× ×ž×•×ª××ž×™× ××™×©×™×ª â€“ ×›×ž×• ×›×œ×œ ×œ×¤×™×¦×•×œ ×ž×©×¤×˜×™× ×œ×¤×™ `;`  
××• ×œ×‘×¦×¢ × ×™×ª×•×— ×ž×•×ª×× ×œ×¤× ×™ ××• ××—×¨×™ ×©×œ×‘×™× ×›×ž×• POS ××• NER.

**×”- SpaCy Pipeline ðŸ› ï¸**

×”Ö¾**Pipeline** ×©×œ SpaCy ×”×•× ×¨×¦×£ ×©×œ ×©×œ×‘×™× ×©×ž×•×¤×¢×œ×™× ×¢×œ ×”×˜×§×¡×˜ ×›×“×™ ×œ×”×ž×™×¨ ××•×ª×• ×ž×˜×§×¡×˜ ×’×•×œ×ž×™ ×œ×ž×™×“×¢ ×©× ×™×ª×Ÿ ×œ× ×ª×— ×•×œ×¢×‘×•×“ ××™×ª×• ×‘×ª×•×›× ×”.

×›××©×¨ ×™×•×¦×¨×™× ××•×‘×™×™×§×˜ ×ž×¡×•×’ `Doc`, ×”×•× ×¢×•×‘×¨ ××•×˜×•×ž×˜×™×ª ×“×¨×š ×›×œ ×©×œ×‘×™ ×”Ö¾pipeline.

âœ… ×”×©×œ×‘×™× ×”× ×¤×•×¦×™×:
- ×¤- Tokenization â€“ ×¤×™×¦×•×œ ×”×˜×§×¡×˜ ×œ×ž×™×œ×™×
- ×¤- POS Tagging â€“ ×–×™×”×•×™ ×—×œ×§×™ ×“×™×‘×¨
- ×¤- Dependency Parsing â€“ × ×™×ª×•×— ×ª×—×‘×™×¨×™
- ×¤- Lemmatization â€“ ×”×¤×©×˜×ª ×ž×™×œ×™× ×œ×©×•×¨×©
- ×¤- NER â€“ ×–×™×”×•×™ ×™×©×•×™×•×ª
- ×¤- Sentence Segmentation â€“ ×¤×™×¦×•×œ ×œ×ž×©×¤×˜×™×

**×”×ª××ž×” ××™×©×™×ª ×©×œ ×”Ö¾Pipeline ðŸ§©**

× ×™×ª×Ÿ ×œ×©× ×•×ª ××ª ×”Ö¾pipeline ×›×“×™ ×œ×”×ª××™× ×œ×¦×¨×›×™× ×©×•× ×™×:

- âŒ ×œ×”×¡×™×¨ ×©×œ×‘×™× ×©×œ× ×¦×¨×™×›×™× (×›×ž×• NER ×× ×œ× × ×“×¨×©)
- âž• ×œ×”×•×¡×™×£ ×©×œ×‘×™× ×ž×•×ª××ž×™× ×ž×©×œ×›× (×œ×ž×©×œ × ×™×ª×•×— ×¨×’×©×•×ª, ×¡×™×•×•×’ ×˜×§×¡×˜ ×•×›×•')

ðŸ“Œ ×—×©×•×‘ ×œ×”×‘×™×Ÿ:
- ×”×©×œ×‘×™× ×ž×•×¤×¢×œ×™× **×‘×¡×“×¨ ×§×‘×•×¢**
- ×‘×›×œ ×©×œ×‘, ×”××•×‘×™×™×§×˜ `Doc` ×ž×ª×¢×“×›×Ÿ ×¢× ×”×ž×™×“×¢ ×©× ×•×¦×¨
- ×œ×›×Ÿ **×”×¡×“×¨ ×©×‘×• ×”×©×œ×‘×™× ×ž×•×¤×¢×œ×™× ×—×©×•×‘ ×ž××•×“**

**×”×•×¡×¤×ª ×¨×›×™×‘ ×ž×•×ª×× ×œÖ¾pipeline ×©×œ SpaCy ðŸ§±**

×œ××—×¨ ×©×™×¦×¨× ×• ×›×œ×œ ×—×“×© ×œ×¤×™×¦×•×œ ×ž×©×¤×˜×™× (×œ×ž×©×œ ×œ×¤×™ `;`), ××¤×©×¨ ×œ×”×•×¡×™×£ ××•×ª×• ×›×¨×›×™×‘ (component) ×‘×ª×•×š ×”Ö¾pipeline ×©×œ SpaCy.

×œ×©× ×›×š, ×¢×œ×™× ×• ×œ×¦×™×™×Ÿ:
- ××ª **×©× ×”×¨×›×™×‘** ×©×™×¦×¨× ×• (`set_custom_boundaries`)
- ×•××ª **×”×ž×™×§×•× ×©×œ×•** ×‘Ö¾pipeline (×œ×ž×©×œ: ×œ×¤× ×™ ×©×œ×‘ `parser`)

```python
from spacy.language import Language
import spacy

@Language.component('set_custom_boundaries')
def set_custom_boundaries(doc):
    for token in doc[:-1]:
        if token.text == ';':
            doc[token.i + 1].is_sent_start = True
    return doc

nlp = spacy.load('en_core_web_sm')

nlp.add_pipe('set_custom_boundaries', before='parser')
nlp.pipe_names
```

Output:
```
['tok2vec',
 'tagger',
 'set_custom_boundaries',  # --> new pipeline component
 'parser',
 'attribute_ruler',
 'lemmatizer',
 'ner']
```

×•×¢×›×©×™×• × ×¨×™×¥ ×©×•×‘: 

```python
doc = nlp('Management is doing things right; leadership is doing the right things. â€“Peter Drucker')

for sent in doc.sents:
    print(sent)
```

Output:
```
Management is doing things right;
leadership is doing the right things.
â€“Peter Drucker
```
