# ×™×¢×¨ ×¨× ×“×•××œ×™ (Random Forest)

## ×”×‘×¢×™×” ×©×¨×•×¦×™× ×œ×¤×ª×•×¨

×¢×¦×™ ×”×—×œ×˜×” ×‘×•×“×“×™× ×¡×•×‘×œ×™× ××‘×¢×™×•×ª ×›××•:
- **Overfitting** â€” ×”×¢×¥ ×œ×•××“ ×˜×•×‘ ××“×™ ××ª ×”×“××˜×” ×•×œ× ××¡×•×’×œ ×œ×—×–×•×ª ×¢×œ ×“××˜×” ×—×“×©
- **×”×ª×¢×œ××•×ª ××ª×›×•× ×•×ª** â€” ×”×¢×¥ ××ª××§×“ ×¨×§ ×‘×—×œ×§ ××”×ª×›×•× ×•×ª ×‘×¤×™×¦×•×œ×™× (×›×™ ×¢×¥ ×”×—×œ×˜×” ×¨×’×™×œ ×‘× ×•×™ ×›×š ×©×”×•× ×‘×›×œ ×©×œ×‘ ×‘×•×—×¨ ××ª ×”×¤×™×¦×•×œ ×”×›×™ ×˜×•×‘ â€” ×›×œ×•××¨ ××ª ×”×ª×›×•× ×” ×©××‘×™××” ×œ×”×¤×¨×“×” ×”×›×™ ×‘×¨×•×¨×” ×‘×™×Ÿ ×”××—×œ×§×•×ª)

**×“×•×’××”**:

× × ×™×— ×©×× ×• ×¨×•×¦×™× ×œ×—×–×•×ª ×× ×œ×§×•×— ×™×§×‘×œ ××©×¨××™, ×‘×”×ª×‘×¡×¡ ×¢×œ ×’×™×œ ×•××©×›×•×¨×ª. ×¢×¥ ×”×—×œ×˜×” ×™×—×™×“ ×¢×©×•×™ ×œ×‘× ×•×ª ×—×•×§×™× ××“×•×™×§×™× ×××•×“ ×œ×“××˜×” ×”×§×™×™×, ××š ×™×™×›×©×œ ×œ×—×–×•×ª × ×›×•× ×” ×œ×§×•×—×•×ª ×—×“×©×™×

## ××™×š Random Forest ×¤×•×ª×¨ ××ª ×”×‘×¢×™×”?

×‘××§×•× ×œ×‘× ×•×ª **×¢×¥ ××—×“**, ×™×¢×¨ ×¨× ×“×•××œ×™ ×‘×•× ×” **×”×¨×‘×” ×¢×¦×™× ×©×•× ×™×** ×¢×œ **×—×œ×§×™× ×©×•× ×™× ××”××™×“×¢** ×•**×—×œ×§×™× ×©×•× ×™× ××”×ª×›×•× ×•×ª**:

- ×‘×‘×¢×™×™×ª ×¡×™×•×•×’ (Classification) â†’ ×œ×•×§×—×™× **×¨×•×‘ ×§×•×œ×•×ª** (Voting)
- ×‘×‘×¢×™×™×ª ×¨×’×¨×¡×™×” (Regression) â†’ ×œ×•×§×—×™× **×××•×¦×¢** ×©×œ ×”×ª×—×–×™×•×ª

## ×ª×”×œ×™×š ×¤×¢×•×œ×” ×©×œ Random Forest

1. ×‘×—×™×¨×” **××“×’× ××§×¨××™ ×¢× ×”×—×–×¨×”** ××”×“××˜×” (Bootstrapping)
2. ×‘×›×œ ×¤×™×¦×•×œ: **×‘×—×™×¨×” ××§×¨××™×ª** ×©×œ ×§×‘×•×¦×ª ×ª×›×•× ×•×ª
3. ×‘×¢×¨×š 63% ××”×“××˜×” × ×›× ×¡ ×œ×›×œ ×¢×¥ (×œ×¤×™ ×”×¡×ª×‘×¨×•×ª), ×›×œ×•××¨ ×›Ö¾37% ××”×©×•×¨×•×ª ×œ× ××•×¤×™×¢×•×ª ×‘×›×œ×œ ×‘××•×ª×• ×¢×¥
4. ×‘× ×™×™×ª ×¢×¥ ×¢×“ ×”×¡×•×£ (×œ×œ× ×’×™×–×•×)
5. ×—×–×¨×” ×¢×œ ×”×©×œ×‘×™× ×›×“×™ ×œ×‘× ×•×ª ×”×¨×‘×” ×¢×¦×™×
6. ×ª×—×–×™×ª ×¡×•×¤×™×ª ×¢×œ ×™×“×™ ×¨×•×‘ ×§×•×œ×•×ª ××• ×××•×¦×¢

### âŒ Why Random Forest Skips Pruning

Random Forest uses a **different strategy** to avoid overfitting:

- Each tree is trained on a **random subset** of the data (bootstrapping)
- At each split, only a **random subset of features** is considered
- Each tree is grown **fully** (no pruning)

Instead of simplifying individual trees, Random Forest relies on the **diversity of the forest** â€” the idea that many different trees making mistakes in different ways will average out to a good prediction.

Why is This Effective?

- A single tree might overfit â€” but averaging many trees leads to stable predictions
- Fully grown trees can capture patterns better; randomness reduces the risk of memorizing noise
- No need to manually tune tree size â€” the ensemble handles it through averaging

### ğŸ” Does Random Forest Use Gini Impurity?

Yes â€” by default, **Random Forest uses Gini Impurity** to decide how to split nodes in each decision tree it builds.

What is Gini Impurity?

Gini Impurity is a measure of how "pure" a node is. In classification:
- A node is **pure** if all examples belong to the same class.
- A node is **impure** if there's a mix of classes.

The formula for Gini Impurity is:

\[
G = 1 - \sum_{i=1}^{C} p_i^2
\]

Where:
- \( C \) is the number of classes
- \( p_i \) is the probability of a sample being in class \( i \) at that node

The goal is to **choose the split that minimizes impurity**

**Why is Gini used in Random Forest?**

Because each tree in the forest is a regular decision tree (CART), and CART uses Gini by default.
It is:
- Fast to compute
- Works well for classification
- Often performs similarly to entropy (information gain), but with less computation

**Can I use something else?**

Yes! You can use **entropy** instead by setting the `criterion` parameter:

```python
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(criterion="entropy")
```

This uses **information gain** instead of Gini to evaluate splits.


In short: Why use Gini? | It's fast, simple, and performs well in practice. **Random Forest relies on Gini Impurity to guide how each tree splits the data â€” unless you tell it otherwise**

### ğŸŒ² Does Random Forest Use CART Trees?

Yes â€” **Random Forest uses CART (Classification and Regression Trees)** as the base tree model for both classification and regression tasks.

**What is CART?**

**CART** stands for **Classification and Regression Trees**, introduced by Breiman et al. in 1986. It's a type of decision tree algorithm with the following characteristics:

âœ… Key Features of CART:
- **Binary Splits Only**: Every internal node splits into exactly two branches (left/right)
- **Supports Two Tasks**:
  - **Classification** â†’ typically uses **Gini Impurity** or **Entropy**
  - **Regression** â†’ uses **Mean Squared Error (MSE)**
- **No Multi-way Splits**: Unlike algorithms like ID3 or C4.5, CART only splits one feature at a time into two groups
- **Can Grow Fully**: Unless explicitly pruned or limited, CART trees grow until all leaves are pure or stopping conditions are met

**ğŸŒ³ How CART Fits into Random Forest**

Random Forest is essentially a collection (an ensemble) of CART trees:
- Each tree is built using the CART algorithm
- Random Forest adds randomness by:
  - Bootstrapping the data (random samples with replacement)
  - Randomly selecting features for each split (`max_features`)

This combination allows the forest to reduce overfitting while keeping the power of CART.

**Summary**

| Feature | CART | Random Forest |
|--------|------|----------------|
| Tree Type | Binary-only CART | Ensemble of CART trees|
| Supports Classification? | âœ… | âœ… |
| Supports Regression? | âœ… | âœ… |
| Uses pruning? | Optional | âŒ Not used by default |
| Split criterion | Gini, Entropy, MSE | Gini/Entropy per tree |

Ensemble = a method that combines multiple models (="weak learners") to produce a stronger, more accurate model

Random Forest builds **many CART trees** with added randomness â€” and combines their predictions to form a powerful, robust model

### âš–ï¸ Does Random Forest Require Feature Scaling?

No â€” **Random Forest does not require feature scaling** such as standardization or normalization.

**Why Not?**

Random Forest is based on **decision trees**, which split data using thresholds like:
```text
Is feature X <= some value?
```

This splitting behavior is **not affected** by the magnitude or distribution of the feature values.

- Whether a feature ranges from 0â€“1 or 0â€“1000, the tree only cares about where to split the data to separate classes.
- Unlike models like Logistic Regression or KNN, **distance and scale do not impact the treeâ€™s logic.**

**When Is Scaling Needed?**

Feature scaling is needed when the algorithm relies on **distance, gradient optimization, or numerical stability**

Examples:
- âœ… **Needed**:
  - Logistic Regression
  - Linear Regression (if using gradient descent)
  - Polynomial Regression
  - K-Nearest Neighbors (KNN)
  - Support Vector Machines (SVM)
- âŒ **Not Needed**:
  - Decision Trees
  - Random Forest
  - XGBoost
  - LightGBM

**Summary**

| Model Type | Needs Scaling? |
|------------|----------------|
| Random Forest | âŒ No |
| Decision Tree | âŒ No |
| Logistic Regression | âœ… Yes |
| Linear Regression (with GD) | âœ… Yes |
| Polynomial Regression | âœ… Yes |
| KNN | âœ… Yes |

So with Random Forest, **you can skip feature scaling** â€” and itâ€™ll still perform just fine

## ××” ×–×” Bootstrapping ×‘××•×“×œ?

 ×ª×”×œ×™×š ×©×‘×• ×× ×• ×™×•×¦×¨×™× ××“×’× ×—×“×© ××”× ×ª×•× ×™× ×”×§×™×™××™× **×¢×œ ×™×“×™ ×‘×—×™×¨×” ××§×¨××™×ª ×©×œ ×“×’×™××•×ª ×¢× ×”×—×–×¨×”**. ×›×œ×•××¨:
- ×™×™×ª×›×Ÿ ×©×“×•×’××” ×ª×™×‘×—×¨ ×™×•×ª×¨ ××¤×¢× ××—×ª
- ×™×™×ª×›×Ÿ ×©×“×•×’××” ××¡×•×™××ª ×œ× ×ª×™×‘×—×¨ ×‘×›×œ×œ

×”××˜×¨×”: ×›×œ ×¢×¥ ×¨×•××” ×’×¨×¡×” ××¢×˜ ×©×•× ×” ×©×œ ×”×“××˜×”. ×–×” ××™×™×¦×¨ **×’×™×•×•×Ÿ** ×‘×™×Ÿ ×”×¢×¦×™× ×•×¢×•×–×¨ ×œ××•×“×œ ×œ×”×™×•×ª ×™×•×ª×¨ ×™×¦×™×‘ ×•×œ×¤×—×•×ª ××•×˜×”.

**×“×•×’××” ×¤×©×•×˜×”**:

×× ×™×© ×œ× ×• 5 ×“×•×’×××•×ª: A, B, C, D, E

××—×“ ×”×¢×¦×™× ×™×›×•×œ ×œ×”×™×‘× ×•×ª ×¢×œ ×‘×¡×™×¡ ××“×’× ×›××•: A, B, B, D, E

## ×‘×§×˜× ×”: ××” ×–×” Out-Of-Bag Error?

×›××©×¨ ×¢×•×©×™× Bootstrapping, ×œ× ×›×œ ×”×“×•×’×××•×ª × ×‘×—×¨×•×ª ×œ××“×’×
- ××ª ×”×“×•×’×××•×ª **×©×œ× × ×‘×—×¨×•** ××¤×©×¨ ×œ×©××•×¨ ×œ×‘×“×™×§×”
- ×›×š × ×™×ª×Ÿ ×œ×‘×“×•×§ **×›××” ×˜×•×‘ ×›×œ ×¢×¥ ×× ×‘×** ×¢×œ ×”×“×•×’×××•×ª ×©×œ× ×©×™××©×• ×œ××™××•×Ÿ ×©×œ×•
- ××©× ××—×©×‘×™× **OOB Error** ×©×”×•× ××“×“ ×œ×‘×™×¦×•×¢×™ ×”××•×“×œ ×‘×œ×™ ×œ×”×¤×¨×™×“ ×¡×˜ ×‘×“×™×§×” (Train/Test)

**×“×•×’××” ×¡×•×¤×¨ ×¤×©×•×˜×”:**

| ×’×™×œ | ××©×›×•×¨×ª | ×§×™×‘×œ ××©×¨××™ |
|-----|---------|--------------|
| 25  | 3000    | ×œ×           |
| 30  | 5000    | ×›×Ÿ           |
| 45  | 7000    | ×›×Ÿ           |
| 22  | 2500    | ×œ×           |
| 35  | 6000    | ×›×Ÿ           |

×‘×™×¢×¨ ×¨× ×“×•××œ×™:
- ×›×œ ×¢×¥ ×¨×•××” ××“×’× ×©×•× ×” ××”×“××˜×” (×¢× ×”×—×–×¨×”)
- ×‘×›×œ ×¤×™×¦×•×œ ×™×™×‘×—×¨×• ×ª×›×•× ×•×ª ×©×•× ×•×ª
- ×›×œ ×¢×¥ ×™×œ××“ ×¢×œ ×“××˜×” ××—×¨
- ×”×ª×—×–×™×ª ×”×¡×•×¤×™×ª ×ª×”×™×” ×××•×¦×¢×ª ××›×œ ×”×¢×¦×™× âœ¨

## ××ª××˜×™×§×” ×‘×¡×™×¡×™×ª

××:
$$
T_1(x), T_2(x), \ldots, T_K(x)
$$
×”× ×”×¢×¦×™×,

××– ×”×ª×—×–×™×ª ×”×¡×•×¤×™×ª ×”×™×:

- **×¡×™×•×•×’**:
$$
\text{Prediction}(x) = \text{Majority Vote}(T_1(x), T_2(x), \ldots, T_K(x))
$$

- **×¨×’×¨×¡×™×”**:
$$
\text{Prediction}(x) = \frac{1}{K} \sum_{i=1}^{K} T_i(x)
$$

## ×”×©×•×•××”: ×¢×¥ ×”×—×œ×˜×” ×™×—×™×“ ××•×œ Random Forest

| × ×•×©× | ×¢×¥ ×”×—×œ×˜×” ×‘×•×“×“ | ×™×¢×¨ ×¨× ×“×•××œ×™ |
|:-----|:--------------|:------------|
| ×“×™×•×§ | ×’×‘×•×” ×¢×œ ××™××•×Ÿ, × ××•×š ×¢×œ ×—×“×© | ×™×¦×™×‘ ×•×’×‘×•×” |
| ×©×™××•×© ×‘×ª×›×•× ×•×ª | ×—×œ×§ ××”×ª×›×•× ×•×ª ××•×–× ×—×•×ª | ×¨×•×‘ ×”×ª×›×•× ×•×ª ××©×•×œ×‘×•×ª |
| ×©×•× ×•×ª | ×’×‘×•×”×” | × ××•×›×” |

## ×¤×¨××˜×¨×™× ×—×©×•×‘×™× ×‘-Random Forest

- `n_estimators` â†’ ×›××” ×¢×¦×™× ×œ×‘× ×•×ª. ×›×›×œ ×©×™×© ×™×•×ª×¨ ×¢×¦×™×, ×”××•×“×œ ×œ×•××“ ×˜×•×‘ ×™×•×ª×¨ (×¢×“ ×’×‘×•×œ ××¡×•×™×)
- `max_features` â†’ ×›××” ×ª×›×•× ×•×ª ×™×™×‘×—×¨×• ×‘××§×¨××™ ×‘×›×œ ×¤×™×¦×•×œ. ××©×¤×™×¢ ×¢×œ ×’×™×•×•×Ÿ ×”×¢×¦×™×
- `bootstrap` â†’ ×”×× ×œ×‘×¦×¢ ×“×’×™××” ×¢× ×”×—×–×¨×” (×‘×•×˜×¡×˜×¨××¤). ×‘×¨×™×¨×ª ××—×“×œ: ×××ª
- `oob_score` â†’ ×× ×××ª, ×”××•×“×œ ×™×©×ª××© ×‘×“×•×’×××•×ª ×©×œ× × ×‘×—×¨×• ×‘××“×’× ×›×“×™ ×œ×”×¢×¨×™×š ××ª ×”×“×™×•×§ (×©×’×™××ª-××—×•×¥-×œ×ª×™×§) ×‘×œ×™ ×¦×•×¨×š ×‘×¡×˜ ×‘×“×™×§×” × ×¤×¨×“

### ×œ××” Bootstrapping ×—×©×•×‘?

- ×™×•×¦×¨ ×’×™×•×•×Ÿ ×‘×™×Ÿ ×”×¢×¦×™×
- ××¤×—×™×ª ×§×•×¨×œ×¦×™×” ×‘×™×Ÿ ×¢×¦×™×
- ××©×¤×¨ ×™×¦×™×‘×•×ª ×•×”×›×œ×œ×” (generalization)
- ×××¤×©×¨ ×œ×—×©×‘ Out-Of-Bag Error ××‘×œ×™ ×œ×”×¤×¨×™×“ ×“××˜×”

---

## ×“×•×’××ª ×§×•×“ ×¤×©×•×˜×” ×‘×¤×™×™×ª×•×Ÿ

```python
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Creating a sample dataset directly in the code
# Data represents students with study hours, sleep hours, and breakfast (yes/no)
# Target is whether they passed an exam (1) or failed (0)

# Features data
study_hours = [2, 8, 5, 1, 7, 3, 6, 4, 9, 2, 7, 4, 8, 3, 6, 9, 5, 3, 7, 2]
sleep_hours = [5, 8, 9, 4, 7, 8, 6, 7, 9, 6, 8, 5, 7, 6, 9, 8, 7, 5, 8, 4]
had_breakfast = [1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0]  # 1=yes, 0=no

# Create features array
X = np.column_stack((study_hours, sleep_hours, had_breakfast))

# Target data (pass/fail)
y = np.array([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0])

# Feature names for better visualization
feature_names = ["Study Hours", "Sleep Hours", "Had Breakfast"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Create a Random Forest model
rf_model = RandomForestClassifier(
    n_estimators=100,
    bootstrap=True,
    oob_score=True,
    random_state=42
)

# Train the model
rf_model.fit(X_train, y_train)

# Predict on test set
y_pred = rf_model.predict(X_test)

# Calculate accuracy scores
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.2f}")
print(f"Out-of-Bag Accuracy: {rf_model.oob_score_:.2f}")

# Print detailed classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
```

Output
```
Model Accuracy: 1.00
Out-of-Bag Accuracy: 0.93

Classification Report:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00         3
           1       1.00      1.00      1.00         3

    accuracy                           1.00         6
   macro avg       1.00      1.00      1.00         6
weighted avg       1.00      1.00      1.00         6
```

**×”×¡×‘×¨:**
- ×”×“××˜×” ××¤×•×¦×œ ×¢×œ ×¢×¦×™× ×©×•× ×™× (×¢× ×ª×›×•× ×•×ª ×©×•× ×•×ª)
- ×‘×¡×•×£ ××•×¡×¤×™× ××ª ×›×œ ×”×ª×—×–×™×•×ª ×•××§×‘×œ×™× ×”×—×œ×˜×” ×¡×•×¤×™×ª

### ×“×™×•×§ ×”××•×“×œ (Model Accuracy) ××•×œ Out-of-Bag Accuracy

×‘×•××• × ×‘×™×Ÿ ×‘×¦×•×¨×” ×‘×¨×•×¨×” ××” ×”×”×‘×“×œ ×‘×™×Ÿ ×©× ×™ ××“×“×™ ×”×“×™×•×§ ×”×—×©×•×‘×™× ×”××œ×•: `Model Accuracy` ×•Ö¾`OOB Accuracy`

#### âœ… ××” ×–×” `accuracy_score` ×•××™×š ××—×©×‘×™× ××•×ª×•?

```python
accuracy = accuracy_score(y_true, y_pred)
```

×–×• ×¤×©×•×˜ ×”× ×•×¡×—×”:

\[
\text{Accuracy} = \frac{CorrectPredictions}{TotalPredictions}
\]

#### ×“×•×’××”:
×× ×”×™×• ×œ×š 10 ×“×•×’×××•×ª, ×•×”××•×“×œ × ×™×—×© × ×›×•×Ÿ ×‘Ö¾9 ××”×Ÿ:

\[
\text{Accuracy} = \frac{9}{10} = 0.90
\]

#### ××” ×–×” Out-of-Bag Accuracy (OOB Accuracy)?

×›×©×× ×—× ×• ××©×ª××©×™× ×‘Ö¾**Bootstrapping** (××“×’× ×¢× ×”×—×–×¨×”), ×›×œ ×¢×¥ ×‘×™×¢×¨ ×¨× ×“×•××œ×™ ××§×‘×œ ×¨×§ ×—×œ×§ ××”×©×•×¨×•×ª ××”×“××˜×”

- ×—×œ×§ ××”×©×•×¨×•×ª **×œ× × ×›× ×¡×•×ª ×‘×›×œ×œ** ×œ××™××•×Ÿ ×©×œ ××•×ª×• ×¢×¥ â€” ××œ×• × ×§×¨××•×ª **Out-of-Bag samples**
- ×× ×—× ×• ×™×›×•×œ×™× ×œ×”×©×ª××© ×‘×©×•×¨×•×ª ×”××œ×• ×›×“×™ ×œ×‘×“×•×§ **×›××” ×˜×•×‘ ×”×¢×¥ ×× ×‘× ×“×•×’×××•×ª ×©×”×•× ×œ× ×¨××”**

**×”×ª×”×œ×™×š:**
- ×¢×‘×•×¨ ×›×œ ×“×•×’××” â€” ××•×¡×¤×™× ×ª×—×–×™×•×ª ×¨×§ ××”×¢×¦×™× **×©×œ× ×¨××• ××•×ª×” ×‘××™××•×Ÿ**
- ×œ×•×§×—×™× ×××•×¦×¢/×¨×•×‘ ×§×•×œ×•×ª â†’ ×–×• ×”×ª×—×–×™×ª ×”×¡×•×¤×™×ª
- ××©×•×•×™× ×œ×ª×•×¦××” ×”×××™×ª×™×ª
- ×›×š ××—×©×‘×™× ××ª ×”×“×™×•×§ ×”×›×•×œ×œ ×¢×œ ×“×•×’×××•×ª ×”Ö¾OOB

#### ğŸ”¢ ×“×•×’××” ×ª×•×¦××”:
```python
Model Accuracy: 1.00
Out-of-Bag Accuracy: 0.93
```

#### ×”×¡×‘×¨:
- `Model Accuracy = 1.00` â†’ ×”××•×“×œ × ×™×—×© ××ª **×›×œ ×”×“×•×’×××•×ª ×‘×¡×˜ ×”×‘×“×™×§×”** × ×›×•× ×” (×™×•×¤×™!)
- `OOB Accuracy = 0.93` â†’ ×”××•×“×œ ×”×¦×œ×™×— ×‘Ö¾93% ××”××§×¨×™× **×¢×œ ×”×“××˜×” ×©×œ× ×©×™××© ×œ××™××•×Ÿ** ×‘×›×œ ×¢×¥

**âœ¨ ×™×ª×¨×•× ×•×ª OOB Accuracy**

- ×××¤×©×¨ **×”×¢×¨×›×” ×¤× ×™××™×ª** ×©×œ ×”××•×“×œ ×‘×œ×™ ×œ×”×©×ª××© ×‘Ö¾`train_test_split`
- ×—×•×¡×š ×–××Ÿ ×•××××¥
- ××–×›×™×¨ ×××•×“ **Cross Validation** ××‘×œ ××•×˜×•××˜×™

**×œ×¡×™×›×•×:**

| ××“×“ | ××™×š × ×‘× ×” | ××” ×”×•× ××•×“×“ |
|-----|----------|--------------|
| `Model Accuracy` | ×ª×—×–×™×ª ×¢×œ ×¡×˜ ×‘×“×™×§×” | ×“×™×•×§ ×—×™×¦×•× ×™ ×§×œ××¡×™ |
| `OOB Accuracy` | ×ª×—×–×™×•×ª ×¢×œ ×“×•×’×××•×ª ×©×œ× × ×‘×—×¨×• ×‘××“×’× | ×”×¢×¨×›×” ×¤× ×™××™×ª ×œ××™××•×Ÿ |


## ×’×¨×£ ×”××—×©×”

<img src="rand2.png" style="width: 60%" />

<img src="rand1.png" style="width: 90%" />


### Pairplot â€“ ×”×¡×‘×¨ ×¢×œ ×”×’×¨×£

×”-×¤×™×™×¨-×¤×œ×•×˜ ×”×•× ×›×œ×™ ×•×™×–×•××œ×™ ×—×©×•×‘ ×œ×”×‘× ×ª ×”×“××˜×” ×¢×•×“ ×œ×¤× ×™ ×©××¨×™×¦×™× ×¢×œ×™×• ××•×“×œ. ×”× ×” ×”×¡×‘×¨ ×¤×©×•×˜ ×œ××” ×”×•× ××©××©, ××™×š ×œ×§×¨×•× ××•×ª×•, ×•××” ××¤×©×¨ ×œ×œ××•×“ ××× ×•

### ××” ×–×” Pairplot?

××¦×™×’:
- ××ª **×”×”×ª×¤×œ×’×•×ª ×©×œ ×›×œ ×¤×™×¦'×¨** ×œ××•×¨×š ×”××œ×›×¡×•×Ÿ ×”×¨××©×™
- ××ª **×”×§×©×¨×™× (×§×•×¨×œ×¦×™×”)** ×‘×™×Ÿ ×›×œ ×–×•×’ ×¤×™×¦'×¨×™× ×‘××©×‘×¦×•×ª ×”××—×¨×•×ª
- ×¦×‘×¢ ×©×•× ×” ×œ×›×œ **××—×œ×§×” (Class)** â€“ ×œ××©×œ: "×¢×‘×¨" ×‘×¦×‘×¢ ×›×—×•×œ, "× ×›×©×œ" ×‘×¦×‘×¢ ××“×•×

### ××™×š ×œ×§×¨×•× ××ª ×”×’×¨×£?

- ×× ×©×ª×™ ×§×‘×•×¦×•×ª (×œ××©×œ: ×¢×‘×¨/× ×›×©×œ) ××•×¤×¨×“×•×ª ×™×¤×” ×‘×¦×™×¨ ××¡×•×™× â†’ ×›× ×¨××” ×©×–×” ×¤×™×¦'×¨ ×—×–×§
- ×× ×”×”×ª×¤×œ×’×•×ª ×—×•×¤×¤×ª ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª â†’ ×”×¤×™×¦'×¨ ×›× ×¨××” ×œ× ×ª×•×¨× ×”×¨×‘×” ×œ××•×“×œ
- ×‘×™×Ÿ ×–×•×’×•×ª ×¤×™×¦'×¨×™×: ×× ×¨×•××™× ×”×¦×˜×‘×¨×•×ª ×¦×‘×¢×™× ××¡×•×™××ª ×‘×–×•×•×™×ª â†’ ×™×™×ª×›×Ÿ ×§×©×¨ ××¢× ×™×™×Ÿ ×‘×™× ×™×”×

1. **Study Hours**:
   - ×”×¢×§×•××” ×©×œ ×”"×¢×‘×¨×•" × ×•×˜×” ×œ×©×¢×•×ª ×œ×™××•×“ ×’×‘×•×”×•×ª ×™×•×ª×¨
   - ×”×¢×§×•××” ×©×œ ×”"× ×›×©×œ×•" × ×•×˜×” ×œ×©×¢×•×ª ×œ×™××•×“ × ××•×›×•×ª
   - ××¡×§× ×”: ×–×” ×¤×™×¦'×¨ ×˜×•×‘ ×œ××•×“×œ!

2. **Had Breakfast**:
   - ×”×¢×§×•××•×ª ×›××¢×˜ × ×¤×¨×“×•×ª ×œ×’××¨×™
   - ×ª×œ××™×“×™× ×©××›×œ×• ××¨×•×—×ª ×‘×•×§×¨ ×¢×‘×¨×• ×™×•×ª×¨!
   - ×’× ×–×” ×¤×™×¦'×¨ ×—×–×§ ×××•×“ ğŸ’ª

3. **Sleep Hours**:
   - ×§×¦×ª ×—×•×¤×£, ××‘×œ ×‘×›×œ ×–××ª ×™×© × ×˜×™×™×” ×‘×¨×•×¨×”
   - ×›×Ÿ ×ª×•×¨×, ××‘×œ ×¤×—×•×ª ×—×–×§ ××”×©× ×™×™× ×”××—×¨×™×

### ×œ××” ××©×ª××©×™× ×‘×•?

- ×›×“×™ ×œ×”×‘×™×Ÿ ×‘××”×™×¨×•×ª ××ª ×”×“××˜×”
- ×›×“×™ ×œ×‘×—×•×¨ ××™×œ×• ×¤×™×¦'×¨×™× ×©×•×•×” ×œ×”×›× ×™×¡ ×œ××•×“×œ
- ×›×“×™ ×œ××¦×•× ×“×¤×•×¡×™× ××¢× ×™×™× ×™× ×©×œ× ×¨×•××™× ×‘×˜×‘×œ×”

### ×¡×™×›×•× 

×¤×™×™×¨-×¤×•×œ×˜ × ×•×ª×Ÿ ×ª××•× ×” ×¨××©×•× ×™×ª ×—×©×•×‘×” ×¢×œ ×”×§×©×¨×™× ×‘×™×Ÿ ×¤×™×¦'×¨×™× ×œ×‘×™×Ÿ ×”×ª×•×¦××”. ×‘×¢×–×¨×ª ×’×¨×£ ×¤×©×•×˜ ××—×“, ××¤×©×¨ ×œ×–×”×•×ª ××™×œ×• ××©×ª× ×™× ×”× ×‘×¢×œ×™ ××©××¢×•×ª ×—×–×§×” ×™×•×ª×¨ ×•×œ×”×ª××§×“ ×‘×”×

××—×¨×™ ×–×” â€“ ××¤×©×¨ ×œ×”×™×›× ×¡ ×œ×›×œ×™ ×›××• Random Forest ×•×œ×ª×ª ×œ×• ×œ×¢×©×•×ª ××ª ×”×¢×‘×•×“×” ×”×¨×¦×™× ×™×ª 

---

## âœ¨ ×¡×™×›×•× ×¨××©×•× ×™

- ×™×¢×¨ ×¨× ×“×•××œ×™ = ×”×¨×‘×” ×¢×¦×™ ×”×—×œ×˜×” ×©×•× ×™×
- ××ª××™× ×œ×¡×™×•×•×’ ×•×œ×¨×’×¨×¡×™×”
- ××¤×—×™×ª Overfitting ×•××©×¤×¨ ×“×™×•×§
- ×“×•×¨×© ×™×•×ª×¨ ×–××Ÿ ×—×™×©×•×‘, ××‘×œ ×××•×“ ×××™×Ÿ ×•×‘×™×¦×™×‘



