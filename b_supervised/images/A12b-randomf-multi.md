# ğŸ¯ Random Forest ×œ×‘×¢×™×•×ª ×¨×™×‘×•×™ ××—×œ×§×•×ª (Multiclass Classification)

×‘×•× × ×‘×™×Ÿ ××™×š ×¢×•×‘×“ Random Forest ×›××©×¨ ×™×© ×™×•×ª×¨ ××©×ª×™ ××—×œ×§×•×ª (labels), ×›××• ×œ××©×œ:
- ×–×™×”×•×™ ×¡×•×’ ×¤×¨×— (setosa / versicolor / virginica)
- ×¡×™×•×•×’ ×œ×§×•×—×•×ª ×œ×¨××•×ª ×¡×™×›×•×Ÿ (× ××•×š / ×‘×™× ×•× ×™ / ×’×‘×•×”)
- ×—×™×–×•×™ ×¦×™×•× ×™× ×œ×¤×™ ×¨××•×ª (A / B / C / D / F)

## âœ… ××™×š ×–×” ×¢×•×‘×“?

×¨× ×“×•× ×¤×•×¨×¡×˜ **×ª×•××š ×‘×¦×•×¨×” ×˜×‘×¢×™×ª ×‘Ö¾××•×œ×˜×™ ×§×œ××¡**, ×‘×œ×™ ×¦×•×¨×š ×‘×¢×˜×™×¤×•×ª ××™×•×—×“×•×ª
×”×•× ×¤×©×•×˜ ×‘×•× ×” ×”×¨×‘×” ×¢×¦×™×, ×©×›×œ ××—×“ ××”× ××¡×•×’×œ ×œ×ª×ª ×ª×—×–×™×ª ×¢×œ ××—×œ×§×” ××—×ª ××ª×•×š ×›××”

### ×ª×”×œ×™×š ×”×¢×‘×•×“×”:
1. ×›×œ ×¢×¥ ××ª×××Ÿ ×¢×œ ××“×’× ×©×•× ×” (Bootstrapping)
2. ×‘×›×œ ×¤×™×¦×•×œ ×‘×¢×¥: × ×‘×—×¨×ª ×§×‘×•×¦×ª ×ª×›×•× ×•×ª ××§×¨××™×•×ª
3. ×›×œ ×¢×œ×” ××¡××Ÿ **××—×œ×§×” ×¡×•×¤×™×ª ××—×ª** ××ª×•×š ×›××” ××¤×©×¨×™×•×ª
4. ×‘×¡×•×£ ×›×œ ×”×¢×¦×™× ××¦×‘×™×¢×™×:
   - ×›×œ ×¢×¥ × ×•×ª×Ÿ ×ª×—×–×™×ª ×œ××—×œ×§×” ××¡×•×™××ª
   - ×”××•×“×œ ×‘×•×—×¨ ××ª ×”××—×œ×§×” ×©×§×™×‘×œ×” ×”×›×™ ×”×¨×‘×” ×§×•×œ×•×ª (×¨×•×‘)


## ğŸ”¢ ×“×•×’××”:
× × ×™×— ×©×™×© ×œ× ×• ×©×œ×•×© ××—×œ×§×•×ª: `A`, `B`, ×•Ö¾`C`

×× ×—××™×©×” ×¢×¦×™× × ×ª× ×• ××ª ×”×ª×—×–×™×•×ª ×”×‘××•×ª:
- ×¢×¥ 1: `A`
- ×¢×¥ 2: `C`
- ×¢×¥ 3: `A`
- ×¢×¥ 4: `B`
- ×¢×¥ 5: `A`

×”×ª×—×–×™×ª ×”×¡×•×¤×™×ª ×ª×”×™×”:
```
×¨×•×‘ ×§×•×œ×•×ª â†’ ××—×œ×§×” A (3 ×§×•×œ×•×ª)
```

## ğŸ“¦ ××™×š ×–×” ××™×•×©× ×‘×¤×™×™×ª×•×Ÿ (×¢× Scikit-learn)?

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# ×˜×•×¢× ×™× ×“××˜×” ×¢× 3 ××—×œ×§×•×ª
X, y = load_iris(return_X_y=True)

# ×¤×™×¦×•×œ ×œÖ¾Train/Test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ××•×“×œ Random Forest
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# ×ª×—×–×™×ª ×•×“×•"×— ×“×™×•×§
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
```

### ×”×¤×œ×˜ ×™×™×¨××” ×›×š:
```
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      1.00      1.00        13
           2       1.00      1.00      1.00        13

    accuracy                           1.00        45
```

## âœ¨ ×™×ª×¨×•× ×•×ª ×‘Ö¾Multiclass:

- ×œ× ×¦×¨×™×š ×©×™× ×•×™ ××™×•×—×“ ×‘×§×•×“ â€” ×¢×•×‘×“ ×›××• ×‘Ö¾Binary
- ××ª××™× ×œ×›×œ ××¡×¤×¨ ××—×œ×§×•×ª
- × ×•×ª×Ÿ ×ª×—×–×™×ª ×××•×“ ×™×¦×™×‘×” ×•×¢××™×“×” ×œ×¨×¢×©

## ğŸ§  ×”×¢×¨×” ×˜×›× ×™×ª
Scikit-learn handles multiclass classification natively in Random Forest. Each decision tree predicts a single class label (not probabilities), and the final prediction is based on majority voting among all the trees

This is conceptually similar to a **One-vs-Rest** strategy, but implemented implicitly within the forest. Each tree votes once, and the class receiving the most votes is chosen as the final prediction

There is no use of Softmax â€” probabilities are derived by counting votes and normalizing (dividing the number of votes for each class by the total number of trees, so that the results become proper probabilities)

## ×¡×™×›×•× ×§×¦×¨

| × ×•×©× | Multiclass ×‘Ö¾Random Forest |
|------|----------------------------|
| ×¦×¨×™×š ×”×ª×××•×ª ××™×•×—×“×•×ª? | ×œ× |
| ×ª×—×–×™×ª ×¡×•×¤×™×ª | ×¨×•×‘ ×§×•×œ×•×ª ×‘×™×Ÿ ×”×¢×¦×™× |
| ×ª×•××š ×‘×›×œ ××¡×¤×¨ ××—×œ×§×•×ª? | ×›×Ÿ |
| ××“×•×™×§ ×•×™×¦×™×‘? | ×××•×“ âœ… |


