# Χ¨Χ’Χ¨Χ΅Χ™Χ” ΧΧ•Χ’Χ™Χ΅ΧΧ™Χ

## Χ”Χ‘ΧΆΧ™Χ” Χ©Χ¨Χ•Χ¦Χ™Χ ΧΧ¤ΧΧ•Χ¨

Χ¨Χ’Χ¨Χ΅Χ™Χ” ΧΧ•Χ’Χ™Χ΅ΧΧ™Χ Χ”Χ™Χ Χ©Χ™ΧΧ” Χ΅ΧΧΧ™Χ΅ΧΧ™Χ Χ”ΧΧ©ΧΧ©Χ ΧΧ Χ™Χ‘Χ•Χ™ ΧΧ•Χ¦ΧΧ•Χ Χ‘Χ™Χ ΧΧ¨Χ™Χ•Χ (0 ΧΧ• 1, Χ›Χ ΧΧ• ΧΧ) ΧΆΧ Χ‘Χ΅Χ™Χ΅ ΧΧ©ΧΧ Χ™Χ ΧΧ΅Χ‘Χ™Χ¨Χ™Χ. Χ‘Χ Χ™Χ’Χ•Χ“ ΧΧ¨Χ’Χ¨Χ΅Χ™Χ” ΧΧ™Χ ΧΧ¨Χ™Χ Χ©ΧΧ Χ‘ΧΧ ΧΆΧ¨Χ›Χ™Χ Χ¨Χ¦Χ™Χ¤Χ™Χ, Χ¨Χ’Χ¨Χ΅Χ™Χ” ΧΧ•Χ’Χ™Χ΅ΧΧ™Χ ΧΧ—Χ©Χ‘Χ ΧΧ Χ”Χ”Χ΅ΧΧ‘Χ¨Χ•Χ Χ©Χ“Χ•Χ’ΧΧ” ΧΧ΅Χ•Χ™ΧΧ ΧΧ©ΧΧ™Χ™Χ ΧΧ§ΧΧ’Χ•Χ¨Χ™Χ” ΧΧ΅Χ•Χ™ΧΧ.

**Χ“Χ•Χ’ΧΧ”**: Χ Χ Χ™Χ— Χ©ΧΧ Χ• Χ¨Χ•Χ¦Χ™Χ ΧΧ—Χ–Χ•Χ Χ”ΧΧ ΧΧΧΧ™Χ“ Χ™ΧΆΧ‘Χ•Χ¨ ΧΧ‘Χ—Χ (Χ™Χ¦ΧΧ™Χ—=1, Χ™Χ™Χ›Χ©Χ=0) ΧΆΧ Χ΅ΧΧ ΧΧ΅Χ¤Χ¨ Χ©ΧΆΧ•Χ Χ”ΧΧ™ΧΧ•Χ“ Χ©ΧΧ•. ΧΧ”ΧΧ Χ ΧΧ•Χ Χ™Χ Χ”Χ™Χ΅ΧΧ•Χ¨Χ™Χ™Χ Χ©Χ ΧΧΧΧ™Χ“Χ™Χ:

| Χ©ΧΆΧ•Χ ΧΧ™ΧΧ•Χ“ | ΧΧ•Χ¦ΧΧ” (1=ΧΆΧ‘Χ¨, 0=Χ Χ›Χ©Χ) |
|------------|----------------------|
| 1          | 0                    |
| 2          | 0                    |
| 3          | 0                    |
| 4          | 0                    |
| 5          | 1                    |
| 6          | 1                    |
| 7          | 1                    |
| 8          | 1                    |
| 9          | 1                    |

ΧΧ Χ• Χ¨Χ•Χ¦Χ™Χ ΧΧΧ¦Χ•Χ ΧΧ•Χ“Χ Χ©Χ™Χ—Χ–Χ” ΧΧ Χ”Χ”Χ΅ΧΧ‘Χ¨Χ•Χ Χ©ΧΧΧΧ™Χ“ Χ™ΧΆΧ‘Χ•Χ¨ ΧΧ Χ”ΧΧ‘Χ—Χ Χ‘Χ”ΧΧ‘Χ΅Χ΅ ΧΆΧ Χ©ΧΆΧ•Χ Χ”ΧΧ™ΧΧ•Χ“ Χ©ΧΧ•.

## Mathematical Formula and Complete Calculation

Logistic regression models the probability that a dependent variable Y equals 1 (in our case, passing the exam) as a function of independent variables X (in our case, study hours). The logistic function (also called the sigmoid function) is used to model this probability:

$$P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}$$

Where:
- $P(Y=1|X)$ is the probability that Y=1 given X
- $\beta_0$ is the intercept
- $\beta_1$ is the coefficient for the independent variable X
- $e$ is the base of the natural logarithm (approximately 2.71828)

The logistic function always outputs a value between 0 and 1, which can be interpreted as a probability.

### π“ Χ”Χ΅Χ‘Χ¨ Χ¤Χ©Χ•Χ ΧΆΧ Χ”Χ Χ•Χ΅Χ—Χ”

Logistic Regression ΧΧ—Χ©Χ‘Χ Χ”Χ΅ΧΧ‘Χ¨Χ•Χ Χ©ΧΧ©Χ”Χ• Χ™Χ§Χ¨Χ”, ΧΧ¤Χ™ ΧΆΧ¨Χ Χ©Χ ΧΧ©ΧΧ Χ” X (ΧΧΧ©Χ: Χ›ΧΧ” Χ©ΧΆΧ•Χ ΧΧΧ™Χ“Χ”)

### Χ”Χ Χ•Χ΅Χ—Χ”:

$$
P(Y = 1 \mid X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}
$$

Χ–Χ• Χ Χ§Χ¨ΧΧ **Χ¤Χ•Χ Χ§Χ¦Χ™Χ™Χ Χ΅Χ™Χ’ΧΧ•ΧΧ™Χ“ (sigmoid function)** β€“ Χ”Χ™Χ ΧΧ—Χ–Χ™Χ¨Χ” ΧΆΧ¨Χ›Χ™Χ Χ‘Χ™Χ 0 ΧΦΎ1, Χ•ΧΧ›Χ ΧΧΧΧ™ΧΧ” ΧΧ—Χ™Χ–Χ•Χ™ Χ”Χ΅ΧΧ‘Χ¨Χ•Χ™Χ•Χ

### π§  Χ”Χ΅Χ‘Χ¨Χ™Χ:

- $$P(Y = 1 \mid X)$$ β€“

   Χ”Χ”Χ΅ΧΧ‘Χ¨Χ•Χ Χ©-Y Χ™Χ”Χ™Χ” 1 Χ‘Χ”Χ™Χ ΧΧ X
  
- $$\beta_0$$ β€“ Χ”Χ§Χ‘Χ•ΧΆ (intercept)  
- $$\beta_1$$ β€“ ΧΧ§Χ“Χ Χ©Χ X  
- $$e$$ β€“ Χ”Χ‘Χ΅Χ™Χ΅ Χ©Χ Χ”ΧΧ•Χ’Χ¨Χ™ΧΧ Χ”ΧΧ‘ΧΆΧ™ (Χ‘ΧΆΧ¨Χ 2.718)

### π§® Χ“Χ•Χ’ΧΧ” Χ—Χ™Χ©Χ•Χ‘Χ™Χ:

ΧΧ:
- $$X = 5$$ (Χ©ΧΆΧ•Χ ΧΧΧ™Χ“Χ”)
- $$\beta_0 = -4$$
- $$\beta_1 = 1$$

ΧΧ–:

$$
P = \frac{1}{1 + e^{-(-4 + 1 \cdot 5)}} = \frac{1}{1 + e^{-1}} \approx 0.73
$$

Χ›ΧΧ•ΧΧ¨: Χ”Χ”Χ΅ΧΧ‘Χ¨Χ•Χ ΧΧΆΧ‘Χ•Χ¨ ΧΧ Χ”ΧΧ‘Χ—Χ ΧΧ—Χ¨Χ™ 5 Χ©ΧΆΧ•Χ ΧΧΧ™Χ“Χ” Χ”Χ™Χ Χ‘ΧΆΧ¨Χ **73%** β…

### β¨ ΧΧΧ” Logistic Regression Χ©Χ™ΧΧ•Χ©Χ™Χ?

- ΧΧ—Χ–Χ™Χ¨Χ” ΧΆΧ¨Χ›Χ™Χ Χ‘Χ™Χ 0 ΧΦΎ1 β†’ ΧΧ•Χ©ΧΧ ΧΧ”Χ΅ΧΧ‘Χ¨Χ•Χ™Χ•Χ
- ΧΧΧ¤Χ©Χ¨Χ ΧΧ”Χ‘Χ™Χ ΧΧ Χ”Χ§Χ©Χ¨ Χ‘Χ™Χ ΧΧ©ΧΧ Χ” Χ¨Χ¦Χ™Χ£ (Χ›ΧΧ• Χ©ΧΆΧ•Χ ΧΧ™ΧΧ•Χ“) ΧΧΧ•Χ¦ΧΧ” Χ‘Χ™Χ ΧΧ¨Χ™Χ (Χ›ΧΧ• Χ”Χ¦ΧΧ—Χ”/Χ›Χ™Χ©ΧΧ•Χ)

## Χ“Χ•Χ’ΧΧ Χ¨ΧΧ©Χ•Χ Χ”: Χ Χ™Χ—Χ•Χ© Ξ²β‚€ Χ•ΦΎΞ²β‚

### Χ©ΧΧ‘ 1: Χ Χ’Χ“Χ™Χ¨ ΧΧ Χ”Χ‘ΧΆΧ™Χ”
Χ™Χ© ΧΧ Χ• Χ ΧΧ•Χ Χ™Χ (X, Y), Χ›ΧΧ©Χ¨:
- X Χ”Χ•Χ ΧΧ©ΧΧ Χ” Χ¨Χ¦Χ™Χ£ (ΧΧΧ©Χ: Χ©ΧΆΧ•Χ ΧΧΧ™Χ“Χ”)
- Y Χ”Χ•Χ Χ‘Χ™Χ ΧΧ¨Χ™: 0 ΧΧ• 1 (ΧΧΧ©Χ: Χ›Χ™Χ©ΧΧ•Χ ΧΧ• Χ”Χ¦ΧΧ—Χ”)

Χ”ΧΧΧ¨Χ” Χ©ΧΧ Χ•:
> ΧΧ—Χ©Χ‘ ΧΧ Χ”Χ”Χ΅ΧΧ‘Χ¨Χ•Χ Χ©ΦΎY Χ™Χ”Χ™Χ” 1 Χ‘Χ”Χ™Χ ΧΧ X

### Χ©ΧΧ‘ 2: Χ Χ’Χ“Χ™Χ¨ ΧΧ Χ”Χ¤Χ•Χ Χ§Χ¦Χ™Χ” Χ”ΧΧ•Χ’Χ™Χ΅ΧΧ™Χ

Χ–Χ• Χ”Χ¤Χ•Χ Χ§Χ¦Χ™Χ” Χ©ΧΧ™ΧΧ” Χ Χ‘Χ Χ” ΧΧ Χ”ΧΧ•Χ“Χ:

$$
P(Y = 1 \mid X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}
$$

Χ”Χ™Χ ΧΧΧ™Χ“ ΧΧ—Χ–Χ™Χ¨Χ” ΧΆΧ¨Χ›Χ™Χ Χ‘Χ™Χ 0 ΧΦΎ1 β€“ ΧΧ•Χ©ΧΧ ΧΧ”Χ΅ΧΧ‘Χ¨Χ•Χ™Χ•Χ.

### Χ©ΧΧ‘ 3: Χ Χ‘Χ Χ” ΧΧ Χ¤Χ•Χ Χ§Χ¦Χ™Χ™Χ Χ”Χ”Χ΅ΧΧ‘Χ¨Χ•Χ Χ”Χ›Χ•ΧΧΧ (Likelihood)

Χ›Χ“Χ™ ΧΧ‘Χ“Χ•Χ§ ΧΆΧ“ Χ›ΧΧ” Χ”ΧΧ•Χ“Χ Χ©ΧΧ Χ• "ΧΧ“Χ•Χ™Χ§", Χ Χ—Χ©Χ‘ ΧΧ Χ”Χ”Χ΅ΧΧ‘Χ¨Χ•Χ Χ©Χ”Χ•Χ Χ—Χ•Χ–Χ” Χ Χ›Χ•Χ ΧΧ Χ›Χ Χ”Χ ΧΧ•Χ Χ™Χ:

$$
L(\beta_0, \beta_1) = \prod_{i=1}^{n} \left( \hat{p}_i \right)^{y_i} \cdot \left(1 - \hat{p}_i \right)^{1 - y_i}
$$

Χ›ΧΧ©Χ¨:

$$
\hat{p}_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_i)}}
$$

#### β… ΧΧ©ΧΧΆΧ•Χ:
- <img src="https://latex.codecogs.com/gif.latex?\hat{p}_i"/> Χ”Χ™Χ Χ”Χ”Χ΅ΧΧ‘Χ¨Χ•Χ Χ©Χ Χ”ΧΧ•Χ“Χ ΧΧ›Χ Χ©Χ”ΧΧ¦Χ¤Χ™Χ Χ”-<img src="https://latex.codecogs.com/gif.latex?i"/> ΧΧ”Χ™Χ” 1
- <img src="https://latex.codecogs.com/gif.latex?y_i"/> Χ”Χ•Χ Χ”ΧΆΧ¨Χ Χ‘Χ¤Χ•ΧΆΧ β€” ΧΧ• 0 ΧΧ• 1
- Χ›ΧΧ©Χ¨ <img src="https://latex.codecogs.com/gif.latex?y_i%20=%201"/>, ΧΧ§Χ‘ΧΧ™Χ: <img src="https://latex.codecogs.com/gif.latex?\hat{p}_i"/>
- Χ›ΧΧ©Χ¨ <img src="https://latex.codecogs.com/gif.latex?y_i%20=%200"/>, ΧΧ§Χ‘ΧΧ™Χ: <img src="https://latex.codecogs.com/gif.latex?1%20-%20\hat{p}_i"/>
  
Χ”Χ Χ•Χ΅Χ—Χ” Χ—Χ›ΧΧ” Χ‘Χ›Χ Χ©Χ”Χ™Χ ΧΧ©ΧΧΧ©Χ Χ‘-<img src="https://latex.codecogs.com/gif.latex?y_i"/> Χ•-<img src="https://latex.codecogs.com/gif.latex?1%20-%20y_i"/> Χ›Χ“Χ™ ΧΧ›Χ΅Χ•Χ ΧΧ Χ©Χ Χ™ Χ”ΧΧ§Χ¨Χ™Χ Χ‘ΧΧ•ΧΧ” Χ Χ•Χ΅Χ—Χ” β€” Χ‘ΧΧ™ ΧΧ ΧΧ™Χ ΧΧ™Χ•Χ—Χ“Χ™Χ

**π― Χ“Χ•Χ’ΧΧ” ΧΧ—Χ™Χ©Χ•Χ‘ Χ¤Χ•Χ Χ§Χ¦Χ™Χ™Χ Likelihood**

Χ Χ Χ™Χ— Χ©Χ™Χ© ΧΧ Χ• Χ©ΧΧ™ Χ“Χ•Χ’ΧΧΧ•Χ Χ‘ΧΧ‘Χ“:

- Χ“Χ•Χ’ΧΧ” Χ¨ΧΧ©Χ•Χ Χ”: x = 1, y = 1. ΧΧ“Χ•Χ’ΧΧ - ΧΧ—Χ¨Χ™ Χ©ΧΆΧ” Χ©Χ Χ”ΧΧ™Χ›Χ” Χ”Χ™Χ” Χ”Χ™Χ” ΧΆΧ•Χ“Χ£ Χ΅Χ•Χ›Χ¨ Χ‘Χ“Χ
- Χ“Χ•Χ’ΧΧ” Χ©Χ Χ™Χ™Χ”: x = 2, y = 0. ΧΧ“Χ•Χ’ΧΧ - ΧΧ—Χ¨Χ™ Χ©ΧΆΧΧ™Χ™Χ Χ©Χ Χ”ΧΧ™Χ›Χ” ΧΧ Χ”Χ™Χ” ΧΆΧ•Χ“Χ£ Χ΅Χ•Χ›Χ¨ Χ‘Χ“Χ

Χ Χ‘Χ—Χ¨ ΧΧ§Χ“ΧΧ™Χ ΧΧΧ•Χ“Χ:
- Ξ²β‚€ = 0  
- Ξ²β‚ = 1

**Χ©ΧΧ‘ Χ: Χ Χ—Χ©Χ‘ ΧΧ Χ”Χ”Χ΅ΧΧ‘Χ¨Χ•Χ (p-hat) Χ©Χ”ΧΧ•Χ“Χ Χ—Χ•Χ–Χ”**

Χ”Χ Χ•Χ΅Χ—Χ” Χ”Χ™Χ:

$$
\hat{p}_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_i)}}
$$

##### ΧΆΧ‘Χ•Χ¨ Χ”Χ“Χ•Χ’ΧΧ” Χ”Χ¨ΧΧ©Χ•Χ Χ” (x = 1):

$$
\hat{p}_1 = \frac{1}{1 + e^{-(0 + 1 \cdot 1)}} = \frac{1}{1 + e^{-1}} \approx 0.731
$$

##### ΧΆΧ‘Χ•Χ¨ Χ”Χ“Χ•Χ’ΧΧ” Χ”Χ©Χ Χ™Χ™Χ” (x = 2):

$$
\hat{p}_2 = \frac{1}{1 + e^{-(0 + 1 \cdot 2)}} = \frac{1}{1 + e^{-2}} \approx 0.881
$$

ΧΧ‘Χ β€“ Χ‘Χ’ΧΧ Χ©ΦΎy = 0 Χ‘ΧΧ§Χ¨Χ” Χ”Χ–Χ”, Χ Χ©ΧΧΧ© Χ‘ΦΎ(1 β’ pΜ‚):

$$
1 - \hat{p}_2 \approx 1 - 0.881 = 0.119
$$


**Χ©ΧΧ‘ Χ‘: Χ Χ—Χ©Χ‘ ΧΧ Χ¤Χ•Χ Χ§Χ¦Χ™Χ™Χ Χ”ΦΎLikelihood Χ”Χ›Χ•ΧΧΧ**

Χ Χ©ΧΧΧ© Χ‘Χ Χ•Χ΅Χ—Χ”:

$$
L = (\hat{p}_1)^{y_1} \cdot (1 - \hat{p}_2)^{1 - y_2}
$$

Χ Χ¦Χ™Χ‘ ΧΧ Χ”ΧΆΧ¨Χ›Χ™Χ:

$$
L = 0.731 \cdot 0.119 \approx 0.087
$$


#### π§  ΧΧ” Χ–Χ” ΧΧ•ΧΧ¨?

- Χ”ΧΧ•Χ“Χ Χ—Χ–Χ” ΧΧ•Χ‘ Χ™Χ—Χ΅Χ™Χ Χ‘Χ“Χ•Χ’ΧΧ” Χ”Χ¨ΧΧ©Χ•Χ Χ” (73% ΧΆΧ‘Χ•Χ¨ y=1)
- ΧΧ‘Χ ΧΧΆΧ” Χ‘Χ“Χ•Χ’ΧΧ” Χ”Χ©Χ Χ™Χ™Χ” β€” Χ—Χ–Χ” Χ”Χ΅ΧΧ‘Χ¨Χ•Χ Χ’Χ‘Χ•Χ”Χ” ΧΦΎy=1, ΧΧΧ¨Χ•Χ Χ©Χ”ΧΧ©Χ•Χ‘Χ” Χ”Χ™Χ™ΧΧ” y=0
- ΧΧ›Χ, Χ¤Χ•Χ Χ§Χ¦Χ™Χ™Χ Χ”ΦΎLikelihood Χ”Χ΅Χ•Χ¤Χ™Χ Χ”Χ™Χ Χ‘ΧΆΧ¨Χ 8.7%

Χ”ΧΧ΅Χ§Χ Χ”: Χ Χ¨Χ¦Χ” ΧΧΧ¦Χ•Χ Ξ²β‚€ Χ•ΦΎΞ²β‚ ΧΧ—Χ¨Χ™Χ Χ©Χ™Χ™ΧΧ Χ• ΧΆΧ¨Χ Χ’Χ‘Χ•Χ” Χ™Χ•ΧΧ¨ ΧΦΎL.

 
## Χ›Χ™Χ¦Χ“ ΧΧ—Χ©Χ‘ ΧΧ Ξ²β‚€ Χ•ΦΎΞ²β‚

Χ¨ΧΧ” Χ‘Χ”ΧΧ©Χ ... Χ Χ΅Χ¤Χ— Χ'

## Χ›Χ™Χ¦Χ“ ΧΧΧ¦Χ•Χ ΧΧ Χ”ΧΧ§Χ“ΧΧ™Χ Χ‘Χ¤Χ™Χ™ΧΧ•Χ

```python
from sklearn.linear_model import LogisticRegression
import numpy as np

# Data: Annual income and loan repayment (1=yes, 0=no)
X = np.array([30, 35, 40, 45, 50, 55, 60, 65, 70, 75]).reshape(-1, 1)
y = np.array([0, 0, 0, 0, 0, 1, 0, 1, 1, 1])

# Fit logistic regression model
model = LogisticRegression(solver='liblinear')
model.fit(X, y)

# Get coefficients
b0 = model.intercept_[0]
b1 = model.coef_[0][0]

# Print the logistic regression equation
print(f"Logistic regression equation:")
print(f"P(return) = 1 / (1 + e^-({b0:.2f} + {b1:.2f} * income))")
```
output:
```
Logistic regression equation:
P(return) = 1 / (1 + e^-(-0.89 + 0.02 * income))
```


## ΧΧ•Χ©Χ’ --  Decision Boundary

In logistic regression, we often need to make a binary decision based on the predicted probability. The most common threshold is 0.5, meaning:
- If $P(Y=1|X) \geq 0.5$, we predict Y=1 (pass)
- If $P(Y=1|X) < 0.5$, we predict Y=0 (fail)

Let's find the decision boundary where $P(\text{pass}|\text{hours}) = 0.5$:

$$0.5 = \frac{1}{1 + e^{-(-6.7 + 1.5 \times \text{hours})}}$$

$$1 + e^{-(-6.7 + 1.5 \times \text{hours})} = 2$$

$$e^{-(-6.7 + 1.5 \times \text{hours})} = 1$$

$$-(-6.7 + 1.5 \times \text{hours}) = 0$$

$$-6.7 + 1.5 \times \text{hours} = 0$$

$$1.5 \times \text{hours} = 6.7$$

$$\text{hours} = \frac{6.7}{1.5} \approx 4.47$$

So according to our model, students need to study approximately 4.47 hours to have a 50% chance of passing the exam.

## Χ’Χ¨Χ£

<img src="log2.png" style="width:80%;"/>

Χ”Χ’Χ¨Χ£ ΧΧ¦Χ™Χ’ ΧΧ ΧΆΧ§Χ•ΧΧ Χ”Χ¨Χ’Χ¨Χ΅Χ™Χ” Χ”ΧΧ•Χ’Χ™Χ΅ΧΧ™Χ (Χ”Χ§Χ• Χ”Χ›Χ—Χ•Χ) Χ©ΧΧΧΧ¨ ΧΧ Χ”Χ”Χ΅ΧΧ‘Χ¨Χ•Χ ΧΧΆΧ‘Χ•Χ¨ ΧΧ Χ”ΧΧ‘Χ—Χ Χ›Χ¤Χ•Χ Χ§Χ¦Χ™Χ” Χ©Χ Χ©ΧΆΧ•Χ Χ”ΧΧ™ΧΧ•Χ“. Χ©Χ™ΧΧ• ΧΧ‘ ΧΧ¦Χ•Χ¨Χ” Χ”ΧΧ•Χ¤Χ™Χ™Χ Χ™Χ Χ©Χ ΧΆΧ§Χ•ΧΧ Χ”-S (Χ΅Χ™Χ’ΧΧ•ΧΧ™Χ“) Χ©Χ
Χ”Χ¨Χ’Χ¨Χ΅Χ™Χ” Χ”ΧΧ•Χ’Χ™Χ΅ΧΧ™Χ. Χ”Χ Χ§Χ•Χ“Χ•Χ Χ”ΧΧ“Χ•ΧΧ•Χ Χ”Χ Χ”Χ ΧΧ•Χ Χ™Χ Χ”ΧΧ§Χ•Χ¨Χ™Χ™Χ, Χ•Χ”Χ§Χ• Χ”ΧΧ§Χ•Χ•Χ§Χ• Χ”ΧΧ Χ›Χ™ ΧΧ¦Χ™Χ™Χ ΧΧ Χ”Χ’Χ‘Χ•Χ Χ©Χ‘Χ• Χ”Χ”Χ΅ΧΧ‘Χ¨Χ•Χ ΧΧΆΧ‘Χ•Χ¨ ΧΧ Χ”ΧΧ‘Χ—Χ Χ”Χ™Χ 0.5.

## Χ§Χ•Χ“ Χ¤Χ™Χ™ΧΧ•Χ

Χ”Χ Χ” Χ§Χ•Χ“ Χ¤Χ™Χ™ΧΧ•Χ ΧΧ™Χ™Χ©Χ•Χ Χ¨Χ’Χ¨Χ΅Χ™Χ” ΧΧ•Χ’Χ™Χ΅ΧΧ™Χ:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
import pandas as pd
from math import log

# Our data
hours_studied = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]).reshape(-1, 1)  # Study hours
exam_results = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])  # Exam results (0=fail, 1=pass)

# Create logistic regression model
model = LogisticRegression(solver='liblinear')
model.fit(hours_studied, exam_results)

# Print results
print(f"Intercept (Ξ²β‚€): {model.intercept_[0]:.2f}")
print(f"Coefficient (Ξ²β‚): {model.coef_[0][0]:.2f}")

# Calculate equation
equation = f"P(pass) = 1 / (1 + e^-({model.intercept_[0]:.2f} + {model.coef_[0][0]:.2f} Γ— hours))"
print(f"Logistic equation: {equation}")

# Helper: get boundary for any probability
def get_threshold(probability):
    logit = log(probability / (1 - probability))
    return (logit - b0) / b1

# Coefficients
b0 = model.intercept_[0]
b1 = model.coef_[0][0]

# Find decision boundary (probability = 0.5)
decision_boundary = -model.intercept_[0] / model.coef_[0][0]
# or
decision_boundary = get_threshold(0.5)

print(f"Decision boundary: {decision_boundary:.2f} hours")

# Generate points for the logistic curve
x_test = np.linspace(0, 10, 100).reshape(-1, 1)
y_proba = model.predict_proba(x_test)[:, 1]

# Create the graph
plt.figure(figsize=(10, 6))
plt.scatter(hours_studied, exam_results, color='red', marker='o', label='Data points')
plt.plot(x_test, y_proba, color='blue', label='Logistic curve')
plt.axvline(x=decision_boundary, color='green', linestyle='--', label=f'Decision boundary ({decision_boundary:.2f} hours)')

# Add labels and formatting
plt.title('Logistic Regression - Study Hours vs. Exam Result')
plt.xlabel('Study Hours')
plt.ylabel('Probability of Passing Exam')
plt.grid(True, alpha=0.3)
plt.legend()
plt.text(1, 0.8, equation, fontsize=10, bbox=dict(facecolor='white', alpha=0.5))
plt.ylim(-0.05, 1.05)
plt.show()

# Make predictions for specific study hours
print("\nPredictions:")
for hours in [1, 2, 3, 4, 5, 6]:
    probability = 1 / (1 + np.exp(-(model.intercept_[0] + model.coef_[0][0] * hours)))
    outcome = "Pass" if probability >= 0.5 else "Fail"
    print(f"{hours} hours: {probability:.2f} probability of passing ({outcome})")
```

## Χ“Χ•Χ’ΧΧ Χ”Χ¨Χ¦Χ”

Χ›ΧΧ©Χ¨ Χ Χ¨Χ™Χ¥ ΧΧ Χ”Χ§Χ•Χ“, Χ Χ§Χ‘Χ:

<img src="log1.png" />

```
Intercept (Ξ²β‚€): -1.04
Coefficient (Ξ²β‚): 0.38
Logistic equation: P(pass) = 1 / (1 + e^-(-1.04 + 0.38 Γ— hours))
Decision boundary: 2.72 hours

Predictions:
1 hours: 0.34 probability of passing (Fail)
2 hours: 0.43 probability of passing (Fail)
3 hours: 0.53 probability of passing (Pass)
4 hours: 0.62 probability of passing (Pass)
5 hours: 0.70 probability of passing (Pass)
6 hours: 0.78 probability of passing (Pass)
```

Χ”ΧΧ•Χ¦ΧΧ•Χ ΧΧ¨ΧΧ•Χ Χ©ΧΧΧΧ™Χ“ Χ¦Χ¨Χ™Χ ΧΧΧΧ•Χ“ ΧΧ¤Χ—Χ•Χ 4.43 Χ©ΧΆΧ•Χ Χ›Χ“Χ™ Χ©Χ”Χ΅Χ™Χ›Χ•Χ™ Χ©ΧΧ• ΧΧΆΧ‘Χ•Χ¨ ΧΧ Χ”ΧΧ‘Χ—Χ Χ™Χ”Χ™Χ” ΧΧΆΧ 50%. Χ›ΧΧ• Χ›Χ, ΧΧΧΧ™Χ“ Χ©ΧΧ•ΧΧ“ 6 Χ©ΧΆΧ•Χ Χ™Χ© ΧΧ• Χ΅Χ™Χ›Χ•Χ™ Χ©Χ Χ›-78% ΧΧΆΧ‘Χ•Χ¨ ΧΧ Χ”ΧΧ‘Χ—Χ.

## Χ”Χ©Χ•Χ•ΧΧ” Χ‘Χ™Χ Χ¨Χ’Χ¨Χ΅Χ™Χ” ΧΧ™Χ ΧΧ¨Χ™Χ ΧΧ¨Χ’Χ¨Χ΅Χ™Χ” ΧΧ•Χ’Χ™Χ΅ΧΧ™Χ

| Χ”Χ™Χ‘Χ | Χ¨Χ’Χ¨Χ΅Χ™Χ” ΧΧ™Χ ΧΧ¨Χ™Χ | Χ¨Χ’Χ¨Χ΅Χ™Χ” ΧΧ•Χ’Χ™Χ΅ΧΧ™Χ |
|------|-----------------|------------------|
| **Χ΅Χ•Χ’ ΧΧ©ΧΧ Χ” ΧΧΧ•Χ™** | Χ¨Χ¦Χ™Χ£ (ΧΧ΅Χ¤Χ¨Χ™) | Χ‘Χ™Χ ΧΧ¨Χ™ (0/1, Χ›Χ/ΧΧ) |
| **ΧΧ•Χ•Χ— Χ”ΧΧ—Χ–Χ™Χ** | Χ›Χ ΧΆΧ¨Χ ΧΧΧ©Χ™ $(-\infty, \infty)$ | ΧΆΧ¨Χ Χ‘Χ™Χ 0 Χ-1 (Χ”Χ΅ΧΧ‘Χ¨Χ•Χ) |
| **Χ¤Χ•Χ Χ§Χ¦Χ™Χ™Χ Χ”Χ§Χ™Χ©Χ•Χ¨** | Χ–Χ”Χ•Χ $(y = \beta_0 + \beta_1 x)$ | ΧΧ•Χ’Χ™Χ $(\ln\frac{p}{1-p} = \beta_0 + \beta_1 x)$ |
| **Χ©Χ™ΧΧ ΧΧΧ™Χ“Χ”** | Χ©Χ™ΧΧ Χ”Χ¨Χ™Χ‘Χ•ΧΆΧ™Χ Χ”Χ¤Χ—Χ•ΧΧ™Χ | Χ©Χ™ΧΧ Χ”Χ Χ¨ΧΧ•Χ Χ”ΧΧ§Χ΅Χ™ΧΧΧ™Χ |
| **Χ¤Χ¨Χ©Χ Χ•Χ Χ”ΧΧ§Χ“ΧΧ™Χ** | Χ©Χ™Χ Χ•Χ™ Χ‘-Y ΧΧ™Χ—Χ™Χ“Χ” Χ©Χ X | Χ©Χ™Χ Χ•Χ™ Χ‘ΧΧ•Χ’ Χ©Χ Χ™Χ—Χ΅ Χ”Χ΅Χ™Χ›Χ•Χ™Χ™Χ |
| **Χ¦Χ•Χ¨Χ Χ”Χ§Χ©Χ¨** | Χ§Χ•Χ•Χ™ | ΧΆΧ§Χ•ΧΧ S (Χ΅Χ™Χ’ΧΧ•ΧΧ™Χ“) |
| **Χ”Χ Χ—Χ•Χ** | ΧΧ™Χ Χ™ΧΧ¨Χ™Χ•Χ, Χ Χ•Χ¨ΧΧΧ™Χ•Χ Χ©Χ Χ©ΧΧ¨Χ™Χ•Χ | ΧΧ ΧΧ Χ™Χ—Χ” Χ”ΧΧ¤ΧΧ’Χ•Χ Χ Χ•Χ¨ΧΧΧ™Χ |

## Χ™Χ™Χ©Χ•ΧΧ™Χ Χ©Χ Χ¨Χ’Χ¨Χ΅Χ™Χ” ΧΧ•Χ’Χ™Χ΅ΧΧ™Χ

Χ¨Χ’Χ¨Χ΅Χ™Χ” ΧΧ•Χ’Χ™Χ΅ΧΧ™Χ ΧΧ©ΧΧ©Χ Χ‘ΧΧ’Χ•Χ•Χ ΧΧ—Χ•ΧΧ™Χ:

1. **Χ¨Χ¤Χ•ΧΧ”**: Χ—Χ™Χ–Χ•Χ™ Χ”Χ΅Χ™Χ›Χ•Χ™ ΧΧ”ΧΧ¤ΧΧ—Χ•Χ ΧΧ—ΧΧ” Χ‘Χ”ΧΧ‘Χ΅Χ΅ ΧΆΧ Χ’Χ•Χ¨ΧΧ™ Χ΅Χ™Χ›Χ•Χ
2. **Χ¤Χ™Χ Χ Χ΅Χ™Χ**: Χ”ΧΆΧ¨Χ›Χ Χ΅Χ™Χ›Χ•Χ ΧΧ©Χ¨ΧΧ™ Χ•Χ—Χ™Χ–Χ•Χ™ Χ”ΧΧ ΧΧ§Χ•Χ— Χ™Χ—Χ–Χ™Χ¨ Χ”ΧΧ•Χ•ΧΧ”
3. **Χ©Χ™Χ•Χ•Χ§**: Χ—Χ™Χ–Χ•Χ™ Χ”ΧΧ ΧΧ§Χ•Χ— Χ™Χ¨Χ›Χ•Χ© ΧΧ•Χ¦Χ¨ ΧΧ΅Χ•Χ™Χ
4. **Χ—Χ™Χ Χ•Χ**: Χ—Χ™Χ–Χ•Χ™ Χ”Χ¦ΧΧ—Χ” ΧΧ• Χ›Χ™Χ©ΧΧ•Χ Χ©Χ ΧΧΧΧ™Χ“Χ™Χ
5. **ΧΧ“ΧΆΧ™ Χ”Χ—Χ‘Χ¨Χ”**: Χ Χ™ΧΧ•Χ— Χ©Χ Χ”Χ—ΧΧΧ•Χ Χ‘Χ™Χ ΧΧ¨Χ™Χ•Χ

## ΧΧ¨Χ’Χ™Χ Χ Χ•Χ΅Χ£

**ΧΧ¨Χ’Χ™Χ**: 
Χ—Χ‘Χ¨Χ ΧΧ©Χ¨ΧΧ™ Χ¨Χ•Χ¦Χ” ΧΧ—Χ–Χ•Χ Χ”ΧΧ ΧΧ§Χ•Χ— Χ™Χ—Χ–Χ™Χ¨ ΧΧ Χ”Χ”ΧΧ•Χ•ΧΧ” Χ©ΧΧ• (1=Χ”Χ—Χ–Χ™Χ¨, 0=ΧΧ Χ”Χ—Χ–Χ™Χ¨) ΧΆΧ Χ΅ΧΧ Χ”Χ”Χ›Χ Χ΅Χ” Χ”Χ©Χ ΧΧ™Χ Χ©ΧΧ• (Χ‘ΧΧΧ¤Χ™ Χ©Χ§ΧΧ™Χ). ΧΧ”ΧΧ Χ ΧΧ•Χ Χ™Χ Χ”Χ™Χ΅ΧΧ•Χ¨Χ™Χ™Χ:

| Χ”Χ›Χ Χ΅Χ” Χ©Χ ΧΧ™Χ (ΧΧΧ¤Χ™ Χ©"Χ—) | Χ”Χ—Χ–Χ™Χ¨ Χ”ΧΧ•Χ•ΧΧ” (1=Χ›Χ, 0=ΧΧ) |
|------------------------|---------------------------|
| 30                     | 0                         |
| 35                     | 0                         |
| 40                     | 0                         |
| 45                     | 0                         |
| 50                     | 0                         |
| 55                     | 1                         |
| 60                     | 0                         |
| 65                     | 1                         |
| 70                     | 1                         |
| 75                     | 1                         |
| 80                     | 1                         |
| 85                     | 1                         |
| 90                     | 1                         |

1. Χ‘Χ Χ” ΧΧ•Χ“Χ Χ¨Χ’Χ¨Χ΅Χ™Χ” ΧΧ•Χ’Χ™Χ΅ΧΧ™Χ Χ©ΧΧΧΧ¨ ΧΧ Χ”Χ§Χ©Χ¨ Χ‘Χ™Χ Χ”Χ”Χ›Χ Χ΅Χ” Χ”Χ©Χ ΧΧ™Χ ΧΧ‘Χ™Χ Χ”Χ—Χ–Χ¨ Χ”Χ”ΧΧ•Χ•ΧΧ”.
2. ΧΧ” Χ”Χ”Χ΅ΧΧ‘Χ¨Χ•Χ Χ©ΧΧ§Χ•Χ— ΧΆΧ Χ”Χ›Χ Χ΅Χ” Χ©Χ ΧΧ™Χ Χ©Χ 58 ΧΧΧ£ Χ©"Χ— Χ™Χ—Χ–Χ™Χ¨ ΧΧ Χ”Χ”ΧΧ•Χ•ΧΧ”?
3. ΧΧ”Χ™ Χ”Χ”Χ›Χ Χ΅Χ” Χ”Χ©Χ ΧΧ™Χ Χ”ΧΧ™Χ Χ™ΧΧΧ™Χ Χ©ΧΧ§Χ•Χ— Χ¦Χ¨Χ™Χ Χ›Χ“Χ™ Χ©Χ”Χ”Χ΅ΧΧ‘Χ¨Χ•Χ Χ©Χ™Χ—Χ–Χ™Χ¨ ΧΧ Χ”Χ”ΧΧ•Χ•ΧΧ” ΧΧ”Χ™Χ” ΧΧ¤Χ—Χ•Χ 75%?

## Χ Χ΅Χ¤Χ— Χ'- Χ›Χ™Χ¦Χ“ ΧΧΧ¦Χ•Χ ΧΧ Ξ²β‚€ Χ•ΦΎΞ²β‚

- ΧΧ—Χ©Χ‘Χ™Χ Χ Χ’Χ–Χ¨Χ•Χ Χ©Χ Χ”Χ¤Χ•Χ Χ§Χ¦Χ™Χ” ΧΧ¤Χ™ Ξ²β‚€ Χ•ΦΎΞ²β‚
- ΧΧ©Χ•Χ•Χ™Χ ΧΦΎ0 Χ›Χ“Χ™ ΧΧΧ¦Χ•Χ Χ Χ§Χ•Χ“Χ Χ§Χ™Χ¦Χ•Χ
- ΧΧ‘Χ... ΧΧ™ ΧΧ¤Χ©Χ¨ ΧΧ¤ΧΧ•Χ¨ ΧΧ Χ–Χ” Χ™Χ“Χ Χ™Χ Χ›ΧΧ• Χ‘Χ¨Χ’Χ¨Χ΅Χ™Χ” Χ¨Χ’Χ™ΧΧ”

#### ΧΧ” Χ”ΧΧΧ¨Χ”?
ΧΧ Χ—Χ Χ• Χ¨Χ•Χ¦Χ™Χ ΧΧΧ¦Χ•Χ ΧΧ Χ”ΧΆΧ¨Χ›Χ™Χ Χ©Χ ΧΧ¤Χ™ Ξ²β‚€ Χ•ΦΎΞ²β‚ Χ©ΧΧ‘Χ™ΧΧ™Χ ΧΧ Χ¤Χ•Χ Χ§Χ¦Χ™Χ™Χ Χ”ΧΧ•Χ’ΦΎΧΧ™Χ™Χ§ΧΧ™ ΧΧΧ§Χ΅Χ™ΧΧ•Χ

ΧΧ¤Χ™ ΧΧΧΧΧ™Χ§Χ”:
- Χ Χ§Χ•Χ“Χ ΧΧ§Χ΅Χ™ΧΧ•Χ Χ ΧΧ¦ΧΧ Χ›ΧΧ©Χ¨ **Χ”Χ Χ’Χ–Χ¨Χ = 0**
- Χ›ΧΧ•ΧΧ¨: Χ”Χ¤Χ•Χ Χ§Χ¦Χ™Χ” ΧΧ ΧΆΧ•ΧΧ” Χ•ΧΧ Χ™Χ•Χ¨Χ“Χ β€“ Χ–Χ• Χ¤Χ΅Χ’Χ” (ΧΧ• ΧΧ—ΧΧ™Χ, ΧΧ‘Χ Χ›ΧΧ ΧΧ“Χ•Χ‘Χ¨ ΧΆΧ ΧΧ§Χ΅Χ™ΧΧ•Χ)

#### ΧΧ”Χ™ Χ”Χ¤Χ•Χ Χ§Χ¦Χ™Χ” Χ©ΧΧ Χ•?

Χ–Χ•Χ”Χ™ Χ¤Χ•Χ Χ§Χ¦Χ™Χ™Χ **Χ”ΧΧ•Χ’ΦΎΧΧ™Χ™Χ§ΧΧ™** (log-likelihood):

$$
\log L(\beta_0, \beta_1) = \sum_{i=1}^{n} \left[ y_i \cdot \log(\hat{p}_i) + (1 - y_i) \cdot \log(1 - \hat{p}_i) \right]
$$

Χ›ΧΧ©Χ¨:

$$
\hat{p}_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_i)}}
$$

#### Χ Χ’Χ–Χ¨Χ•Χ:

Χ”Χ Χ’Χ–Χ¨Χ•Χ Χ©Χ Χ”Χ¤Χ•Χ Χ§Χ¦Χ™Χ” ΧΧ¤Χ™ Ξ²β‚€ Χ•ΦΎΞ²β‚ Χ”Χ:

- ΧΧ¤Χ™ $$\beta_1$$:

$$
\frac{\partial}{\partial \beta_1} \log L = \sum_{i=1}^{n} (y_i - \hat{p}_i) \cdot x_i
$$

- ΧΧ¤Χ™ $$\beta_0$$:

$$
\frac{\partial}{\partial \beta_0} \log L = \sum_{i=1}^{n} (y_i - \hat{p}_i)
$$


#### ΧΧΧ” ΧΧ©Χ•Χ•Χ™Χ ΧΦΎ0?

Χ›Χ“Χ™ ΧΧΧ¦Χ•Χ ΧΧ§Χ΅Χ™ΧΧ•Χ, ΧΧ©Χ•Χ•Χ™Χ ΧΧ Χ”Χ Χ’Χ–Χ¨Χ•Χ ΧΦΎ0:

$$
\frac{\partial}{\partial \beta} \log L = 0
$$

ΧΧ‘Χ... ΧΧ Χ Χ™ΧΧ ΧΧ¤ΧΧ•Χ¨ Χ™Χ©Χ™Χ¨Χ•Χ, ΧΧ›Χ Χ ΧΆΧ‘Χ•Χ¨ ΧΧ¤ΧΧ¨Χ•Χ Χ Χ•ΧΧ¨Χ™ (Χ›ΧΧ• Gradient Descent)

## π§Έ Χ“Χ•Χ’ΧΧ” ΧΧΧ”ΧΧ™Χ 

Χ Χ Χ™Χ— Χ©Χ™Χ© ΧΧ Χ• 2 ΧΧ¦Χ¤Χ™Χ•Χ:

| x  | y |
|----|---|
| 1  | 1 |
| 2  | 0 |

Χ•Χ Χ Χ™Χ— Χ©Χ›Χ¨Χ’ΧΆ:
- $$\beta_0 = 0$$
- $$\beta_1 = 0$$

#### Χ©ΧΧ‘ 1: Χ—Χ™Χ©Χ•Χ‘ Χ”Χ”Χ΅ΧΧ‘Χ¨Χ•Χ™Χ•Χ (p-hat)

$$
\hat{p}_1 = \frac{1}{1 + e^{-(0 + 0 \cdot 1)}} = 0.5
$$

$$
\hat{p}_2 = \frac{1}{1 + e^{-(0 + 0 \cdot 2)}} = 0.5
$$

#### Χ©ΧΧ‘ 2: 

**Χ—Χ™Χ©Χ•Χ‘ Χ Χ’Χ–Χ¨Χ ΧΧ¤Χ™ $$\beta_1$$**

$$
\frac{\partial}{\partial \beta_0} \log L = \sum_{i=1}^{n} (y_i - \hat{p}_i)
$$

$$
\sum (y_i - \hat{p}_i) \cdot x_i =
(1 - 0.5) \cdot 1 + (0 - 0.5) \cdot 2 =
0.5 - 1 = -0.5
$$

ΧΧΧ—Χ¨ Χ•Χ”Χ Χ’Χ–Χ¨Χ Χ©ΧΧ™ΧΧ™Χ, Χ”ΧΧ΅Χ§Χ Χ” Χ”Χ™Χ:
> **Χ¦Χ¨Χ™Χ ΧΧ”Χ’Χ“Χ™Χ ΧΧ** $$\beta_1$$ Χ›Χ“Χ™ ΧΧ©Χ¤Χ¨ ΧΧ Χ”ΧΧ•Χ“Χ.
> 

**Χ—Χ™Χ©Χ•Χ‘ Χ Χ’Χ–Χ¨Χ ΧΧ¤Χ™ $$\beta_0$$**

$$
\frac{\partial}{\partial \beta_0} \log L = \sum_{i=1}^{n} (y_i - \hat{p}_i)
$$

$$
\sum (y_i - \hat{p}_i) =
(1 - 0.5) + (0 - 0.5)  =
0.5 - 0.5 = 0
$$

- Χ”Χ Χ’Χ–Χ¨Χ ΧΧ¤Χ™ Ξ²β‚€ Χ©Χ•Χ•Χ” ΧΦΎ0 Χ‘Χ΅Χ™Χ‘Χ•Χ‘ Χ”Χ–Χ”
- ΧΧ›Χ **ΧΧ Χ ΧΆΧ“Χ›Χ ΧΧ Ξ²β‚€** Χ›Χ¨Χ’ΧΆ, Χ›Χ™ ΧΧ™Χ ΧΧΆΧ ΧΧ–Χ•Χ– ΧΧ ΧΧ Χ—Χ Χ• Χ‘Χ“Χ™Χ•Χ§ Χ‘ΧΧΧ¦ΧΆ

### π― Χ΅Χ™Χ›Χ•Χ:
- ΧΧ Χ”Χ Χ’Χ–Χ¨Χ Χ—Χ™Χ•Χ‘Χ™Χ β†’ ΧΧΆΧΧ™Χ/ΧΧ•Χ¨Χ™Χ“Χ™Χ ΧΧ Χ”ΧΧ§Χ“Χ ΧΧΧ•Χ™ Χ‘Χ¤Χ•Χ Χ§Χ¦Χ™Χ™Χ Χ”ΧΆΧΧ•Χ (Χ—Χ™Χ•Χ‘Χ™Χ/Χ©ΧΧ™ΧΧ™Χ)
- ΧΧ Χ”Χ Χ’Χ–Χ¨Χ Χ©ΧΧ™ΧΧ™Χ β†’ ΧΧ•Χ¨Χ™Χ“Χ™Χ/ΧΧΆΧΧ™Χ ΧΧ Χ”ΧΧ§Χ“Χ ΧΧΧ•Χ™ Χ‘Χ¤Χ•Χ Χ§Χ¦Χ™Χ™Χ Χ”ΧΆΧΧ•Χ (Χ—Χ™Χ•Χ‘Χ™Χ/Χ©ΧΧ™ΧΧ™Χ)
- Χ ΧΧ©Χ™Χ Χ›Χ ΧΆΧ“ Χ©Χ”Χ Χ’Χ–Χ¨Χ ΧΧ”Χ™Χ” β‰ 0 β€“ Χ•Χ ΧΧ¦Χ ΧΧ ΧΧ§Χ΅Χ™ΧΧ•Χ Χ”Χ¤Χ•Χ Χ§Χ¦Χ™Χ” π’›

### ΧΧ©ΧΧΧ©Χ™Χ Χ‘Χ¤ΧΧ¨Χ•Χ Χ Χ•ΧΧ¨Χ™

Χ‘Χ’ΧΧ Χ©Χ”Χ¤Χ•Χ Χ§Χ¦Χ™Χ” ΧΧ΅Χ•Χ‘Χ›Χ, Χ”ΧΧ—Χ©Χ‘ Χ¤Χ•ΧΧ¨ ΧΧ•ΧΧ” ΧΆΧ Χ™Χ“Χ™ Χ Χ™Χ΅Χ•Χ™ Χ•ΧΧΆΧ™Χ™Χ” Χ—Χ›ΧΧ”:

- Gradient Descent
- Newton-Raphson
- Χ©Χ™ΧΧ•Χ ΧΧ—Χ¨Χ•Χ Χ©Χ ΧΧ¦ΧΧ•Χ Χ‘ΧΧ•Χ“ΧΧ™Χ Χ›ΧΧ• `LogisticRegression` Χ‘Χ΅Χ§Χ™Χ§Χ™ΧΦΎΧΧ¨Χ

Χ›Χ Χ¤ΧΆΧ Χ”ΧΧ—Χ©Χ‘ ΧΧ©Χ Χ” Χ§Χ¦Χ ΧΧ Ξ²β‚€ Χ•ΦΎΞ²β‚, Χ‘Χ•Χ“Χ§ ΧΧ Χ”Χ”Χ΅ΧΧ‘Χ¨Χ•Χ™Χ•Χ Χ Χ”Χ™Χ• Χ™Χ•ΧΧ¨ ΧΧ•Χ‘Χ•Χ,  
Χ•ΧΧΧ©Χ™Χ ΧΆΧ“ Χ©Χ”Χ•Χ ΧΧ•Χ¦Χ ΧΧ Χ”ΧΆΧ¨Χ›Χ™Χ Χ”Χ›Χ™ ΧΧ•Χ‘Χ™Χ.

### β… Χ΅Χ™Χ›Χ•Χ:
- ΧΧ™Χ Χ Χ•Χ΅Χ—Χ” Χ΅Χ’Χ•Χ¨Χ” ΧΧΧ¦Χ™ΧΧ Ξ²β‚€ Χ•ΦΎΞ²β‚
- Χ”Χ¤ΧΧ¨Χ•Χ ΧΧΧ§Χ‘Χ Χ‘ΧΆΧ–Χ¨Χ ΧΧ•Χ¤ΧΧ™ΧΧ™Χ–Χ¦Χ™Χ” ΧΧΧΧΧ™Χ (Χ Χ•ΧΧ¨Χ™Χ)
- Χ”Χ¤Χ•Χ Χ§Χ¦Χ™Χ” Χ©ΧΧ Χ—Χ Χ• ΧΧΧ§Χ΅ΧΧ™Χ Χ Χ§Χ¨ΧΧ log-likelihood
