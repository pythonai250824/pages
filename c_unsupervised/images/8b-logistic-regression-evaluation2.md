# ğŸ¯ ×”××©×š ×”×¢×¨×›×ª ××•×“×œ ×¨×’×¨×¡×™×” ×œ×•×’×™×¡×˜×™×ª

---

## â— ×¤×¨×“×•×§×¡ ×”×“×™×•×§ (Accuracy Paradox)

×œ×¤×¢××™× ××“×“ ×”×“×™×•×§ **××˜×¢×”** â€“ ×‘××™×•×—×“ ×›×©×™×© **×—×•×¡×¨ ××™×–×•×Ÿ ×‘× ×ª×•× ×™×**.

### ğŸ” ×“×•×’××”:
× × ×™×— ×©×‘×××’×¨ ×©×œ× ×•:
- 95 ××ª×•×š 100 ×”×× ×©×™× **×œ× ×—×•×œ×™×**
- ×¨×§ 5 ×—×•×œ×™×

×”××•×“×œ "××ª×—×›×" ×•×× ×‘× ×©×›×•×œ× ×œ× ×—×•×œ×™× (×›×œ×•××¨ ×ª××™×“ 0).

#### ğŸ¯ ×ª×•×¦××”:
- Accuracy = 95%
- ××‘×œ ×”××•×“×œ **××£ ×¤×¢× ×œ× ××–×”×” ×—×•×œ×” ×××™×ª×™**
- ×›×œ×•××¨ ×”××•×“×œ ×—×¡×¨ ×ª×•×¢×œ×ª ×œ××¨×•×ª ×“×™×•×§ ×’×‘×•×”

---

## ğŸ“¢ Recall (×©×—×–×•×¨)

××•×“×“ ×›××” ××”××§×¨×™× **×”×—×™×•×‘×™×™× ×”×××™×ª×™×™×** ×”××•×“×œ ×”×¦×œ×™×— ×œ×’×œ×•×ª.

$$
\text{Recall} = \frac{TP}{TP + FN}
$$

- TP = ×—×™×–×•×™ ×—×™×•×‘×™ × ×›×•×Ÿ  
- FN = ×¤×¡×¤×•×¡ ×©×œ ×—×™×•×‘×™

### ğŸ§  ×“×•×’××”:
×× ×™×© 5 ×—×•×œ×™×, ×•×”××•×“×œ ×’×™×œ×” 4 â†’  

$$
\text{Recall} = \frac{4}{5} = 0.8
$$

---

## ğŸ¯ Precision (×“×™×•×§ ×ª×—×–×™×•×ª ×—×™×•×‘×™×•×ª)

××•×“×“ ×›××” ××”×ª×—×–×™×•×ª ×©×”××•×“×œ ×—×–×” ×›"×—×™×•×‘×™" ×”×Ÿ ×‘×××ª × ×›×•× ×•×ª.

$$
\text{Precision} = \frac{TP}{TP + FP}
$$

- FP = ×—×™×–×•×™ ×—×™×•×‘×™ ×©×’×•×™ (False Alarm)

### ğŸ§  ×“×•×’××”:
×× ×”××•×“×œ ×—×–×” 6 ×—×™×•×‘×™×™×, ××‘×œ ×¨×§ 4 ×‘×××ª ×—×•×œ×™× â†’  

$$
\text{Precision} = \frac{4}{6} \approx 0.666
$$

---

## âš–ï¸ F1 Score â€“ ×××•×¦×¢ ×”×¨××•× ×™ ×©×œ Precision ×•-Recall

×××–×Ÿ ×‘×™×Ÿ Precision ×•-Recall. ×—×©×•×‘ ×‘××™×•×—×“ ×›×©×™×© ×—×•×¡×¨ ××™×–×•×Ÿ ×‘×™×Ÿ ×”××§×¨×™×.

$$
\text{F1 Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
$$

### ğŸ§  ×“×•×’××”:
Precision = 0.666, Recall = 0.8

$$
\text{F1} = \frac{2 \cdot 0.666 \cdot 0.8}{0.666 + 0.8} \approx 0.72
$$

**×“×•×’××” ×‘×¤×™×™×ª×•×Ÿ:**

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
import numpy as np

# Example data: study hours and exam result
X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9]])
y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])  # 0 = fail, 1 = pass

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train the logistic regression model
model = LogisticRegression(solver='liblinear')
model.fit(X_train, y_train)

# Predict the test set
y_pred = model.predict(X_test)

# Calculate and print evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy:  {accuracy:.2f}")
print(f"Recall:    {recall:.2f}")
print(f"Precision: {precision:.2f}")
print(f"F1 Score:  {f1:.2f}")
```

Output (using a small sample we will get 100%):

```
Accuracy:  1.00
Recall:    1.00
Precision: 1.00
F1 Score:  1.00
```

